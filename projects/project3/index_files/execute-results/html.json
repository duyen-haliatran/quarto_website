{
  "hash": "be516f186335991e498bdb3a913507ba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Conjoint\"\nauthor: Duyen Tran\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\neditor_options: \n  chunk_output_type: console\n---\n\n\n\nThis assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans.\n\n\n## 1. Estimating Yogurt Preferences\n\n### Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, size, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n### Yogurt Dataset\n\nWe will use the `yogurt_data` dataset, which provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were \"featured\" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc.\n\n#### Data Overview\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `id`          | anonymized consumer identifiers.                                                           |\n| `y1`, `y2`, `y3`, `y4`            | a vector indicating the chosen product.                                                            |\n| `f1`, `f2`, `f3`, `f4`              | a vector indicating if any products were \"featured\" in the store as a form of advertising                                                         |\n| `p1`, `p2`, `p3`, `p4`             |the products’ prices                                                    |\n::::\n\n::: {#902449b7 .cell execution_count=2}\n``` {.python .cell-code}\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>y1</th>\n      <th>y2</th>\n      <th>y3</th>\n      <th>y4</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.108</td>\n      <td>0.081</td>\n      <td>0.061</td>\n      <td>0.079</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.108</td>\n      <td>0.098</td>\n      <td>0.064</td>\n      <td>0.075</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.108</td>\n      <td>0.098</td>\n      <td>0.061</td>\n      <td>0.086</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.108</td>\n      <td>0.098</td>\n      <td>0.061</td>\n      <td>0.086</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.125</td>\n      <td>0.098</td>\n      <td>0.049</td>\n      <td>0.079</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#1073eab4 .cell execution_count=3}\n``` {.python .cell-code}\nyogurt_data.describe(include='all')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>y1</th>\n      <th>y2</th>\n      <th>y3</th>\n      <th>y4</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2430.0000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n      <td>2430.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1215.5000</td>\n      <td>0.341975</td>\n      <td>0.401235</td>\n      <td>0.029218</td>\n      <td>0.227572</td>\n      <td>0.055556</td>\n      <td>0.039506</td>\n      <td>0.037449</td>\n      <td>0.037449</td>\n      <td>0.106248</td>\n      <td>0.081532</td>\n      <td>0.053622</td>\n      <td>0.079507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>701.6249</td>\n      <td>0.474469</td>\n      <td>0.490249</td>\n      <td>0.168452</td>\n      <td>0.419351</td>\n      <td>0.229109</td>\n      <td>0.194836</td>\n      <td>0.189897</td>\n      <td>0.189897</td>\n      <td>0.020587</td>\n      <td>0.011047</td>\n      <td>0.008054</td>\n      <td>0.007714</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.012000</td>\n      <td>0.000000</td>\n      <td>0.025000</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>608.2500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.103000</td>\n      <td>0.081000</td>\n      <td>0.050000</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1215.5000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.108000</td>\n      <td>0.086000</td>\n      <td>0.054000</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1822.7500</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.115000</td>\n      <td>0.086000</td>\n      <td>0.061000</td>\n      <td>0.086000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2430.0000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.193000</td>\n      <td>0.111000</td>\n      <td>0.086000</td>\n      <td>0.104000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Statistics Summary:**\n\n- There are 2,430 records.\n\n- Binary fields (y1, y2, y3, y4, f1, f2, f3, f4) indicate varying levels of frequency with which different yogurts were chosen or conditions were met.\n\n- Price or index fields (p1, p2, p3, p4) show distributions with differing means, minima, and maxima, suggesting variability in yogurt pricing or attributes across the samples.\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we'll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts' prices:  \n\n$$\nx_j' = \\left[ \\mathbb{1}(\\text{Yogurt 1}), \\mathbb{1}(\\text{Yogurt 2}), \\mathbb{1}(\\text{Yogurt 3}), X_f, X_p \\right]\n$$\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). \n\nWhat we would like to do is reorganize the data from a \"wide\" shape with $n$ rows and multiple columns for each covariate, to a \"long\" shape with $n \\times J$ rows and a single column for each covariate.  As part of this re-organization, we'll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be \"pivoted\" or \"melted\" from wide to long.  \n\n**Reshape and prep the data**\n\n::: {#dc5f2ddd .cell execution_count=4}\n``` {.python .cell-code}\n# Melt the data into a long format\nlong_data = pd.melt(yogurt_data, id_vars=['id'], \n                    value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],\n                    var_name='product_feature', value_name='value')\n\n# Extract product and feature types from the 'product_feature' column\nlong_data['product'] = long_data['product_feature'].str.extract('(\\d)').astype(int)\nlong_data['feature'] = long_data['product_feature'].str.extract('([a-z]+)')\n\n# Pivot the table to get one row per consumer per product\nreshaped_yogurt = long_data.pivot_table(index=['id', 'product'], columns='feature', values='value', aggfunc='first').reset_index()\n\n# Add the binary indicators for the first three yogurts\nfor j in range(1, 4):\n    reshaped_yogurt[f'Yogurt{j}'] = (reshaped_yogurt['product'] == j).astype(int)\n\n# Ensure the resulting DataFrame is correctly structured\nreshaped_yogurt.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>feature</th>\n      <th>id</th>\n      <th>product</th>\n      <th>f</th>\n      <th>p</th>\n      <th>y</th>\n      <th>Yogurt1</th>\n      <th>Yogurt2</th>\n      <th>Yogurt3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.108</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.081</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.061</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.079</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.108</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Estimation\n\n**Code up the log-likelihood function.**\n\n::: {#b2c39a5f .cell execution_count=5}\n``` {.python .cell-code}\ndef log_likelihood(beta, X, Y):\n    # Calculate utilities: V[n, j] = X[n, j, :] dot beta\n    V = np.tensordot(X, beta, axes=([2], [0]))\n    \n    # Compute the log-sum-exp for each individual to avoid numerical overflow issues\n    log_sum_exp = np.log(np.sum(np.exp(V), axis=1))\n    \n    # Calculate log likelihood component for each choice\n    log_probs = V - log_sum_exp[:, np.newaxis]\n\n    # Compute the log-likelihood\n    log_likelihood = np.sum(Y * log_probs)\n\n    return log_likelihood\n```\n:::\n\n\n**Use `optimize()` in Python to find the MLEs for the 5 parameters ($\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p$).**\n\n::: {#f093648a .cell execution_count=6}\n``` {.python .cell-code}\ndef negative_log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n    # Return negative log likelihood\n    return -np.sum(log_likelihood)\n\n# Prepare the input matrix X and the choice vector\nn_products = 4  # There are 4 products\nfeatures = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'f', 'p']\nX = reshaped_yogurt[features].values\nchoices = reshaped_yogurt['y'].values\n\n# Define initial guesses for the parameters\ninitial_beta = np.zeros(len(features))\n\n# Rerun the optimization with numpy properly imported and initial_beta defined\nresult = minimize(negative_log_likelihood, initial_beta, args=(X, choices))\n\n# Print the results\nresult.x\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([  1.38775052,   0.64350461,  -3.08611318,   0.4874149 ,\n       -37.05782801])\n```\n:::\n:::\n\n\n### Discussion\n\nThe estimated parameters for the three yogurt product intercepts are:\n\n$\\beta_1$ = 1.39 \n\n$\\beta_2$ = 0.64\n\n$\\beta_3$ = - 3.09\n\nThese coefficients represent the intrinsic utilities (or preferences) of the three yogurt products when all other variables (such as price and whether the product was featured) are held constant. Here's how to interpret these intercepts in the context of consumer preferences:\n\n$\\beta_1$ (Yogurt 1): The positive and highest value among the three suggests that Yogurt 1 is the most preferred when no other attributes (like price or features) are considered. It has the highest intrinsic utility.\n\n$\\beta_2$ (Yogurt 2): This is also positive but lower than $\\beta_1$\n, indicating that Yogurt 2 is less preferred than Yogurt 1 but still has a positive intrinsic appeal compared to a baseline (which could be another product not included in these three, like Yogurt 4 in this analysis).\n\n$\\beta_3$ (Yogurt 3): The negative value here suggests that Yogurt 3 is least preferred among the three, having a lower intrinsic utility relative to the others.\n\nGiven these interpretations, Yogurt 1 appears to be the most preferred option among the first three, followed by Yogurt 2, with Yogurt 3 being the least preferred under the assumption that other factors are equal. This intrinsic preference could be driven by factors not explicitly modeled but captured by the intercepts, such as brand affinity, flavor preferences, or other unobserved attributes associated with each product.\n\n**Use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.**\n\n::: {#5f09ba79 .cell execution_count=7}\n``` {.python .cell-code}\n# Extracted beta values for Yogurt 1 and Yogurt 3 and the price coefficient\nbeta_1 = 1.39\nbeta_3 = -3.09\nbeta_p = -37.06  # The negative price coefficient\n\n# Calculate utility difference\nutility_difference = beta_1 - beta_3\n\n# Convert utility difference to dollar benefit using the price coefficient\ndollar_benefit = utility_difference / abs(beta_p)\n\nprint(\"Per-unit monetary measure of brand value is \", round(dollar_benefit, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPer-unit monetary measure of brand value is  0.1209\n```\n:::\n:::\n\n\nThe per-unit monetary measure of brand value between the most-preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is approximately $0.12 per unit. This means that, in terms of brand value, consumers might be willing to pay an extra 12 cents per unit for Yogurt 1 compared to Yogurt 3, based solely on their preference (utility difference) as captured by the model. This is a useful way to quantify the monetary value of consumer preferences in this context\n\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\n\nCalculate the market shares in the market at the time the data were collected.  Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of $N \\times 4$ estimated choice probabilities).  Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1.  Do the yogurt 1 market shares decrease?_\n\n::: {#cde6caf2 .cell execution_count=8}\n``` {.python .cell-code}\ndef calculate_probabilities(beta, X):\n    utility = X.dot(beta)\n    exp_util = np.exp(utility)\n    sum_exp_util = np.sum(exp_util.reshape(-1, n_products), axis=1)\n    probabilities = exp_util / np.repeat(sum_exp_util, n_products)\n    return probabilities.reshape(-1, n_products)\n\n# Use the estimated beta values from the optimization\nestimated_beta = result.x\n\n# Calculate the initial choice probabilities for all products and all consumers\ninitial_probabilities = calculate_probabilities(estimated_beta, X)\n\n# Calculate the current market shares by taking the mean of probabilities across all consumers for each product\ncurrent_market_shares = np.mean(initial_probabilities, axis=0)\n\n# Display the current market shares\ncurrent_market_shares\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0.3419753 , 0.40123457, 0.02921809, 0.22757204])\n```\n:::\n:::\n\n\nThe current market shares for the four yogurt products are approximately:\n\nYogurt 1: 34.2%\n\nYogurt 2: 40.1%\n\nYogurt 3: 2.9%\n\nYogurt 4: 22.8%\n\nNext, let's increase the price of Yogurt 1 by $0.10 and then use the fitted model to predict the new choice probabilities. We'll see how the market shares change, particularly for Yogurt 1, as a result of this price increase.\n\n::: {#806f7dac .cell execution_count=9}\n``` {.python .cell-code}\n# Increase the price of Yogurt 1 by $0.10\n# First, create a new X matrix with the updated price for Yogurt 1\nX_new_prices = X.copy()\nprice_increase = 0.10\nX_new_prices[:, 4][X_new_prices[:, 0] == 1] += price_increase  # Only increase the price in the entries for Yogurt 1\n\n# Calculate the new choice probabilities with the increased price of Yogurt 1\nnew_probabilities = calculate_probabilities(estimated_beta, X_new_prices)\n\n# Calculate the new market shares by taking the mean of new probabilities across all consumers for each product\nnew_market_shares = np.mean(new_probabilities, axis=0)\n\n# Display the new market shares\nnew_market_shares\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.02111777, 0.59114505, 0.04404012, 0.34369705])\n```\n:::\n:::\n\n\nThe new market shares for the four yogurt products after increasing the price of Yogurt 1 by $0.10 are approximately:\n\nYogurt 1: 2.1%\n\nYogurt 2: 59.1%\n\nYogurt 3: 4.4%\n\nYogurt 4: 34.4%\n\nYogurt 1's market share dramatically decreases from 34.2% to 2.1% due to the price increase.\n\nYogurt 2's market share significantly increases, absorbing most of the share lost by Yogurt 1.\n\nYogurt 3 and Yogurt 4 also see some increase in their market shares.\nThis demonstrates the sensitivity of market share to price changes in competitive markets, especially under the assumption of a Multinomial Logit model where the relative utilities directly affect the choice probabilities. Yogurt 1's substantial price increase leads consumers to switch to the more affordable alternatives, illustrating the impact of price elasticity on consumer choice behavior\n\n\n## 2. Estimating Minivan Preferences\n\n\n### Data\n\n_todo: download the dataset from here:_ http://goo.gl/5xQObB \n\n_todo: describe the data a bit. How many respondents took the conjoint survey?  How many choice tasks did each respondent complete?  How many alternatives were presented on each choice task? For each alternative._\n\n::: {#6e720118 .cell execution_count=10}\n``` {.python .cell-code}\nminivan = pd.read_csv(\"rintro-chapter13conjoint.csv\")\nminivan\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resp.id</th>\n      <th>ques</th>\n      <th>alt</th>\n      <th>carpool</th>\n      <th>seat</th>\n      <th>cargo</th>\n      <th>eng</th>\n      <th>price</th>\n      <th>choice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>6</td>\n      <td>2ft</td>\n      <td>gas</td>\n      <td>35</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>yes</td>\n      <td>8</td>\n      <td>3ft</td>\n      <td>hyb</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>yes</td>\n      <td>6</td>\n      <td>3ft</td>\n      <td>gas</td>\n      <td>30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>6</td>\n      <td>2ft</td>\n      <td>gas</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>yes</td>\n      <td>7</td>\n      <td>3ft</td>\n      <td>gas</td>\n      <td>35</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8995</th>\n      <td>200</td>\n      <td>14</td>\n      <td>2</td>\n      <td>no</td>\n      <td>7</td>\n      <td>3ft</td>\n      <td>gas</td>\n      <td>35</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8996</th>\n      <td>200</td>\n      <td>14</td>\n      <td>3</td>\n      <td>no</td>\n      <td>7</td>\n      <td>3ft</td>\n      <td>hyb</td>\n      <td>35</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8997</th>\n      <td>200</td>\n      <td>15</td>\n      <td>1</td>\n      <td>no</td>\n      <td>7</td>\n      <td>2ft</td>\n      <td>gas</td>\n      <td>35</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8998</th>\n      <td>200</td>\n      <td>15</td>\n      <td>2</td>\n      <td>no</td>\n      <td>8</td>\n      <td>3ft</td>\n      <td>elec</td>\n      <td>40</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8999</th>\n      <td>200</td>\n      <td>15</td>\n      <td>3</td>\n      <td>no</td>\n      <td>6</td>\n      <td>3ft</td>\n      <td>gas</td>\n      <td>35</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9000 rows × 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `resp.id`          | Identifier for the respondent.                                                           |\n| `ques`            | Question or choice task number.                                                             |\n| `alt`              | Alternative within each choice task.                                                          |\n| `carpool`, `seat`, `cargo`, `eng`              |Attributes of each alternative, such as carpool availability, seating capacity, cargo space, and engine type.                                                    |\n|`price`                                          | Price associated with each alternative.|\n|`choice` |Whether the alternative was chosen (1) or not (0).|\n::::\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n::: {#909f2088 .cell execution_count=11}\n``` {.python .cell-code}\n# Number of respondents\nnum_respondents = minivan['resp.id'].nunique()\n\n# Number of choice tasks completed by each respondent\ntasks_per_respondent = minivan.groupby('resp.id')['ques'].nunique()\n\n# Number of alternatives per choice task\n# Assuming the structure is consistent across the dataset\nalternatives_per_task = minivan.groupby(['resp.id', 'ques'])['alt'].nunique().max()\n\nnum_respondents, tasks_per_respondent.describe(), alternatives_per_task\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n(200,\n count    200.0\n mean      15.0\n std        0.0\n min       15.0\n 25%       15.0\n 50%       15.0\n 75%       15.0\n max       15.0\n Name: ques, dtype: float64,\n 3)\n```\n:::\n:::\n\n\n**Here's a summary of the conjoint survey data:**\n\n- Number of Respondents: There are 200 respondents who participated in the survey.\n\n- Number of Choice Tasks per Respondent: Each respondent completed 15 choice tasks. This number is consistent across all respondents.\n\n- Number of Alternatives per Choice Task: Each choice task presented 3 alternatives.\n\n### Model\n\n_todo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors.  You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation._  \n\n::: {#102f0471 .cell execution_count=12}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Create dummy variables for the categorical attributes, excluding base levels\nminivan['seat_7'] = (minivan['seat'] == 7).astype(int)\nminivan['seat_8'] = (minivan['seat'] == 8).astype(int)\nminivan['cargo_3ft'] = (minivan['cargo'] == '3ft').astype(int)\nminivan['eng_hybrid'] = (minivan['eng'] == 'hyb').astype(int)\nminivan['eng_electric'] = (minivan['eng'] == 'elec').astype(int)\n\n# Define the model formula\nformula = 'choice ~ price + seat_7 + seat_8 + cargo_3ft + eng_hybrid + eng_electric'\n\n# Fit the model using statsmodels\nmnl_model = smf.glm(formula=formula, data=minivan, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model which includes coefficients and standard errors\nmnl_model.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>choice</td>      <th>  No. Observations:  </th>  <td>  9000</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  8993</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     6</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -5028.0</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 15 May 2024</td> <th>  Deviance:          </th> <td>  10056.</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>12:53:13</td>     <th>  Pearson chi2:      </th> <td>9.01e+03</td>\n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>4</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1442</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>    <td>    5.5322</td> <td>    0.224</td> <td>   24.677</td> <td> 0.000</td> <td>    5.093</td> <td>    5.972</td>\n</tr>\n<tr>\n  <th>price</th>        <td>   -0.1591</td> <td>    0.006</td> <td>  -25.616</td> <td> 0.000</td> <td>   -0.171</td> <td>   -0.147</td>\n</tr>\n<tr>\n  <th>seat_7</th>       <td>   -0.5248</td> <td>    0.060</td> <td>   -8.800</td> <td> 0.000</td> <td>   -0.642</td> <td>   -0.408</td>\n</tr>\n<tr>\n  <th>seat_8</th>       <td>   -0.2931</td> <td>    0.059</td> <td>   -5.009</td> <td> 0.000</td> <td>   -0.408</td> <td>   -0.178</td>\n</tr>\n<tr>\n  <th>cargo_3ft</th>    <td>    0.4385</td> <td>    0.049</td> <td>    9.004</td> <td> 0.000</td> <td>    0.343</td> <td>    0.534</td>\n</tr>\n<tr>\n  <th>eng_hybrid</th>   <td>   -0.7605</td> <td>    0.057</td> <td>  -13.361</td> <td> 0.000</td> <td>   -0.872</td> <td>   -0.649</td>\n</tr>\n<tr>\n  <th>eng_electric</th> <td>   -1.4347</td> <td>    0.062</td> <td>  -23.217</td> <td> 0.000</td> <td>   -1.556</td> <td>   -1.314</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nNow let's try our own function\n\n::: {#fa33562d .cell execution_count=13}\n``` {.python .cell-code}\n# Prepare the design matrix X with intercept\nX_custom = np.column_stack([\n    np.ones(len(minivan)),  # Intercept\n    minivan['price'],\n    minivan['seat_7'],\n    minivan['seat_8'],\n    minivan['cargo_3ft'],\n    minivan['eng_hybrid'],\n    minivan['eng_electric']\n])\n\n# Response vector y\ny_custom = minivan['choice'].values\ndef negative_log_likelihood(beta, X, y):\n    # Logistic model for P(y=1)\n    logits = np.dot(X, beta)\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Log-likelihood of the logistic model\n    log_likelihood = y * np.log(probabilities + 1e-9) + (1 - y) * np.log(1 - probabilities + 1e-9)  # Add small constant for numerical stability\n    \n    # Return negative log-likelihood\n    return -np.sum(log_likelihood)\n\n# Initial guess for the parameters\ninitial_beta = np.zeros(X_custom.shape[1])\n\n# Perform the optimization to find the MLEs\nresult_custom = minimize(negative_log_likelihood, initial_beta, args=(X_custom, y_custom))\n\n# Extract estimated parameters\nestimated_beta_custom = result_custom.x\nestimated_beta_custom\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([ 5.53219323, -0.15913327, -0.52475184, -0.29308497,  0.43853855,\n       -0.76048953, -1.43468089])\n```\n:::\n:::\n\n\nThese results are consistent with those obtained using statsmodels, which confirms the robustness of our custom function for this analysis.\n\n### Results\n\nInterpretation:\n\nPrice: The negative coefficient (-0.1591) suggests that as the price increases by one thousand dollars, the log odds of choosing a particular car decrease, indicating a typical negative relationship between price and purchase probability.\n\nSeat 7: Having 7 seats, compared to the baseline of 6 seats, is associated with lower odds of the car being chosen.\n\nSeat 8: Similarly, having 8 seats is also less preferable compared to 6 seats but less so than 7 seats.\n\nCargo 3ft: More cargo space (3ft) increases the odds of choosing the car compared to the base level of 2ft.\nThis feature is preferred over the baseline of 2ft cargo space, as indicated by the positive coefficient. Consumers prefer more cargo space, all else being equal.\n\nEngine Hybrid and Electric: Both hybrid and electric engines are less preferred compared to a traditional gas engine, with electric being the least preferred among the options.\n\n\n_todo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?_\n\n::: {#bb34007e .cell execution_count=14}\n``` {.python .cell-code}\n# utility_cargo_3ft = 0.4385\n# price_coefficient = -0.1591  # Negative because price reduces utility\n\n# # Calculate the dollar value using the price coefficient as the conversion factor\n# dollar_value_cargo_3ft = utility_cargo_3ft / abs(price_coefficient)\n\n# # Convert the result to reflect the price in thousands of dollars\n# dollar_value_cargo_3ft * 1000\n\n # Coefficients from the model results\ncargo_coeff = 0.4385\nprice_coeff = -0.1591\n\n# Calculate the dollar value of having 3ft of cargo space compared to 2ft\ndollar_value_cargo = (cargo_coeff / price_coeff) * (-1)\ndollar_value_cargo\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n2.7561282212445004\n```\n:::\n:::\n\n\nThe dollar value of having 3 feet of cargo space compared to 2 feet, based on the model, is approximately $2,756. This amount represents the additional value that respondents place on having an extra foot of cargo space in their vehicle choice.\n\n_todo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market._\n\n| Minivan | Seats | Cargo | Engine | Price |\n|---------|-------|-------|--------|-------|\n| A       | 7     | 2     | Hyb    | 30    |\n| B       | 6     | 2     | Gas    | 30    |\n| C       | 8     | 2     | Gas    | 30    |\n| D       | 7     | 3     | Gas    | 40    |\n| E       | 6     | 2     | Elec   | 40    |\n| F       | 7     | 2     | Hyb    | 35    |\n\n::: {#5a0b2fb7 .cell execution_count=15}\n``` {.python .cell-code}\n# Define the minivans data according to the attributes\nminivans = pd.DataFrame({\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'Seats': [7, 6, 8, 7, 6, 7],\n    'Cargo': [2, 2, 2, 3, 2, 2],\n    'Engine': ['Hyb', 'Gas', 'Gas', 'Gas', 'Elec', 'Hyb'],\n    'Price': [30, 30, 30, 40, 40, 35]\n})\n\n# Create dummy variables for the minivan data\nminivans['seat_7'] = (minivans['Seats'] == 7).astype(int)\nminivans['seat_8'] = (minivans['Seats'] == 8).astype(int)\nminivans['cargo_3ft'] = (minivans['Cargo'] == 3).astype(int)\nminivans['eng_hyb'] = (minivans['Engine'] == 'Hyb').astype(int)\nminivans['eng_elec'] = (minivans['Engine'] == 'Elec').astype(int)\n\n# Extract the coefficients from the model\n\n# Check if the keys exist in the params and then extract them\nparams = mnl_model.params\nintercept = params['const'] if 'const' in params.index else 0\ncoeff_seats_7 = params['seat_7'] if 'seat_7' in params.index else 0\ncoeff_seats_8 = params['seat_8'] if 'seat_8' in params.index else 0\ncoeff_cargo_3ft = params['cargo_3ft'] if 'cargo_3ft' in params.index else 0\ncoeff_eng_hyb = params['eng_hyb'] if 'eng_hyb' in params.index else 0\ncoeff_eng_elec = params['eng_elec'] if 'eng_elec' in params.index else 0\ncoeff_price = params['price'] if 'price' in params.index else 0\n\n\n\n# Calculate the utilities for each minivan\nminivans['Utility'] = (intercept +\n                       coeff_seats_7 * minivans['seat_7'] +\n                       coeff_seats_8 * minivans['seat_8'] +\n                       coeff_cargo_3ft * minivans['cargo_3ft'] +\n                       coeff_eng_hyb * minivans['eng_hyb'] +\n                       coeff_eng_elec * minivans['eng_elec'] +\n                       coeff_price * minivans['Price'])\n\n# Convert utilities to market shares\nminivans['Market_Share'] = np.exp(minivans['Utility']) / np.exp(minivans['Utility']).sum()\n\nminivans[['Minivan', 'Utility', 'Market_Share']]\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Minivan</th>\n      <th>Utility</th>\n      <th>Market_Share</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>-5.298733</td>\n      <td>0.197552</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>-4.773982</td>\n      <td>0.333870</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>-5.067066</td>\n      <td>0.249054</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>-6.451522</td>\n      <td>0.062378</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E</td>\n      <td>-6.365309</td>\n      <td>0.067994</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>F</td>\n      <td>-6.094397</td>\n      <td>0.089152</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nMinivan B appears to be the most preferred option with the highest utility leading to a market share of 33.39%. This suggests that the combination of features and price for Minivan B aligns well with consumer preferences in the modelled scenario.\n\nMinivan A also shows strong preference, holding nearly 20% of the market share.\n\nMinivan C follows closely with almost 25% market share, indicating that it also meets a significant portion of consumer preferences.\n\nMinivans D, E, and F exhibit lower utilities, translating into smaller market shares. These models, possibly due to higher prices or less preferred attributes, are less attractive to the modeled consumer base.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}