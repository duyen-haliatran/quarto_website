[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, I‚Äôm Duyen Tran üëã - Business Analyst Extraordinaire",
    "section": "",
    "text": "I bridge the gap between IT and business, translating complex business needs into actionable technical requirements. With a background in diverse job experience, I pride myself on being the lynchpin that ensures project success.\n\nüìä Currently leveraging data analytics to drive business improvements.\nüìò Continuing my learning journey in data science to enrich my BA toolkit at UC San Diego.\nüíº Previously collaborated with cross-functional teams in the finance and healthcare sectors.\nüì´ Reach out to me at: duyen.tran@rady.ucsd.edu\n‚ö° Fun fact: In my free time, I enjoy making handmade things and exploring local coffee shops.\n\n\n\n\nMS in Business Analytics - University of California, San Diego (2023 - 2024)\nMBA in International Marketing - Management Development Institute of Singapore (2018 - 2019)\nBA in Real Estates - National Economics University, Viet Nam (2013 - 2017)\n\n\n\n\n\nDocumentation: User Stories, Use Cases\nData Analysis: Excel, SQL, PostgreSQL, Python(Especially: Pandas, Numpy, Scipy, Matplotlib), R(Especially: Tidyverse, Simmer)\n\n\n\n\n\nProduct Manager/Analyst (Feb 2020 - Dec 2023)\nDigital Marketing Analyst (Jul 2017 - Feb 2020)\nAppraiser Intern (Jan 2017 - Jun 2017)\n\n\n\n\n\nFinancial Forecasting Tool: Collaborated with finance experts to develop a tool that predicts quarterly revenue.\nE-Commerce Recommendation Engine: Worked with the marketing department to integrate a product recommendation system into an online marketplace.\n\n\n\n\n\n\n\nLinkedIn Badge\n\n\n\n\n\nI‚Äôm keen on integrating more data science techniques into my BA toolkit. If you have any course or book recommendations or any collaboration opportunities, please drop me a message!\n\n‚≠êÔ∏è From DuyenTran"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Fraud Detection Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised anomaly detection (fraud) algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Online Sport Retail Revenue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyze International Debt Statistic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit card applications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning for Uplift\n\n\n\n\n\n\nDuyen Tran\n\n\n\n\n\n\n\n\n\n\n\n\nCG Propensity\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nConjoint\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPentathlon: Next Product to Buy Models\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation Methods\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntuit Upgrade Notebook\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nDuyen Tran\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "A Replication of Karlan and List",
    "section": "",
    "text": "Code\n# import sys;\n# print(sys.executable)"
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the ‚Äúfigures suggest‚Äù comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients ‚Äì what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You‚Äôll then calculate a vector of 10,000 differences, and then you‚Äôll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g.¬†50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the ‚Äúmiddle‚Äù of the distribution or whether it‚Äôs in the ‚Äútail.‚Äù"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "A Replication of Karlan and List",
    "section": "Sub-Header",
    "text": "Sub-Header"
  },
  {
    "objectID": "projects/project1/index.html#description",
    "href": "projects/project1/index.html#description",
    "title": "Project 1",
    "section": "",
    "text": "/usr/local/bin/python3\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows √ó 51 columns"
  },
  {
    "objectID": "projects/project1/index.html#data",
    "href": "projects/project1/index.html#data",
    "title": "A Replication of Karlan and List",
    "section": "Data",
    "text": "Data\n\nDescription\nThe data set contains 50,083 observations and 51 variables. The key variables are as follows:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows √ó 51 columns\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB"
  },
  {
    "objectID": "projects/project1/index.html#balance-test",
    "href": "projects/project1/index.html#balance-test",
    "title": "A Replication of Karlan and List",
    "section": "Balance Test",
    "text": "Balance Test\n\nNumber of Months since last donation\nTest months since last donation to see if the treatment and control group are statistically different at the 95% confidence level\n\nT-test\n\\[H_0: \\mu_{months\\;treatment} = \\mu_{months\\;control} \\] \\[H_a: \\mu_{months\\;treatment} \\neq \\mu_{months\\;control}  \\]\nThe t-value formula is given by:\n\\[\nt_{\\text{value}} = \\frac{\\bar{x}_{\\text{1}} - \\bar{x}_{\\text{2}}}{\\sqrt{\\frac{s_{\\text{1}}^2}{n_{\\text{1}}} + \\frac{s_{\\text{2}}^2}{n_{\\text{2}}}}}\n\\]\nI know there are serverals library that can automatically calculate the t values for us, but as structure, we will define the t_value using the formula in the class slides\n\n\nCode\ndef t_value(treatment, control):\n    # calculate x_bar\n    x_treatment = treatment.mean()\n    x_control = control.mean()\n    #calculate std\n    s_treatment = treatment.std()\n    s_control = control.std()\n    \n    n_treatment = len(treatment)\n    n_control = len(control)\n    \n    t_value = (x_treatment - x_control)/np.sqrt((s_treatment**2/n_treatment) + (s_control**2/n_control))\n    return t_value\n\n\n\n\nCode\nt_value_months = t_value(months_treatment, months_control)\nprint(f\"T_values is {round(t_value_months, 4)}\")\n\n\nT_values is 0.1195\n\n\nThe formula of degree of freedom is:\n\\[\ndf = \\frac{(n_1 - 1)(n_2 - 1)}{(n_2 - 1)(\\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}})^2 + (n_1 - 1)(1 - \\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}})^2}\n\\]\nDefine a fuction to find degree of freedom\n\n\nCode\ndef dof(treatment, control):\n    # calculate x_bar\n    x_treatment = treatment.mean()\n    x_control = control.mean()\n    #calculate std\n    s_treatment = treatment.std()\n    s_control = control.std()\n    \n    n_treatment = len(treatment)\n    n_control = len(control)\n    \n    # Find the degree of freedom\n\n    #Assign the complex formula in the denorminator to b\n    b = (s_treatment**2/n_treatment)/(s_treatment**2/n_treatment+s_control**2/n_control)\n    \n    numerator = (n_treatment-1)*(n_control-1)\n    \n    denominator = (n_control-1)*(b**2) + (n_treatment-1)*((1-b)**2)\n    \n    degree_of_freedom = numerator/denominator\n    return degree_of_freedom\n\n\n\n\nCode\ndof_months = dof(months_treatment, months_control)\nprint(f\"Degree of freedom is {round(dof_months, 2)}\")\n\n\nDegree of freedom is 33394.14\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_months, dof_months))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.905\n\n\nThe independent t-test on the ‚Äòmrm2‚Äô variable (months since last donation) between the treatment and control groups yields a t-statistic of approximately 0.1195 and a p-value of 0.905.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean number of months since the last donation between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'mrm2',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n12.998\n0.094\n138.979\n0.000\n***\n\n\n1\ntreatment\n0.014\n0.115\n0.119\n0.905\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.014 with a standard error of about 0.115.\nThe t-statistic for the treatment coefficient is 0.119, and the p-value is 0.905.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the number of months since last donation (mrm2) between the treatment and control groups at the 95% confidence level.\n\n\n\nHighest previous contribution\nTest the highest previous contribution to see if the treatment and control group are statistically different at 95% confidence level\n\\[H_0: \\mu_{contribution\\;treatment} = \\mu_{contribution\\;control} \\] \\[H_a: \\mu_{contribution\\;treatment} \\neq \\mu_{contribution\\;control}  \\]\n\n\nCode\nhpa_treatment = data.loc[data['treatment'] == 1,'hpa']\nhpa_control = data.loc[data['treatment'] == 0,'hpa']\n\n# Apply the t_values function that we defined above\nt_value_hpa = t_value(hpa_treatment, hpa_control)\nprint(f\"T_value is {round(t_value_hpa, 4)}\")\n\n\nT_value is 0.9704\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_hpa = dof(hpa_treatment, hpa_control)\nprint(f\"Degree of freedom is {round(dof_hpa, 2)}\")\n\n\nDegree of freedom is 35913.89\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_hpa, dof_hpa))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.332\n\n\nThe independent t-test on the ‚Äòhpa‚Äô variable (highest previous contribution) between the treatment and control groups yields a t-statistic of approximately 0.9704 and a p-value of 0.332.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean number of the highest previous contribution between the treatment and control groups.\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'hpa',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n58.960\n0.551\n107.005\n0.000\n***\n\n\n1\ntreatment\n0.637\n0.675\n0.944\n0.345\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.637 with a standard error of about 0.675.\nThe t-statistic for the treatment coefficient is 0.944, and the p-value is 0.345.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the number of highest previous contribution of treatment and control groups at the 95% confidence level.\n\n\n\nPercent already donated in 2005\nTest the percent already donated in 2005 to see if the treatment and control group are statistically different at 95% confidence level\n\\[H_0: \\mu_{percent\\;treatment} = \\mu_{percent\\;control} \\] \\[H_a: \\mu_{percent\\;treatment} \\neq \\mu_{percent\\;control}  \\]\n\nT-test\n\n\nCode\ndormant_treatment = data.loc[data['treatment'] == 1,'dormant']\ndormant_control = data.loc[data['treatment'] == 0,'dormant']\n\n# Apply the t_values function that we defined above\nt_value_dormant = t_value(dormant_treatment, dormant_control)\nprint(f\"T_value is {round(t_value_dormant, 3)}\")\n\n\nT_value is 0.174\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_dormant = dof(dormant_treatment, dormant_control)\nprint(f\"Degree of freedom is {round(dof_dormant, 2)}\")\n\n\nDegree of freedom is 33362.05\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_dormant, dof_dormant))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.862\n\n\nThe independent t-test on the ‚Äòdormant‚Äô variable (Percent already donated in 2005) between the treatment and control groups yields a t-statistic of approximately 0.174 and a p-value of 0.862.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean percent already donated in 2005 between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'dormant',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.523\n0.004\n135.247\n0.000\n***\n\n\n1\ntreatment\n0.001\n0.005\n0.174\n0.862\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.001 with a standard error of about 0.005.\nThe t-statistic for the treatment coefficient is 0.171, and the p-value is 0.862.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the percent already donated in 2005 of treatment and control groups at the 95% confidence level.\n\n\n\nProportion of white people within the zipcode\n\\[H_0: \\mu_{pwhite\\;treatment} = \\mu_{pwhite\\;control} \\] \\[H_a: \\mu_{pwhite\\;treatment} \\neq \\mu_{pwhite\\;control}  \\]\n\nT-test\n\n\nCode\npwhite_treatment = data.loc[data['treatment'] == 1,'pwhite']\npwhite_control = data.loc[data['treatment'] == 0,'pwhite']\n\n# Apply the t_values function that we defined above\nt_value_pwhite = t_value(pwhite_treatment, pwhite_control)\nprint(f\"T_value is {round(t_value_pwhite, 3)}\")\n\n\nT_value is -0.57\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_pwhite = dof(pwhite_treatment, pwhite_control)\nprint(f\"Degree of freedom is {round(dof_pwhite, 2)}\")\n\n\nDegree of freedom is 33185.24\n\n\n\n\nCode\n# Find p-value\np_value = (t.cdf(t_value_pwhite, dof_pwhite))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.569\n\n\nThe independent t-test on the ‚Äòpwhite‚Äô variable (proportion of white people within zipcode) between the treatment and control groups yields a t-statistic of approximately -0.57 and a p-value of 0.569.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the proportion of white people within zipcode between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'pwhite',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.820\n0.001\n616.281\n0.000\n***\n\n\n1\ntreatment\n-0.001\n0.002\n-0.560\n0.575\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately - 0.001 with a standard error of about 0.002.\nThe t-statistic for the treatment coefficient is -0.560, and the p-value is 0.575.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the proportion of white people in the zipcode of treatment and control groups at the 95% confidence level.\nSidenote: I can‚Äôt replicate the results same as table 1 in the paper. Mean for pwhite is:\n\n\nCode\navg_white_treatment = pwhite_treatment.mean()\navg_white_control = pwhite_control.mean()\nprint(f\"The avg pwhite of treatment group is {round(avg_white_treatment,2)}\")\n\nprint(f\"The avg pwhite of treatment group is {round(avg_white_control,2)}\")\n\n\nThe avg pwhite of treatment group is 0.8199999928474426\nThe avg pwhite of treatment group is 0.8199999928474426\n\n\nWhereas the results in table 1 are 0.831 and 0.830, respectively"
  },
  {
    "objectID": "projects/project1/index.html#charitable-contribution-made",
    "href": "projects/project1/index.html#charitable-contribution-made",
    "title": "A Replication of Karlan and List",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\n\nProportion of donation\n\n\nCode\ngave_treatment = data[data['treatment'] == 1]['gave'].mean()\ngave_control = data[data['treatment'] == 0]['gave'].mean()\n\nproportions = [gave_treatment, gave_control]\ngroup_labels = ['Treatment', 'Control']\n\n# Create the bar plot\nplt.bar(group_labels, proportions, color=['blue', 'orange'])\n\n# Add labels and title\nplt.ylabel('Proportion who donated')\nplt.title('Proportion of Donations by Group')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAs visual, we can interpret that the treatment group had a higher response rate for making donations than the control group, which implies that the treatment might have effective in encouraging donations. However, we have to do further statistical analysis to assess the reliability of this observed difference\n\n\nT-test\nFirst of all, let‚Äôs check the similarity between our data and Panel A in table 2A\n\n\nCode\ndonated_treatment = data[data['treatment'] == 1]['gave']\ndonated_control = data[data['treatment'] == 0]['gave']\n\nx_bar_treatment = donated_treatment.mean()\nx_bar_control = donated_control.mean()\nprint(f'The response rate of treatment group panel A is {round(x_bar_treatment, 3)}')\nprint(f'The response rate of treatment group panel A is {round(x_bar_control, 3)}')\n\n\nThe response rate of treatment group panel A is 0.022\nThe response rate of treatment group panel A is 0.018\n\n\n\n\nCode\n# Apply the t_values function that we defined above\nt_value_donated = t_value(donated_treatment, donated_control)\nprint(f\"T_value is {round(t_value_donated, 3)}\")\n\n\nT_value is 3.209\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_donated = dof(donated_treatment, donated_control)\nprint(f\"Degree of freedom is {round(dof_donated, 2)}\")\n\n\nDegree of freedom is 36576.84\n\n\n\n\nCode\n# Find p-value\np_value = (1 - t.cdf(t_value_donated, dof_donated))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.001\n\n\nThe independent t-test on the ‚Äògave‚Äô variable between the treatment and control groups yields a t-statistic of approximately 3.209 and a p-value of 0.001.\nThis p-value is less than the alpha level of 0.05, indicating that there is a statistically significant difference between the treatment and control groups in terms of the proportion of participants who made a donation.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'gave',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\ntreatment\n0.004\n0.001\n3.101\n0.002\n**\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.004 with a standard error of about 0.001.\nThe t-statistic for the treatment coefficient is 3.1, and the p-value is 0.002.\n\nThis p-value is also less than the alpha level of 0.05, suggesting that the treatment has a statistically significant effect on the likelihood of making a donation. The positive coefficient indicates that being in the treatment group is associated with higher odds of giving a donation compared to the control group.\nBoth the t-test and logistic regression demonstrate that there is a significant difference in the donation behavior between the treatment and control groups, with the treatment group showing a higher propensity to give\nIn the context of the experiment, the statistical significance of the treatment group‚Äôs coefficient implies that the treatment has a positive effect on the likelihood that someone will donate. This insight into human behavior suggests that the strategy employed to encourage donations in the treatment group was effective in increasing donation rates.\nIn other words, if the experiment involved sending out letters asking for donations, and the treatment group received letters with a special message or offer not given to the control group, we could interpret these results to mean that the special message or offer motivated more people to donate. The important takeaway here is that small changes in how we ask for donations can have a significant impact on people‚Äôs willingness to contribute to a cause. This aligns with the findings presented in Table 2A, Panel A, which we aimed to confirm with our analysis.\n\n\nProbit Regression\n\n\nCode\nX_linear = sm.add_constant(data['treatment'])  # Add a constant to the independent variable\ny_linear = data['gave'] \n# Fit the Probit regression model using 'treatment' as the predictor and 'gave' as the binary outcome\nprobit_model = sm.Probit(y_linear, X_linear).fit()\n\n# Get the summary of the Probit regression results\nprobit_summary = probit_model.summary()\n# Extract only the regression results table\nprobit_results_table = probit_summary.tables[1]\n\n# To display or print out the table\nprint(probit_results_table)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThe Probit regression results are as follows:\nThe coefficient for treatment is 0.0868, with a standard error of 0.028. The z-statistic for the treatment coefficient is 3.113, and the p-value is 0.002.\nThe coeficient and standard errors do not match between the Table 3 on the paper and the Probit Regression, it matches to the Linear regression results instead\nHowever, p value of both models are the same, which is smaller than 0.05, sugessts that treatment had a positive impact on the probability of giving, which supports the same notion: the treatment apprears to have effectively encouraged more people to donate"
  },
  {
    "objectID": "projects/project1/index.html#differences-between-match-rates",
    "href": "projects/project1/index.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nThe Z score formula of proportion is \\[\nz = \\frac{p_a - p_b}{\\sqrt{\\frac{p_a(1 - p_a)}{n_a} + \\frac{p_b(1 - p_b)}{n_b}}}\n\\]\nSame as the t_value function above, let‚Äôs define z_score function\n\n\nCode\ndef z_score(data1, data2, full_data):\n    p1 = data1.mean()\n    p2 = data2.mean()\n\n    numerator = p1-p2\n    denominator = np.sqrt((p1*(1-p1))/len(data1) + (p2*(1-p2)/len(data2)))\n\n    z_score = numerator / denominator\n    return z_score\n\n\nDouble check the similarity response rate to Table 2 Panel A\n\n\nCode\ngave_ratio1 = data[data['ratio'] == 1]['gave']\ngave_control = data[data['ratio'] == \"Control\"]['gave']\ngave_ratio2 = data[data['ratio'] == 2]['gave']\ngave_ratio3 = data[data['ratio'] == 3]['gave']\n\nprint(f'Response rate of 1:1 matching is {round(gave_ratio1.mean(), 3)}')\nprint(f'Response rate of 2:1 matching is {round(gave_ratio2.mean(), 3)}')\nprint(f'Response rate of 3:1 matching is {round(gave_ratio3.mean(), 3)}')\n\n\nResponse rate of 1:1 matching is 0.021\nResponse rate of 2:1 matching is 0.023\nResponse rate of 3:1 matching is 0.023\n\n\n\nControl vs 1:1 match\n\\[H_0: p_{Control} = p_{1:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{1:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\n# Apply the function that we defined above\nz_score_ratio1 = z_score(gave_ratio1, gave_control, data)\nprint(f'Z score is {round(z_score_ratio1, 3)}')\n\n\nZ score is 1.705\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio1)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.088\n\n\nZ-Value: A z-value of 1.705 indicates that the effect of being in the 1:1 match ratio group is 1.705 standard deviations away from the mean effect of being in the control group. This suggests a positive direction of influence towards increasing the likelihood of donation.\nP-Value: The p-value of 0.088, though close, is above the conventional threshold of 0.05 (95% confidence level). This suggests that while there is some evidence to suggest an effect, it does not meet the usual criteria for statistical significance. Hence, we would not reject the null hypothesis at the 5% significance level, which states there is no difference in donation likelihood between the 1:1 match and control groups\n\n\n\nControl vs 2:1 match\n\\[H_0: p_{Control} = p_{2:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{2:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\nz_score_ratio2 = z_score(gave_ratio2, gave_control, data)\nprint(f'Z score is {round(z_score_ratio2, 4)}')\n\n\nZ score is 2.7397\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio2)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.006\n\n\nZ-Value: A z-value of 2.739 indicates that the effect of being in the 2:1 match ratio group is 2.739 standard deviations away from the mean effect of being in the control group. This represents a stronger and more distinct effect compared to the control, suggesting a significant positive influence on donation likelihood.\nP-Value: The p-value of 0.006 is well below the conventional threshold of 0.05, indicating strong statistical significance. This suggests that we can reject the null hypothesis, or there is significant difference in the likelihood of making a donation between the 2:1 match ratio and control groups.\n\n\n\nControl vs 3:1 match\n\\[H_0: p_{Control} = p_{3:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{3:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\nz_score_ratio3 = z_score(gave_ratio3, gave_control, data)\nprint(f'Z score is {round(z_score_ratio3, 3)}')\n\n\nZ score is 2.793\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio3)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.005\n\n\nZ-Value: A z-value of 2.793 indicates that the effect of being in the 3:1 match ratio group is nearly 2.8 standard deviations away from the mean effect of being in the control group. This shows a strong positive impact of the 3:1 matching offer on the likelihood of making a donation.\nP-Value: The p-value of 0.0052 clearly falls below the typical significance threshold of 0.05, affirming that this result is statistically significant. This strongly suggests rejecting the null hypothesis, or there is significant difference in donation likelihood between the 3:1 match ratio and the control groups.\nAbout the figures suggest\n\nSupport the statement: If the match thresholds for different ratios were relatively low and thus easilu attainable, and the data still shows a significant different between higher ratios and the control, it supports the notion that increasing the match ratio is an effectve strategy. It suggests that the actual ratio, rather than the ease of achieving a match, motivates donors\nContradiction of the statement: The significant effects effects observed at higher match ratios contradict any implication the match ratios do not influence donor behavior. It shows that higher ratios can indeed motivate more donations\n\n\n\n\nRegression on ratio1, ratio2, ratio3\nFirst of all, create a new ratio1 variable\n\n\nCode\ndata['ratio1'] = rsm.ifelse(data.ratio == 1, 1, 0)\n\n\n\nLinear regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'gave',\n    evar = ['ratio1', 'ratio2', 'ratio3']\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\nratio1\n0.003\n0.002\n1.661\n0.097\n.\n\n\n2\nratio2\n0.005\n0.002\n2.744\n0.006\n**\n\n\n3\nratio3\n0.005\n0.002\n2.802\n0.005\n**\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\nratio1 represent 1:1 matching:\n\nCoefficient: 0.003: suggesting that ratio increases the probability of making a donation by 0.3% points\np_value = 0.097\n\nAt the 95% confidence level, the coefficient for 1:1 match ratio is not statistically significant because p_value is greater than 0.05, which means we are less confident that 1:1 match ratio has a real effect on the donation probability in the population\nratio2 represent 2:1 matching:\n\nCoefficient: 0.005, indicates a 0.5% point increase in the probability of making a donation\nP-value: 0.006\n\nThe coefficient for the 2:1 match ratio is statistically significant at the 95% confidence level, as the p-value is well below 0.05, which means we can quite confident that the 2:1 match ratio has a positive effect on the probability of donating\nratio3 represent 3:1 matching:\n\nCoefficient: 0.005, also indicates a 0.5% point increase in the probability of making a donation\nP-value: 0.005\n\nSimilarly, the coefficient for the 3:1 match ratio is statistically significant at the 95% confidence level.\nStatistical Precision\nThe ‚Äòstd.error‚Äô column shows the standard error of each coefficient, which is a measure of the precision of the coefficient estimate. Smaller standard errors indicate more precise estimates. In this table, the standard errors for the treatment levels are the same (0.002), suggesting similar levels of precision across these estimates.\nThe results indicate that only the 2:1 and 3:1 match ratios significantly increase the likelihood of donations compared to the control group at the 95% confidence level. The effects of these higher match ratios are robust, suggesting that they are effective strategies for increasing donation rates.\nThe 1:1 match ratio, while showing a positive effect, does not reach the threshold for statistical significance at this confidence level. This implies that while there might be a slight increase in donation likelihood with a 1:1 match, the evidence is not strong enough to conclusively state this at the 95% confidence level.\n\n\nResponse rate difference\n\n\nCode\nrr_1_1 = data[data['ratio1'] == 1]['gave'].mean()\nrr_2_1 = data[data['ratio2'] == 1]['gave'].mean()\nrr_3_1 = data[data['ratio3'] == 1]['gave'].mean()\n\nrr_diff_11_vs_21 = rr_2_1 - rr_1_1\nrr_diff_21_vs_31 = rr_3_1 - rr_2_1\n\nprint(f'The response rate difference between the 1:1 and 2:1 match ratios {round(rr_diff_11_vs_21, 3)}')\nprint(f'The response rate difference between the 2:1 and 3:1 match ratios {round(rr_diff_21_vs_31, 3)}')\n\n\nThe response rate difference between the 1:1 and 2:1 match ratios 0.002\nThe response rate difference between the 2:1 and 3:1 match ratios 0.0\n\n\nRecall the linear regression result above:\n\n\nCode\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\nratio1\n0.003\n0.002\n1.661\n0.097\n.\n\n\n2\nratio2\n0.005\n0.002\n2.744\n0.006\n**\n\n\n3\nratio3\n0.005\n0.002\n2.802\n0.005\n**\n\n\n\n\n\n\n\n\n\n\nCode\ncoef_ratio1 = 0.003\ncoef_ratio2 = 0.005\ncoef_ratio3 = 0.005\n\ncoef_11_vs_21 = coef_ratio2 - coef_ratio1\ncoef_21_vs_31 = coef_ratio3 - coef_ratio2\n\nprint(f'The response rate difference between the 1:1 and 2:1 match ratios from regression is {round(coef_11_vs_21, 3)}')\nprint(f'The response rate difference between the 2:1 and 3:1 match ratios from regression is {round(coef_21_vs_31, 3)}')\n\n\nThe response rate difference between the 1:1 and 2:1 match ratios from regression is 0.002\nThe response rate difference between the 2:1 and 3:1 match ratios from regression is 0.0\n\n\n1:1 vs 2:1 : The very small difference (0.002) suggests only a marginal improvement in the response rate from the 1:1 and 2:1 match ratio. This indicates that while the 2:1 match ratio might be slightly more effective than 1:1, the difference is minimal\n2:1 vs 3:1 The zero difference suggest that there is no additional benefit in increasing the match ratio from 2:1 to 3:1 regarding the likelihood of donations. This suggests diminishing returns when the match ratio increase beyond 2:1\nThese findings imply that while increasing the match ratio from 1:1 to 2:1 might offer a slight increase in donation likelihood, increasing it further to 3:1 does not yield additional benefits. This could be crucial for organizations in deciding how aggressively to pursue higher matching ratios in their fundraising strategies. The minimal differences suggest that other factors may be more critical in influencing donation behavior than merely adjusting the match ratio."
  },
  {
    "objectID": "projects/project1/index.html#size-of-charitable-contribution",
    "href": "projects/project1/index.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\n\\[H_0: \\mu_{amount\\;treatment} = \\mu_{amount\\;control} \\] \\[H_a: \\mu_{amount\\;treatment} \\neq \\mu_{amount\\;control}  \\]\n\n\nCode\namount_treatment = data.loc[data['treatment'] == 1,'amount']\namount_control = data.loc[data['treatment'] == 0,'amount']\n\n# Apply the t_values function that we defined above\nt_value_amount = t_value(amount_treatment, amount_control)\nprint(f\"T_value is {t_value_amount}\")\n\n\nT_value is 1.9182617883567805\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_amount = dof(amount_treatment, amount_control)\nprint(f\"Degree of freedom is {dof_amount}\")\n\n\nDegree of freedom is 36216.06015374612\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_amount, dof_amount))*2\nprint(f\"P_value is {p_value}\")\n\n\nP_value is 0.055085678607487365\n\n\nThe t-test results show a t-statistic of approximately 1.918 with a p-value of 0.055. This p-value is slightly above the conventional threshold of 0.05, indicating that the difference in average donation amounts between the treatment and control groups is not statistically significant at the 5% level. However, the p-value is close to the threshold, suggesting a potential trend where the treatment group might have higher donation amounts than the control group, though this difference isn‚Äôt strong enough to be considered statistically significant.\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'amount',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.813\n0.067\n12.063\n0.000\n***\n\n\n1\ntreatment\n0.154\n0.083\n1.861\n0.063\n.\n\n\n\n\n\n\n\n\nLinear Regression Results:\nIntercept: 0.8133\nThis suggests that the average donation amount for the control group is approximately 0.813 units.\nCoefficient for Treatment: 0.154\nThis indicates that being in the treatment group is associated with an increase in the donation amount by approximately 0.154 units compared to the control group.\nP-Value for Treatment: 0.063\nThe p-value is slightly above the conventional threshold of 0.05, indicating that the effect of treatment on donation amount is not statistically significant at the 5% level. However, it‚Äôs close, suggesting a potential positive impact of the treatment on donation amounts.\nOverall Interpretation: Both the t-test and the linear regression suggest that the treatment might have a slight positive effect on donation amounts, but this effect does not reach statistical significance. This could imply that while the treatment potentially influences donations positively, the effect is not large or consistent enough across the sample to conclude definitively about its effectiveness. It‚Äôs also possible that other variables not considered here could be influencing the donations, and including those in the model could potentially change these results.\n\n\nLimit only those who made a donation:\n\n\nCode\ndata_donated = data[data['amount'] &gt; 0]\n\n\n\nT-Test\n\n\nCode\ndonated_treatment = data_donated.loc[data_donated['treatment'] == 1,'amount']\ndonated_control = data_donated.loc[data_donated['treatment'] == 0,'amount']\n\n# Apply the t_values function that we defined above\nt_value_donated = t_value(donated_treatment, donated_control)\nprint(f\"T_value is {t_value_donated}\")\n\n\nT_value is -0.5846089783693985\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_donated = dof(donated_treatment, donated_control)\nprint(f\"Degree of freedom is {dof_donated}\")\n\n\nDegree of freedom is 557.4599283248195\n\n\n\n\nCode\n# Find p-value\np_value = t.cdf(t_value_donated, dof_donated)*2\nprint(f\"P_value is {p_value}\")\n\n\nP_value is 0.5590471873269819\n\n\nThe t-statistic of -0.5846 suggests a lower average donation amount in the treatment group compared to the control group among donors, though the difference is minor.\nThe p-value of 0.559 indicates that this difference is not statistically significant. This means we do not have sufficient evidence to conclude that the treatment affects donation amounts among those who donate.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data_donated,\n    rvar = 'amount',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n45.540\n2.423\n18.792\n0.000\n***\n\n\n1\ntreatment\n-1.668\n2.872\n-0.581\n0.561\n\n\n\n\n\n\n\n\n\nTreatment Coefficient: The coefficient for the treatment group is approximately -1.67 with a standard error of about 2.87. This suggests that, on average, the treatment group donated about 1.67 units less than the control group among those who donated.\nStatistical Significance: P-value for the Treatment: The p-value for the treatment effect is 0.561, which is not statistically significant. This implies that there is no strong evidence to suggest that the treatment effect differs from zero in the population of donors.\nThe negatice coefficient for the treatment group suggests that the treatment potentially lead to a decrease in the amount donated compared to the control group among those who donated. However, the lack of statistical significance (p-value &gt; 0.05) means we can not confidently assert that this treatment effect is different from zero in the broader donor population.\nCausal Interpretation:\nWhether the treatment coefficient can be interpreted causally depends on how the treatment was assigned. If the treatment was randomly assigned to participants, then the coefficient could potentially have a causal interpretation. Random assignment would help control for both observed and unobserved confounding variables, allowing us to attribute differences in outcomes directly to the treatment effect.\nWhat Did We Learn?\nThe analysis indicates that among those who chose to donate, the treatment did not significantly increase donation amounts. In fact, the point estimate suggests a slight decrease in donations, but this result is not statistically significant. This finding helps understand the treatment‚Äôs impact specifically on the subset of the population that decides to donate, complementing the broader analysis which includes non-donors.\n\n\nPlots\n\n\nCode\n# Prepare the plots for treatment and control groups who made donations\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\n# Plot for Treatment Group\naxes[0].hist(donated_treatment, bins=30, color='blue', alpha=0.7)\naxes[0].axvline(donated_treatment.mean(), color='red', linestyle='dashed', linewidth=1)\naxes[0].set_title('Treatment Group Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(donated_treatment.mean(), max(axes[0].get_ylim()) * 0.5, f'Average: {donated_treatment.mean():.2f}', color='red')\n\n# Plot for Control Group\naxes[1].hist(donated_control, bins=30, color='green', alpha=0.7)\naxes[1].axvline(donated_control.mean(), color='red', linestyle='dashed', linewidth=1)\naxes[1].set_title('Control Group Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].set_ylabel('Frequency')\naxes[1].text(donated_control.mean(), max(axes[1].get_ylim()) * 0.5, f'Average: {donated_control.mean():.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTreatment Group Donation Amounts:\nThe majority of donations are concentrated in the lower range of amounts, with the frequency decreasing as the amount increases.\nThere‚Äôs a significant frequency at the lowest amount, indicating that many donations are of a small value.\nThe average donation amount in the treatment group is indicated by a vertical dotted red line and is annotated as $43.87.\nControl Group Donation Amounts:\nSimilar to the treatment group, most donations are of a lower amount, with frequency tapering off for higher donation amounts.\nThe distribution of donations appears slightly more spread out than the treatment group, with some higher amounts being more frequent compared to the treatment group.\nThe average donation amount for the control group is slightly higher than the treatment group, marked by a vertical dotted red line, at $45.54.\nInterpretation:\n\nThe control group has a slightly higher average donation amount compared to the treatment group.\nBoth histograms appear right-skewed, which is common in financial data since a large number of small donations are often accompanied by a smaller number of much larger donations"
  },
  {
    "objectID": "projects/project1/index.html#simulation-experiment",
    "href": "projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nLaw of Large Numbers\n\n\nCode\n# Define probabilities for Bernoulli distributions\np_control = 0.018  # Probability for control group\np_treatment = 0.022  # Probability for treatment group\n\n# Simulate 100,000 draws for the control group\ncontrol_samples = np.random.binomial(1, p_control, 100000)\n\n# Simulate 10,000 draws for the treatment group\ntreatment_samples = np.random.binomial(1, p_treatment, 10000)\n\n# Calculate differences for each of the first 10,000 elements in the control sample (to match treatment sample size)\ndifferences = treatment_samples - control_samples[:10000]\n\n# Calculate cumulative averages of differences\ncumulative_averages = np.cumsum(differences) / (np.arange(10000) + 1)\n\n# Plot the cumulative averages of the differences\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_averages, label='Cumulative Average of Differences', color='blue')\nplt.axhline(y=(p_treatment - p_control), color='red', linestyle='dashed', label='True Difference (0.022 - 0.018)')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average of Difference')\nplt.title('Cumulative Average of Differences Between Treatment and Control')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nExplaination of the plot The plot above illustrates the cumulative average of differences in donation behavior between treatment group (with a charitable donation match) and the control group (without a match), based on simulated data.\n\nBlue line: This represents the cumulative average of the difference between the donation outcomes of indiividual in the treatmetn and control groups across 10,000 simulated trial. The differences are calculated for each trial as the outcome of the treatment minus the outcome of the control.\nRed dashed line: This line marks the true difference in means between the probabilities of making a donation in the treatment and control groups, which is p_treatment - p_control = 0.022 - 0.018 = 0.004.\n\nObservations from the plot\nThe blue line, or the cumulative average of differences, fluctuates initially but starts to stabilize and approach the red dashed line as the number of trials increases. This behavior exemplifies the Law of Large Numbers, which states that as the number of trials increases, the sample average will converge to the expected value (in this case, the true difference in means).\nThe plot confirms that with a sufficient number of trials, the cumulative average of the differences in donation behavior between the treatment and control groups approaches the true difference in means. This simulation reinforces our understanding of statistical concepts like the Law of Large Numbers and provides a visual affirmation that with enough data, our estimates can reliably approximate true population parameters. This also supports the validity of using such statistical methods to evaluate the effects of interventions like charitable donation matches\n\n\nCentral Limit Theorem\n\n\nCode\n# Define sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5), sharey=True)\n\n# Simulate the process and plot the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate drawing samples and calculating the means 1000 times\n    sample_means = np.array([np.mean(np.random.binomial(1, p_treatment, sample_size) - \n                                     np.random.binomial(1, p_control, sample_size)) \n                             for _ in range(1000)])\n    \n    # Plot the histogram\n    axes[i].hist(sample_means, bins=30, orientation='horizontal', color='blue', alpha=0.6, edgecolor='black')\n    \n    # Calculate the mean and standard deviation for the normal distribution curve\n    mean_of_sample_means = np.mean(sample_means)\n    std_dev_of_sample_means = np.std(sample_means)\n\n    # Generate values for the normal distribution curve\n    curve_x = np.linspace(mean_of_sample_means - 3 * std_dev_of_sample_means, \n                          mean_of_sample_means + 3 * std_dev_of_sample_means, 100)\n    curve_y = (1 / (std_dev_of_sample_means * np.sqrt(2 * np.pi)) *\n               np.exp(-(curve_x - mean_of_sample_means) ** 2 / (2 * std_dev_of_sample_means ** 2)))\n    \n    # Scale the curve y to match the histogram scale\n    curve_y_scaled = curve_y * max(np.histogram(sample_means, bins=30)[0]) / max(curve_y)\n    \n    # Draw the normal distribution curve as a red line\n    axes[i].plot(curve_y_scaled, curve_x, '-')\n\n    # Add a red dashed line at the true difference\n    axes[i].axhline(y=0.004, color='red', linestyle='dashed', linewidth=2)\n\n    # Set titles and labels\n    axes[i].set_title(f'Sample Size {sample_size}')\n    axes[i].set_xlabel('Frequency' if i == len(sample_sizes) - 1 else '')  # Only add xlabel to the last subplot\n    axes[i].set_ylabel('Average Difference' if i == 0 else '')  # Only add ylabel to the first subplot\n\n# Adjust layout for better fit\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see:\nSample Size 50: There is considerable variability around the true difference, indicating that smaller sample sizes can produce estimates that fluctuate widely.\nSample Size 200: The histogram becomes tighter around the true difference, with less variability than the sample size of 50.\nSample Size 500: Further reduction in variability is observed, and the distribution of averages is centered closely around the true difference.\nSample Size 1000: This histogram shows the least variability, and the average differences are clustering tightly around the true difference, with the center of the distribution aligning closely with the red line.\nLaw of Large Numbers: As the sample size increases, the cumulative average difference becomes more consistent and stable around the true difference. This is a demonstration of the Law of Large Numbers‚Äîthe sample averages converge to the expected value as the sample size grows.\nCentral Limit Theorem: As the sample size increases, the distribution of sample means (average differences in this case) becomes more symmetrical and bell-shaped, which is evidence of the Central Limit Theorem in action. The sample means for larger sample sizes tend to form a normal distribution centered around the true population mean.\nZero in the Distribution: In all histograms, the zero mark is not in the middle of the distribution; it‚Äôs in the left tail. This is because the treatment group has a higher probability of donating than the control group (0.022 vs.¬†0.018), so the true difference is expected to be positive (0.004), not zero."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\ndata = pd.read_csv('blueprinty.csv')\ndata = data.drop('Unnamed: 0', axis=1)\ndata\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows √ó 4 columns\n\n\n\n\nCompare histograms and means of number of patents by customer status:\n\n\nCode\nmean_patents = data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nThis indicate that, on average, customers have a slightly higher number of patents than non-customers. This might suggest that customer are more engaged or invest more patentable innovations, though other factors could also influence these resutls.\n\n\nCode\n# Set up the figure with two subplots for better comparison\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n# Plot histogram for non-customers\nsns.histplot(data[data['iscustomer'] == 0]['patents'], ax=axes[0], color='blue', bins=30, kde=True)\naxes[0].set_title('Patent Distribution for Non-Customers')\naxes[0].set_xlabel('Number of Patents')\naxes[0].set_ylabel('Frequency')\n\n# Plot histogram for customers\nsns.histplot(data[data['iscustomer'] == 1]['patents'], ax=axes[1], color='orange', bins=30, kde=True)\naxes[1].set_title('Patent Distribution for Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Frequency')\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBoth groups show a similar general shape in their distribution, but customers tend to have a slightly higher frequency at higher patent counts, which corroborates the earlier finding that customers on average have more patents.\nThe distribution for both groups is skewed towards lower numbers of patents, with most individuals holding fewer patents.\nThe skewness is slightly more pronounced for customers, which could indicate that while fewer customers might have patents, those who do are likely to have more of them.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\n# Visualize the age distribution across regions by customer status\nplt.figure(figsize=(10, 5))\n\n# Boxplot to show age distribution\nsns.boxplot(x='region', y='age', hue='iscustomer', data=data, palette=['blue', 'orange'])\n\n# Adjust plot labels and title\nplt.title('Age Distribution by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Age')\nplt.legend(title='Is Customer', labels=['Not Customer', 'Customer'])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group by 'iscustomer' and 'region', and get counts and mean age for each group\nregion_customer_summary = data.groupby(['iscustomer', 'region']).agg(\n    count=('region', 'count'),\n    mean_age=('age', 'mean')\n).reset_index()\n\n# Pivot the table to reshape it\npivot_table = region_customer_summary.pivot_table(\n    index='region',\n    columns='iscustomer',\n    values=['count', 'mean_age'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten the columns MultiIndex properly by converting all elements to string before joining\npivot_table.columns = [' '.join(map(str, col)).strip() for col in pivot_table.columns.values]\n\n# Reset index if needed\npivot_table.reset_index(inplace=True)\npivot_table\n\n\n\n\n\n\n\n\n\n\nindex\nregion\ncount 0\ncount 1\nmean_age 0\nmean_age 1\n\n\n\n\n0\n0\nMidwest\n207\n17\n27.596618\n22.852941\n\n\n1\n1\nNortheast\n488\n113\n26.519467\n24.579646\n\n\n2\n2\nNorthwest\n171\n16\n26.532164\n20.812500\n\n\n3\n3\nSouth\n171\n20\n27.464912\n24.950000\n\n\n4\n4\nSouthwest\n266\n31\n25.907895\n24.500000\n\n\n\n\n\n\n\n\nCount and Customer Status:\nNon customer are more numerous across all regions compared to customers\nNorthleast has the highest count of non-customers, while Midwest has the lowest count of customers\nMean Age\nNon-customers tend to have a higher mean age in all regions compared to customers\nThe Northwest region has the youndest average for customer\nThis information suggests that there might be regional preferences or differences in how products and services are adopted between customer groups. Younger individual tend to be customers more in Northwest, indicating possible more innovate or youth-targeted offerings in that region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe Likelihood Function is: \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{\\lambda}\\lambda^{Y_i}}{Y_i!}\n\\]\nThis can also be expressed as: \\[\nL(\\lambda) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}.\n\\]\nThe log-likelihood function is: \\[\n\\log L(\\lambda) = -n\\lambda + \\log(\\lambda) \\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nDefine likelihood and log-likekihood function for the Poisson model\n\n\nCode\ndef poisson_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    likelihood = np.exp(-n * lam) * (lam ** sum_Y) / np.prod([np.math.factorial(y) for y in Y])\n    return likelihood\n\ndef poisson_log_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lam + np.log(lam) * sum_Y - np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return log_likelihood\n\n\nUse function to plot lambda\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\nY = data['patents']\n\n# Define the range for lambda values\nlambda_range = np.linspace(0.01, 10, 1000)  # Start from 0.01 to avoid log(0)\n\n# Calculate the log-likelihood for each lambda in the range\nlog_likelihood_values = [poisson_log_likelihood(lam, Y) for lam in lambda_range]\n\n# Plot the results\nplt.figure(figsize=(8, 3))\nplt.plot(lambda_range, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('Lambda (Œª)')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Observed Patent Counts Across Lambda Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) , denoted as \\(\\lambda_{MLE}\\), we take the derivative of the log-likelihood with respect to \\(\\lambda\\) and set it equal to zero. The derivative is:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this derivative equal to zero for maximization gives:\n\\[\n0 = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSimplifying and solving for ( ) yields:\n\\[\n\\lambda_{MLE} = \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i\n\\]\nSo the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\), which is intuitive since for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\nCode\nlambda_mle = Y.mean()\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\nFind the MLE by optimizing your likelihood function with scipy.optimize in Python.\n\n\nCode\nfrom scipy.optimize import minimize\n\n# We define the negative log-likelihood function for the Poisson distribution\ndef negative_poisson_log_likelihood(lam, Y):\n    if lam &lt;= 0:  # Avoid log(0) for lam=0\n        return np.inf\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    # The negative sign is important because we want to maximize the log-likelihood,\n    # which is equivalent to minimizing its negative.\n    neg_log_likelihood = n * lam - np.log(lam) * sum_Y + np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return neg_log_likelihood\n\n# Initial guess for lambda can be the sample mean\ninitial_guess = [Y.mean()]\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=negative_poisson_log_likelihood,\n    x0=initial_guess,\n    args=(Y,),\n    method='L-BFGS-B', # This optimization method allows for bounding the solution\n    bounds=[(1e-5, None)] # Lambda must be greater than 0\n)\n\n# Extract the MLE for lambda from the result\nlambda_mle = result.x[0] if result.success else None\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe Log-Likelihood function is:\n\\[\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( -e^{X_i' \\beta} + Y_i X_i' \\beta - \\log(Y_i!) \\right)\n\\]\n\n\nCode\n# Define Log-Likelihood function\ndef possion_regression_function(beta, Y, X):\n   # Calculate lambda for each observation\n    linear_predictor = X.dot(beta)\n\n    lambda_i = np.exp(linear_predictor)\n    \n    # Calculate the log-likelihood\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - np.array([np.log(np.math.factorial(y)) for y in Y]))\n    \n    return log_likelihood\n\n\nUse the fuction to find the MLE vector and the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nfrom scipy.stats import poisson\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age'] ** 2\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Select the numeric predictors\nnumeric_features = ['age', 'age_squared']\n\n# Fit and transform the features\ndata[numeric_features] = scaler.fit_transform(data[numeric_features])\n\nregions = pd.get_dummies(data['region'], drop_first=True)\n\nX = pd.concat([pd.Series(1, index=data.index, name='Intercept'), data[['age', 'age_squared', 'iscustomer']], regions], axis=1)\n\ndef convert_boolean_columns_to_int(df):\n    for column in df.columns:\n        if df[column].dtype == bool:\n            # Convert Boolean column to int\n            df[column] = df[column].astype(int)\n    return df\n\nX = convert_boolean_columns_to_int(X)\n\nX_column_names = X.columns\n\nY = data['patents']\n\nX_glm = X.copy()\nY_glm = Y.copy()\n\nX = X.values\nY = Y.values\n\n\ndef possion_neg_regression_function(beta, Y, X):\n    # Calculate lambda for each observation\n    linear_predictor = np.dot(X, beta)\n    lambda_i = np.exp(linear_predictor)\n    lambda_i = np.clip(lambda_i, 1e-10, np.inf)  \n\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i)\n    return neg_log_likelihood\n\n# initial_beta = np.zeros(X.shape[1])\n# initial_beta = np.random.normal(loc=0, scale=1, size=X.shape[1])\ninitial_beta = np.zeros(X.shape[1])\ninitial_beta[0] = np.log(np.mean(Y))\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=possion_neg_regression_function,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n)\n\n\n\n\nCode\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_i = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_i)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix = inv(hessian_matrix)\n\nstandard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Display the coefficients and their standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': np.round(result.x, 4),\n    'Standard Error': np.round(standard_errors, 3)\n}, index=X_column_names)\nprint(coefficients_table)\n\n\n             Coefficient  Standard Error\nIntercept         1.2154           0.036\nage               1.0464           0.100\nage_squared      -1.1408           0.102\niscustomer        0.1181           0.039\nNortheast         0.0986           0.042\nNorthwest        -0.0201           0.054\nSouth             0.0572           0.053\nSouthwest         0.0514           0.047\n\n\nCheck results using sm.GLM() function.\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson())\nresults = model.fit()\n\n# Get the summary of the results\nglm_summary = results.summary()\n# Extract only the regression results table\nglm_results_table = glm_summary.tables[1]\n\n# To display or print out the table\nprint(glm_results_table)\n\n\n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       1.2154      0.036     33.368      0.000       1.144       1.287\nage             1.0465      0.100     10.414      0.000       0.850       1.243\nage_squared    -1.1408      0.102    -11.131      0.000      -1.342      -0.940\niscustomer      0.1181      0.039      3.035      0.002       0.042       0.194\nNortheast       0.0986      0.042      2.347      0.019       0.016       0.181\nNorthwest      -0.0201      0.054     -0.374      0.709      -0.126       0.085\nSouth           0.0572      0.053      1.085      0.278      -0.046       0.160\nSouthwest       0.0513      0.047      1.088      0.277      -0.041       0.144\n===============================================================================\n\n\nInterpret the results:\n\nIntercept: This value represents the baseline log-odds of patent success when all other predictor variables are held at zero.\nage: For every one-year increase in age, the log-odds of patent success increase by 0.1445. This suggests a positive relationship between the age of the patent application (or applicant) and the likelihood of success.With p &lt; 0.001, this variable indicates a strong positive relationship with the outcome as age increases, up to a point (due to the quadratic term).\nage squared:The negative coefficient for age squared indicates a diminishing return effect; as age increases, its positive impact on patent success starts to decrease. This typically suggests a peak point beyond which additional years in age reduce the likelihood of success. Also with p &lt; 0.001, it confirms the non-linear relationship where increasing age has diminishing returns on the log odds of the outcome.\niscustomer: Being a customer is associated with an increase in the log-odds of patent success by 0.1181 compared to non-customers. This effect is statistically significant and suggests that customers of Blueprinty might have a higher likelihood of patent success. This variable is statistically significant (p = 0.002), showing that being a customer positively influences the outcome.\nRegional Effects:\n\n\nNortheast: Indicates a positive effect on patent success in the Northeast compared to the baseline region (the one dropped during dummy coding).\nNorthwest: Suggests a slight negative effect on patent success compared to the baseline, but this may not be statistically significant.\nSouth and Southwest: Both regions show a positive effect on patent success, though the effects are small and the confidence around these estimates might overlap with zero, suggesting limited statistical significance.\n\nConclusions:\n\nAge: There‚Äôs a clear positive relationship between age and patent success, which peaks and then starts to decline as indicated by the quadratic term (age squared). This could reflect that mid-career individuals or entities are most successful in patent applications, possibly due to optimal combinations of experience and active engagement in their fields.\nCustomer Status: Being a customer of Blueprinty has a positive impact on patent success. This suggests that the services or products provided by Blueprinty are effective in enhancing the success rate of patents for their customers.\nRegional Variations: There are some regional differences in patent success rates, with the Northeast and Southern regions showing a positive impact relative to the baseline region."
  },
  {
    "objectID": "projects/project2/index.html#blueprinty-case-study",
    "href": "projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\ndata = pd.read_csv('blueprinty.csv')\ndata = data.drop('Unnamed: 0', axis=1)\ndata\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows √ó 4 columns\n\n\n\n\nCompare histograms and means of number of patents by customer status:\n\n\nCode\nmean_patents = data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nThis indicate that, on average, customers have a slightly higher number of patents than non-customers. This might suggest that customer are more engaged or invest more patentable innovations, though other factors could also influence these resutls.\n\n\nCode\n# Set up the figure with two subplots for better comparison\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n# Plot histogram for non-customers\nsns.histplot(data[data['iscustomer'] == 0]['patents'], ax=axes[0], color='blue', bins=30, kde=True)\naxes[0].set_title('Patent Distribution for Non-Customers')\naxes[0].set_xlabel('Number of Patents')\naxes[0].set_ylabel('Frequency')\n\n# Plot histogram for customers\nsns.histplot(data[data['iscustomer'] == 1]['patents'], ax=axes[1], color='orange', bins=30, kde=True)\naxes[1].set_title('Patent Distribution for Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Frequency')\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBoth groups show a similar general shape in their distribution, but customers tend to have a slightly higher frequency at higher patent counts, which corroborates the earlier finding that customers on average have more patents.\nThe distribution for both groups is skewed towards lower numbers of patents, with most individuals holding fewer patents.\nThe skewness is slightly more pronounced for customers, which could indicate that while fewer customers might have patents, those who do are likely to have more of them.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\n# Visualize the age distribution across regions by customer status\nplt.figure(figsize=(10, 5))\n\n# Boxplot to show age distribution\nsns.boxplot(x='region', y='age', hue='iscustomer', data=data, palette=['blue', 'orange'])\n\n# Adjust plot labels and title\nplt.title('Age Distribution by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Age')\nplt.legend(title='Is Customer', labels=['Not Customer', 'Customer'])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group by 'iscustomer' and 'region', and get counts and mean age for each group\nregion_customer_summary = data.groupby(['iscustomer', 'region']).agg(\n    count=('region', 'count'),\n    mean_age=('age', 'mean')\n).reset_index()\n\n# Pivot the table to reshape it\npivot_table = region_customer_summary.pivot_table(\n    index='region',\n    columns='iscustomer',\n    values=['count', 'mean_age'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten the columns MultiIndex properly by converting all elements to string before joining\npivot_table.columns = [' '.join(map(str, col)).strip() for col in pivot_table.columns.values]\n\n# Reset index if needed\npivot_table.reset_index(inplace=True)\npivot_table\n\n\n\n\n\n\n\n\n\n\nindex\nregion\ncount 0\ncount 1\nmean_age 0\nmean_age 1\n\n\n\n\n0\n0\nMidwest\n207\n17\n27.596618\n22.852941\n\n\n1\n1\nNortheast\n488\n113\n26.519467\n24.579646\n\n\n2\n2\nNorthwest\n171\n16\n26.532164\n20.812500\n\n\n3\n3\nSouth\n171\n20\n27.464912\n24.950000\n\n\n4\n4\nSouthwest\n266\n31\n25.907895\n24.500000\n\n\n\n\n\n\n\n\nCount and Customer Status:\nNon customer are more numerous across all regions compared to customers\nNorthleast has the highest count of non-customers, while Midwest has the lowest count of customers\nMean Age\nNon-customers tend to have a higher mean age in all regions compared to customers\nThe Northwest region has the youndest average for customer\nThis information suggests that there might be regional preferences or differences in how products and services are adopted between customer groups. Younger individual tend to be customers more in Northwest, indicating possible more innovate or youth-targeted offerings in that region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe Likelihood Function is: \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{\\lambda}\\lambda^{Y_i}}{Y_i!}\n\\]\nThis can also be expressed as: \\[\nL(\\lambda) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}.\n\\]\nThe log-likelihood function is: \\[\n\\log L(\\lambda) = -n\\lambda + \\log(\\lambda) \\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nDefine likelihood and log-likekihood function for the Poisson model\n\n\nCode\ndef poisson_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    likelihood = np.exp(-n * lam) * (lam ** sum_Y) / np.prod([np.math.factorial(y) for y in Y])\n    return likelihood\n\ndef poisson_log_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lam + np.log(lam) * sum_Y - np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return log_likelihood\n\n\nUse function to plot lambda\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\nY = data['patents']\n\n# Define the range for lambda values\nlambda_range = np.linspace(0.01, 10, 1000)  # Start from 0.01 to avoid log(0)\n\n# Calculate the log-likelihood for each lambda in the range\nlog_likelihood_values = [poisson_log_likelihood(lam, Y) for lam in lambda_range]\n\n# Plot the results\nplt.figure(figsize=(8, 3))\nplt.plot(lambda_range, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('Lambda (Œª)')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Observed Patent Counts Across Lambda Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) , denoted as \\(\\lambda_{MLE}\\), we take the derivative of the log-likelihood with respect to \\(\\lambda\\) and set it equal to zero. The derivative is:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this derivative equal to zero for maximization gives:\n\\[\n0 = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSimplifying and solving for ( ) yields:\n\\[\n\\lambda_{MLE} = \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i\n\\]\nSo the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\), which is intuitive since for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\nCode\nlambda_mle = Y.mean()\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\nFind the MLE by optimizing your likelihood function with scipy.optimize in Python.\n\n\nCode\nfrom scipy.optimize import minimize\n\n# We define the negative log-likelihood function for the Poisson distribution\ndef negative_poisson_log_likelihood(lam, Y):\n    if lam &lt;= 0:  # Avoid log(0) for lam=0\n        return np.inf\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    # The negative sign is important because we want to maximize the log-likelihood,\n    # which is equivalent to minimizing its negative.\n    neg_log_likelihood = n * lam - np.log(lam) * sum_Y + np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return neg_log_likelihood\n\n# Initial guess for lambda can be the sample mean\ninitial_guess = [Y.mean()]\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=negative_poisson_log_likelihood,\n    x0=initial_guess,\n    args=(Y,),\n    method='L-BFGS-B', # This optimization method allows for bounding the solution\n    bounds=[(1e-5, None)] # Lambda must be greater than 0\n)\n\n# Extract the MLE for lambda from the result\nlambda_mle = result.x[0] if result.success else None\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe Log-Likelihood function is:\n\\[\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( -e^{X_i' \\beta} + Y_i X_i' \\beta - \\log(Y_i!) \\right)\n\\]\n\n\nCode\n# Define Log-Likelihood function\ndef possion_regression_function(beta, Y, X):\n   # Calculate lambda for each observation\n    linear_predictor = X.dot(beta)\n\n    lambda_i = np.exp(linear_predictor)\n    \n    # Calculate the log-likelihood\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - np.array([np.log(np.math.factorial(y)) for y in Y]))\n    \n    return log_likelihood\n\n\nUse the fuction to find the MLE vector and the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nfrom scipy.stats import poisson\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age'] ** 2\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Select the numeric predictors\nnumeric_features = ['age', 'age_squared']\n\n# Fit and transform the features\ndata[numeric_features] = scaler.fit_transform(data[numeric_features])\n\nregions = pd.get_dummies(data['region'], drop_first=True)\n\nX = pd.concat([pd.Series(1, index=data.index, name='Intercept'), data[['age', 'age_squared', 'iscustomer']], regions], axis=1)\n\ndef convert_boolean_columns_to_int(df):\n    for column in df.columns:\n        if df[column].dtype == bool:\n            # Convert Boolean column to int\n            df[column] = df[column].astype(int)\n    return df\n\nX = convert_boolean_columns_to_int(X)\n\nX_column_names = X.columns\n\nY = data['patents']\n\nX_glm = X.copy()\nY_glm = Y.copy()\n\nX = X.values\nY = Y.values\n\n\ndef possion_neg_regression_function(beta, Y, X):\n    # Calculate lambda for each observation\n    linear_predictor = np.dot(X, beta)\n    lambda_i = np.exp(linear_predictor)\n    lambda_i = np.clip(lambda_i, 1e-10, np.inf)  \n\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i)\n    return neg_log_likelihood\n\n# initial_beta = np.zeros(X.shape[1])\n# initial_beta = np.random.normal(loc=0, scale=1, size=X.shape[1])\ninitial_beta = np.zeros(X.shape[1])\ninitial_beta[0] = np.log(np.mean(Y))\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=possion_neg_regression_function,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n)\n\n\n\n\nCode\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_i = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_i)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix = inv(hessian_matrix)\n\nstandard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Display the coefficients and their standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': np.round(result.x, 4),\n    'Standard Error': np.round(standard_errors, 3)\n}, index=X_column_names)\nprint(coefficients_table)\n\n\n             Coefficient  Standard Error\nIntercept         1.2154           0.036\nage               1.0464           0.100\nage_squared      -1.1408           0.102\niscustomer        0.1181           0.039\nNortheast         0.0986           0.042\nNorthwest        -0.0201           0.054\nSouth             0.0572           0.053\nSouthwest         0.0514           0.047\n\n\nCheck results using sm.GLM() function.\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson())\nresults = model.fit()\n\n# Get the summary of the results\nglm_summary = results.summary()\n# Extract only the regression results table\nglm_results_table = glm_summary.tables[1]\n\n# To display or print out the table\nprint(glm_results_table)\n\n\n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       1.2154      0.036     33.368      0.000       1.144       1.287\nage             1.0465      0.100     10.414      0.000       0.850       1.243\nage_squared    -1.1408      0.102    -11.131      0.000      -1.342      -0.940\niscustomer      0.1181      0.039      3.035      0.002       0.042       0.194\nNortheast       0.0986      0.042      2.347      0.019       0.016       0.181\nNorthwest      -0.0201      0.054     -0.374      0.709      -0.126       0.085\nSouth           0.0572      0.053      1.085      0.278      -0.046       0.160\nSouthwest       0.0513      0.047      1.088      0.277      -0.041       0.144\n===============================================================================\n\n\nInterpret the results:\n\nIntercept: This value represents the baseline log-odds of patent success when all other predictor variables are held at zero.\nage: For every one-year increase in age, the log-odds of patent success increase by 0.1445. This suggests a positive relationship between the age of the patent application (or applicant) and the likelihood of success.With p &lt; 0.001, this variable indicates a strong positive relationship with the outcome as age increases, up to a point (due to the quadratic term).\nage squared:The negative coefficient for age squared indicates a diminishing return effect; as age increases, its positive impact on patent success starts to decrease. This typically suggests a peak point beyond which additional years in age reduce the likelihood of success. Also with p &lt; 0.001, it confirms the non-linear relationship where increasing age has diminishing returns on the log odds of the outcome.\niscustomer: Being a customer is associated with an increase in the log-odds of patent success by 0.1181 compared to non-customers. This effect is statistically significant and suggests that customers of Blueprinty might have a higher likelihood of patent success. This variable is statistically significant (p = 0.002), showing that being a customer positively influences the outcome.\nRegional Effects:\n\n\nNortheast: Indicates a positive effect on patent success in the Northeast compared to the baseline region (the one dropped during dummy coding).\nNorthwest: Suggests a slight negative effect on patent success compared to the baseline, but this may not be statistically significant.\nSouth and Southwest: Both regions show a positive effect on patent success, though the effects are small and the confidence around these estimates might overlap with zero, suggesting limited statistical significance.\n\nConclusions:\n\nAge: There‚Äôs a clear positive relationship between age and patent success, which peaks and then starts to decline as indicated by the quadratic term (age squared). This could reflect that mid-career individuals or entities are most successful in patent applications, possibly due to optimal combinations of experience and active engagement in their fields.\nCustomer Status: Being a customer of Blueprinty has a positive impact on patent success. This suggests that the services or products provided by Blueprinty are effective in enhancing the success rate of patents for their customers.\nRegional Variations: There are some regional differences in patent success rates, with the Northeast and Southern regions showing a positive impact relative to the baseline region."
  },
  {
    "objectID": "projects/project2/index.html#airbnb-case-study",
    "href": "projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nBasics exploratory data analysis\n\n\nCode\nairbnb = pd.read_csv('airbnb.csv')\nairbnb = airbnb.drop('Unnamed: 0', axis=1)\nairbnb.head()\n\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nSummary statistics for numerical variables\n\n\nCode\nnumerical_summary = airbnb.describe()\nnumerical_summary\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nCount of missing values for each column\n\n\nCode\nmissing_values = airbnb.isnull().sum()\nmissing_values\n\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nHere‚Äôs what we‚Äôve learned from the dataset:\nMissing Values:\nbathrooms: 160 missing values.\nbedrooms: 76 missing values.\nreview_scores_cleanliness, review_scores_location,review_scores_value: Over 10,000 missing values each. This represents about 25% of the data, which is significant.\nhost_since: 35 missing values.\nLet‚Äôs start by handling the missing values based on the strategy described. We will impute the missing values for bathrooms and bedrooms with the median and decide on the review scores next\nImpute missing values for ‚Äòbathrooms‚Äô and ‚Äòbedrooms‚Äô with their respective medians\n\n\nCode\nairbnb['bathrooms'].fillna(airbnb['bathrooms'].median(), inplace=True)\nairbnb['bedrooms'].fillna(airbnb['bedrooms'].median(), inplace=True)\n# Checking updated missing values status\nupdated_missing_values = airbnb.isnull().sum()\nupdated_missing_values\n\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                        0\nbedrooms                         0\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nReview Scores: Given the high proportion of missing data (around 25%), filling them with the median or mean could introduce bias, especially if the missingness is not random. An alternative approach is to create a binary indicator variable for each of these scores, which will indicate whether the score was originally missing. This way, we retain all listings in our analysis and potentially capture some information about why scores might be missing (e.g., newer listings might not have scores yet).\nImpute missing review scores with the median\n\n\nCode\n# Create binary indicators for missing review scores\nairbnb['cleanliness_missing'] = airbnb['review_scores_cleanliness'].isnull().astype(int)\nairbnb['location_missing'] = airbnb['review_scores_location'].isnull().astype(int)\nairbnb['value_missing'] = airbnb['review_scores_value'].isnull().astype(int)\n\n# Impute missing review scores with the median\nairbnb['review_scores_cleanliness'].fillna(airbnb['review_scores_cleanliness'].median(), inplace=True)\nairbnb['review_scores_location'].fillna(airbnb['review_scores_location'].median(), inplace=True)\nairbnb['review_scores_value'].fillna(airbnb['review_scores_value'].median(), inplace=True)\n\n# Check if all missing values are addressed\nfinal_missing_values_check = airbnb.isnull().sum()\nfinal_missing_values_check\n\n\nid                            0\ndays                          0\nlast_scraped                  0\nhost_since                   35\nroom_type                     0\nbathrooms                     0\nbedrooms                      0\nprice                         0\nnumber_of_reviews             0\nreview_scores_cleanliness     0\nreview_scores_location        0\nreview_scores_value           0\ninstant_bookable              0\ncleanliness_missing           0\nlocation_missing              0\nvalue_missing                 0\ndtype: int64\n\n\nAll the missing values in the review scores have been addressed, and we now have binary indicators for whether the original scores were missing. The host_since column still has 35 missing values, but since it is not directly used in our model, we won‚Äôt focus on imputing it right now.\n\n\nCode\n# Setting up the visualization style\nsns.set(style=\"whitegrid\")\n\n# Plotting the distribution of number of reviews\nplt.figure(figsize=(8, 4))\nsns.histplot(airbnb['number_of_reviews'], bins=50, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet‚Äôs take a closer eye on 80% population of data\n\n\nCode\n# Calculate the 80th percentile\npercentile_80 = airbnb['number_of_reviews'].quantile(0.8)\n\n# Plotting the distribution of number of reviews limited to the 80th percentile\nplt.figure(figsize=(8, 4))\nsns.histplot(airbnb['number_of_reviews'], bins=50, kde=True, binrange=(0, percentile_80))\nplt.title('Distribution of Number of Reviews (80% of Data)')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.xlim(0, percentile_80)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe distribution of the number of reviews is highly skewed to the right, with most listings having a relatively low number of reviews and a few listings having a very high number\n\n\nCode\n# Visualization of number of reviews by room type\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='room_type', y='number_of_reviews', data=airbnb)\nplt.title('Number of Reviews by Room Type')\nplt.xlabel('Room Type')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Scatter plot for number of reviews vs. price\nplt.figure(figsize=(8, 4))\nsns.scatterplot(x='price', y='number_of_reviews', data=airbnb)\nplt.title('Number of Reviews vs. Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.xscale('log')  # Using logarithmic scale due to wide range of prices\nplt.show()\n\n# Scatter plots for number of reviews vs. review scores\nfig, axes = plt.subplots(1, 3, figsize=(8, 4))\nscore_vars = ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\ntitles = ['Cleanliness', 'Location', 'Value']\n\nfor ax, var, title in zip(axes, score_vars, titles):\n    sns.scatterplot(ax=ax, x=airbnb[var], y=airbnb['number_of_reviews'])\n    ax.set_title(f'Number of Reviews vs. {title}')\n    ax.set_xlabel(title)\n    ax.set_ylabel('Number of Reviews')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot interpretation:\nRoom Type:\nThe number of reviews varies by room type, with entire homes/apartments generally receiving more reviews than private or shared rooms. This might reflect a preference or higher turnover in these types of listings.\nPrice:\nThe relationship between price and number of reviews is not linear, suggesting that very high or very low priced listings might have fewer reviews. The logarithmic scale on price helps in visualizing this across a wide range of values.\nReview Scores:\nThere isn‚Äôt a clear trend in the scatter plots between review scores and the number of reviews, indicating that while scores may affect guest satisfaction, they do not necessarily correlate directly with the frequency of bookings (as measured by reviews). There might be a slight increase in reviews with higher scores for cleanliness and value.\n\n\nBuild Poisson Regression Model\n\n\nCode\nfrom patsy import dmatrices\n# Convert variables to the same time interval\n\nairbnb['last_scraped'] = pd.to_datetime(airbnb['last_scraped'])\nairbnb['host_since'] = pd.to_datetime(airbnb['host_since'])\n\n# Calculate the duration in years that each listing has been active\nairbnb['duration_years'] = (airbnb['last_scraped'] - airbnb['host_since']).dt.days / 365.25\n\n# Compute reviews per year\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['duration_years']\n\n# Handle cases where duration_years is zero to avoid division by zero\nairbnb['reviews_per_year'].fillna(0, inplace=True)\n\n\n# Encoding categorical variables using patsy (for statsmodels compatibility)\nformula = \"\"\"reviews_per_year ~ room_type + bathrooms + bedrooms + price +\n             review_scores_cleanliness + review_scores_location +\n             review_scores_value + cleanliness_missing + location_missing + value_missing\"\"\"\n\n# Prepare the design matrices for regression\ny, X = dmatrices(formula, airbnb, return_type='dataframe')\n\n# Fit a Poisson regression model using the same training data\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\npoisson_summary = poisson_results.summary()\n\n# Extract only the regression results table\npoisson_results_table = poisson_summary.tables[1]\n\n# To display or print out the table\nprint(poisson_results_table)\n\n\n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     2.1560      0.026     84.372      0.000       2.106       2.206\nroom_type[T.Private room]     0.0338      0.004      7.737      0.000       0.025       0.042\nroom_type[T.Shared room]      0.0871      0.012      7.525      0.000       0.064       0.110\nbathrooms                     0.0105      0.006      1.903      0.057      -0.000       0.021\nbedrooms                      0.0811      0.003     25.960      0.000       0.075       0.087\nprice                        -0.0002   1.86e-05    -12.370      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1249      0.002     53.488      0.000       0.120       0.129\nreview_scores_location       -0.0711      0.003    -28.236      0.000      -0.076      -0.066\nreview_scores_value          -0.0577      0.003    -20.285      0.000      -0.063      -0.052\ncleanliness_missing          -2.2750      0.122    -18.625      0.000      -2.514      -2.036\nlocation_missing             -1.1305      0.148     -7.651      0.000      -1.420      -0.841\nvalue_missing                -1.4111      0.145     -9.734      0.000      -1.695      -1.127\n=============================================================================================\n\n\n\n\nBuild Binomial Regression Model\n\n\nCode\n# # Encoding categorical variables using patsy (for statsmodels compatibility)\n# formula = \"\"\"number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n#              review_scores_cleanliness + review_scores_location +\n#              review_scores_value + cleanliness_missing + location_missing + value_missing\"\"\"\n\n# Fit the negative binomial regression model\nnb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial())\nnb_results = nb_model.fit()\n\n# Obtain the summary of the model\nnb_summary = nb_results.summary()\n\n# Extract only the regression results table\nnb_results_table = nb_summary.tables[1]\n\n# Display or print out the table\nprint(nb_results_table)\n\n\n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     2.3226      0.081     28.540      0.000       2.163       2.482\nroom_type[T.Private room]     0.0337      0.013      2.573      0.010       0.008       0.059\nroom_type[T.Shared room]      0.0954      0.037      2.588      0.010       0.023       0.168\nbathrooms                     0.0106      0.017      0.611      0.541      -0.023       0.045\nbedrooms                      0.0762      0.010      7.796      0.000       0.057       0.095\nprice                        -0.0002   4.12e-05     -4.826      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1825      0.007     26.014      0.000       0.169       0.196\nreview_scores_location       -0.0949      0.008    -11.657      0.000      -0.111      -0.079\nreview_scores_value          -0.1087      0.009    -11.981      0.000      -0.126      -0.091\ncleanliness_missing          -2.5131      0.165    -15.223      0.000      -2.837      -2.190\nlocation_missing             -0.6405      0.260     -2.466      0.014      -1.150      -0.131\nvalue_missing                -1.6547      0.258     -6.421      0.000      -2.160      -1.150\n=============================================================================================\n\n\nCoeficient Interpret for both model:\n\nIntercep: This is log of expected count of reviews when all other variables are zero. Since this scenario isn‚Äôt realistic, it primarily serves as baseline for the model\nroom_type\n\n\nPrivate room: listings that are private rooms have slightly more reviews compared to entire homes/apartments, holding other factors constant. The effect is relatively small\nShare room: Shared rooms are expected to have more reviews than the baseline category. Shared rooms show a stronger positive association with the number of reviews compared to private rooms.\n\n\nbathrooms: More bathrooms are associated with more reviews, indicating that listings with more bathrooms might be more frequently booked or reviewed.\nbedrooms: More bedrooms are associated with an increase in the expected count of reviews, suggesting that larger properties might attract more bookings and thus more reviews\nprice: A higher price is slightly negatively associated with the number of reviews. This small coeficient suggests that price inceases might slightly reduce the likelihood of getting reviewed.\nReview scores:\n\n\ncleanliness: higher clealiness scrores are positively associated with more reviews, indicating that cleanser listings are more likely to receive reviews.\nlocation: Surprisingly, better location scores are associated with fewer reviews. This might reflex a complex interaction with other factors not captured in the model or the properties in desiable locations might not meet all guest expectations or that guests in such locations review less frequently.\nvalue: Better value scores are also negatively associated with the number of reviews, which might suggest that guest have higher expectations that aren‚Äôt met as often they rate the value highly.\n\n\nMissing Indicators:\n\n\ncleanliness missing: Listing missing cleanliness scores have significantly fewer reviews, possibly indicating newer or less popular listings.\nlocation missing: Similarly, listings missing location scores have fewer reviews\nvalue missing: Listing missing value scores also have fewer reviews."
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization for all projects",
    "section": "",
    "text": "AB testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning for Uplift\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS-mobile Customer Churn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "visualization/Possion/index.html",
    "href": "visualization/Possion/index.html",
    "title": "Poisson Regression Model",
    "section": "",
    "text": "Airbnb Case Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty Case Study"
  },
  {
    "objectID": "visualization/Possion/index.html#column",
    "href": "visualization/Possion/index.html#column",
    "title": "Poisson Regression Model",
    "section": "",
    "text": "Airbnb Case Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty Case Study"
  },
  {
    "objectID": "visualization/A:Btesting/index.html",
    "href": "visualization/A:Btesting/index.html",
    "title": "AB testing",
    "section": "",
    "text": "This is a replication project doing A/B testing"
  },
  {
    "objectID": "visualization/A:Btesting/index.html#row",
    "href": "visualization/A:Btesting/index.html#row",
    "title": "AB testing",
    "section": "",
    "text": "This is a replication project doing A/B testing"
  },
  {
    "objectID": "visualization/A:Btesting/index.html#row-1",
    "href": "visualization/A:Btesting/index.html#row-1",
    "title": "AB testing",
    "section": "Row",
    "text": "Row"
  },
  {
    "objectID": "visualization/A:Btesting/index.html#row-2",
    "href": "visualization/A:Btesting/index.html#row-2",
    "title": "AB testing",
    "section": "Row",
    "text": "Row\n\nColumn\n\n\n\n\n\n\n\n\n\n\n\nColumn"
  },
  {
    "objectID": "visualization/A:Btesting/index.html#row-3",
    "href": "visualization/A:Btesting/index.html#row-3",
    "title": "AB testing",
    "section": "Row",
    "text": "Row"
  },
  {
    "objectID": "visualization/cg/index.html",
    "href": "visualization/cg/index.html",
    "title": "Machine Learning for Uplift",
    "section": "",
    "text": "Uplift and Propensity using Logistic Regression"
  },
  {
    "objectID": "visualization/cg/index.html#column",
    "href": "visualization/cg/index.html#column",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport pyrsm as rsm\nfrom sklearn.model_selection import GridSearchCV\n\ncg_organic_control = pd.read_parquet(\"/Users/duyentran/Desktop/UCSD_Study/MGTA495-MA/quarto_website/projects/cg/cg_organic_control.parquet\").reset_index(drop=True)\n\nrsm.md(\"/Users/duyentran/Desktop/UCSD_Study/MGTA495-MA/quarto_website/projects/cg/cg_organic_control_description.md\")\n\n## loading the treatment data\ncg_ad_treatment = pd.read_parquet(\"/Users/duyentran/Desktop/UCSD_Study/MGTA495-MA/quarto_website/projects/cg/cg_ad_treatment.parquet\").reset_index(drop=True)\n\nrsm.md(\"/Users/duyentran/Desktop/UCSD_Study/MGTA495-MA/quarto_website/projects/cg/cg_ad_treatment_description.md\")\n\n# Load the ad random data\"\ncg_ad_random = pd.read_parquet(\"/Users/duyentran/Desktop/UCSD_Study/MGTA495-MA/quarto_website/projects/cg/cg_ad_random.parquet\")\n\n# a. Add \"ad\" to cg_ad_random and set its value to 1 for all rows\ncg_ad_random[\"ad\"] = 1\n\n# b. Add \"ad\" to cg_organic_control and set its value to 0 for all rows\ncg_organic_control[\"ad\"] = 0\n\n# c. Create a stacked dataset by combining cg_ad_random and cg_organic_control\ncg_rct_stacked = pd.concat([cg_ad_random, cg_organic_control], axis=0)\n\ncg_rct_stacked['converted_yes']= rsm.ifelse(\n    cg_rct_stacked.converted == \"yes\", 1, rsm.ifelse(cg_rct_stacked.converted == \"no\", 0, np.nan)\n)\n\n\n# d. Create a training variable\ncg_rct_stacked['training'] = rsm.model.make_train(\n    data=cg_rct_stacked, test_size=0.3, strat_var=['converted', 'ad'], random_state = 1234)\n\n# Assign variables to evar\nevar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ]\n\nlr_treatment = rsm.model.logistic(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n)\n\nlr_control = rsm.model.logistic(\n    data={'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar\n)\n\ncg_rct_stacked[\"pred_treatment\"] = lr_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control\"] = lr_control.predict(cg_rct_stacked)[\"prediction\"]\n\ncg_rct_stacked[\"uplift_score\"] = (\n    cg_rct_stacked.pred_treatment - cg_rct_stacked.pred_control\n)\n\nuplift_tab = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)\n\n\nCreative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)\n\n\n\n\nCreative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)\nrnd_30k: Dummy variable that randomly selects 30K customers (1) and the remaining 90K (0)"
  },
  {
    "objectID": "visualization/cg/index.html#creative-gaming",
    "href": "visualization/cg/index.html#creative-gaming",
    "title": "Machine Learning for Uplift",
    "section": "Creative gaming",
    "text": "Creative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)"
  },
  {
    "objectID": "visualization/cg/index.html#creative-gaming-1",
    "href": "visualization/cg/index.html#creative-gaming-1",
    "title": "Machine Learning for Uplift",
    "section": "Creative gaming",
    "text": "Creative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)\nrnd_30k: Dummy variable that randomly selects 30K customers (1) and the remaining 90K (0)"
  },
  {
    "objectID": "visualization/cg/index.html#column-1",
    "href": "visualization/cg/index.html#column-1",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-2",
    "href": "visualization/cg/index.html#column-2",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nprice = 14.99\ncost = 1.5\n\ntarget_row = uplift_tab[uplift_tab['cum_prop'] &lt;= 0.25].iloc[-1]\n\n# Define the function to calculate the profit\ndef prof_calc(data, price = 14.99, cost = 1.5):\n    # Given variables\n    target_customers = 30000\n    target_prop = 30000 / 120000\n\n    # Calculate the scale factor\n    scale_factor = 120000 / 9000\n\n    # Calculate the expected incremental customers and profits\n    target_row = data[data['cum_prop'] &lt;= target_prop].iloc[-1]\n    profit = (price*target_row['incremental_resp'] - cost * target_row['T_n']) * scale_factor\n    return profit\n\nuplift_profit_logit = prof_calc(uplift_tab, 14.99, 1.5)\nuplift_profit_logit\n\npropensity_tab = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \n    \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)"
  },
  {
    "objectID": "visualization/cg/index.html#column-3",
    "href": "visualization/cg/index.html#column-3",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment\", \"uplift_score\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment\", \"uplift_score\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-4",
    "href": "visualization/cg/index.html#column-4",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nclf_treatment = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    hidden_layer_sizes = (4, 2),\n    alpha = 0.0001\n)\n\nclf_control = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    hidden_layer_sizes = (4,2),\n    alpha = 1\n)\n\ncg_rct_stacked[\"pred_treatment_nn\"] = clf_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control_nn\"] = clf_control.predict(cg_rct_stacked)[\"prediction\"]\n\ncg_rct_stacked[\"uplift_score_nn\"] = (\n    cg_rct_stacked.pred_treatment_nn - cg_rct_stacked.pred_control_nn\n)\n\nuplift_tab_nn = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-5",
    "href": "visualization/cg/index.html#column-5",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-6",
    "href": "visualization/cg/index.html#column-6",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nprop_tab_nn = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_nn\", \"ad\", 1, qnt = 20\n)\nprop_tab_nn\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_nn\", \"uplift_score_nn\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-7",
    "href": "visualization/cg/index.html#column-7",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_nn\", \"uplift_score_nn\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-8",
    "href": "visualization/cg/index.html#column-8",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nrf_treatment = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    max_features = 0.25,\n    n_estimators = 1000\n)\n\nrf_control = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    max_features = 0.25,\n    n_estimators = 1000\n)\n\n# Predictions\ncg_rct_stacked[\"pred_treatment_rf\"] = rf_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control_rf\"] = rf_control.predict(cg_rct_stacked)[\"prediction\"]\n\ncg_rct_stacked[\"uplift_score_rf\"] = (\n    cg_rct_stacked.pred_treatment_rf - cg_rct_stacked.pred_control_rf\n)\n\nuplift_tab_rf = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-9",
    "href": "visualization/cg/index.html#column-9",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-10",
    "href": "visualization/cg/index.html#column-10",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nuplift_profit_rf = prof_calc(uplift_tab_rf, 14.99, 1.5)\n\nprop_tab_rf = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_rf\", \"ad\", 1, qnt = 20\n)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_rf\", \"uplift_score_rf\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-11",
    "href": "visualization/cg/index.html#column-11",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_rf\", \"uplift_score_rf\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-12",
    "href": "visualization/cg/index.html#column-12",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nimport xgboost as xgb\n# Create X_train, X_test, y_train, y_test for treatment group\nX_train_treatment = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 1) & (cg_rct_stacked[\"ad\"] == 1), evar]\ny_train_treatment = cg_rct_stacked.query(\"training == 1 & ad == 1\").converted_yes\n\nX_test_treatment = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 0) & (cg_rct_stacked[\"ad\"] == 1), evar]\ny_test_treatment = cg_rct_stacked.query(\"training == 0 & ad == 1\").converted_yes\n\n# Setup model\nxgbc_treatment = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_treatment.fit(X_train_treatment, y_train_treatment)\n\n# Set up and fit GridSearchCV\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n    'min_child_weight': [1, 2, 3, 4, 5, 6]\n}\n\nxgbc_cv_treatment = GridSearchCV(xgbc_treatment, param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=5)\nxgbc_cv_treatment.fit(X_train_treatment, y_train_treatment)\n\n# Retrieve the best parameters and retrain the model\nbest_params_treatment = xgbc_cv_treatment.best_params_\nxgbc_treatment = xgb.XGBClassifier(**best_params_treatment, use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_treatment.fit(X_train_treatment, y_train_treatment)\n\n# Create X_train, X_test, y_train, y_test for control group\nX_train_control = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 1) & (cg_rct_stacked[\"ad\"] == 0), evar]\ny_train_control = cg_rct_stacked.query(\"training == 1 & ad == 0\").converted_yes\n\nX_test_control = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 0) & (cg_rct_stacked[\"ad\"] == 0), evar]\ny_test_control = cg_rct_stacked.query(\"training == 0 & ad == 0\").converted_yes\n\n# Use the same param_grid as the treatment group\nxgbc_control = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\n\n# Set up and fit GridSearchCV\nxgbc_cv_control = GridSearchCV(xgbc_control, param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=5)\nxgbc_cv_control.fit(X_train_control, y_train_control)\n\n# Retrieve the best parameters and retrain the model\nbest_params_control = xgbc_cv_control.best_params_\nxgbc_control = xgb.XGBClassifier(**best_params_control, use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_control.fit(X_train_control, y_train_control)\n\nX_full = cg_rct_stacked[evar]\ncg_rct_stacked[\"pred_treatment_xgb\"] = xgbc_treatment.predict_proba(X_full)[:, 1]\ncg_rct_stacked[\"pred_control_xgb\"] = xgbc_control.predict_proba(X_full)[:, 1]\n\ncg_rct_stacked[\"uplift_score_xgb\"] = (\n    cg_rct_stacked.pred_treatment_xgb - cg_rct_stacked.pred_control_xgb\n)\n\nuplift_tab_xgb = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)\n\n\nFitting 5 folds for each of 192 candidates, totalling 960 fits\nFitting 5 folds for each of 192 candidates, totalling 960 fits\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.758 total time=   6.8s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.753 total time=   2.1s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.773 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.762 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.771 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.758 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.753 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.773 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.768 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.774 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.769 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.779 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.774 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.759 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.773 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.774 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.775 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.764 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.781 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.776 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.774 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.763 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.775 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.777 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.778 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.779 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.777 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.777 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.766 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.780 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.777 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.781 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.764 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.779 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.773 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.772 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.780 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.764 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.780 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.775 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.772 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.759 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.776 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.776 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.772 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.781 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.761 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.780 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.768 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.780 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.779 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.783 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.775 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.767 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.777 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.785 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.780 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.768 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.768 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.778 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.778 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.779 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.768 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.788 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.783 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.780 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.788 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.769 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.782 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.779 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.783 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.789 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.783 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.780 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.781 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.783 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.769 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.780 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.784 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.768 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.786 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.784 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.771 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.787 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.782 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.769 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.787 total time=   0.8s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.780 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.785 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.768 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.788 total time=   0.7s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.771 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.771 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.783 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.769 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.788 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.774 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.777 total time=   0.6s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.768 total time=   0.7s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.785 total time=   1.4s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.778 total time=   2.0s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.771 total time=   0.9s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.781 total time=   1.0s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.768 total time=   1.0s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.779 total time=   0.9s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.778 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.775 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.784 total time=   0.3s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.778 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.778 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.786 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.786 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.782 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.780 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.783 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.783 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.782 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.779 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.767 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.788 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.779 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.777 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.778 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.770 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.777 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.777 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.769 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.785 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.781 total time=   0.7s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.774 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.777 total time=   0.6s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.763 total time=   0.7s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.784 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.775 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.768 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.782 total time=   0.6s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.770 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.789 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.770 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.768 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.774 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.766 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.781 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.775 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.768 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.771 total time=   6.6s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.758 total time=   1.8s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.753 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.773 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.753 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.771 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.758 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.753 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.768 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.775 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.769 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.757 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.779 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.768 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.768 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.759 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.758 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.776 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.758 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.773 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.763 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.777 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.777 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.767 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.782 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.778 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.777 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.765 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.779 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.773 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.779 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.765 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.778 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.777 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.778 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.769 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.777 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.764 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.779 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.777 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.773 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.784 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.761 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.772 total time=   0.9s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.771 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.772 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.778 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.763 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.779 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.776 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.781 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.783 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.783 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.781 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.778 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.784 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.775 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.768 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.785 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.785 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.784 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.785 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.768 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.786 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.783 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.782 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.769 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.769 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.784 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.789 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.783 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.783 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.770 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.770 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.789 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.785 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.783 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.789 total time=   0.7s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.790 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.780 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.780 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.784 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.770 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.784 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.770 total time=   0.6s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.781 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.769 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.786 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.779 total time=   0.7s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.775 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.775 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.778 total time=   0.6s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.768 total time=   0.7s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.789 total time=   0.6s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.774 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.783 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.786 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.769 total time=   0.7s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.771 total time=   0.7s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.782 total time=   2.1s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.768 total time=   1.4s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.788 total time=   0.9s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.775 total time=   1.1s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.773 total time=   1.0s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.784 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.770 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.784 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.786 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.776 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.768 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.785 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.782 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.780 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.781 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.778 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.772 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.785 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.780 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.780 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.767 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.786 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.780 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.779 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.780 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.771 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.787 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.781 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.785 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.767 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.786 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.784 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.769 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.782 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.769 total time=   0.7s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.783 total time=   0.6s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.778 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.773 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.781 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.768 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.781 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.768 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.766 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.777 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.766 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.786 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.777 total time=   0.7s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.760 total time=   0.9s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.773 total time=   6.9s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.771 total time=   1.6s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.771 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.753 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.762 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.762 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.762 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.769 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.768 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.774 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.772 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.774 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.772 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.773 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.774 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.758 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.781 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.773 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.775 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.763 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.777 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.768 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.779 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.777 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.778 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.764 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.780 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.773 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.780 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.764 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.778 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.776 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.775 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.763 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.779 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.778 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.775 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.783 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.763 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.780 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.768 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.772 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.776 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.761 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.779 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.775 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.774 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.783 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.778 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.775 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.768 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.781 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.781 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.781 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.785 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.778 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.785 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.779 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.786 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.782 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.782 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.782 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.782 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.789 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.782 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.782 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.789 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.779 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.785 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.770 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.788 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.785 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.780 total time=   0.7s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.784 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.772 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.776 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.777 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.783 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.769 total time=   0.6s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.788 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.775 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.785 total time=   0.8s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.787 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.770 total time=   0.6s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.782 total time=   0.7s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.768 total time=   0.6s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.789 total time=   0.6s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.774 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.786 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.766 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.780 total time=   0.7s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.770 total time=   1.2s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.772 total time=   1.9s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.782 total time=   1.2s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.767 total time=   1.1s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.785 total time=   1.1s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.785 total time=   1.0s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.784 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.785 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.769 total time=   0.6s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.782 total time=   0.3s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.780 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.780 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.769 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.787 total time=   0.3s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.783 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.781 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.781 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.772 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.788 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.780 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.778 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.778 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.786 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.784 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.769 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.789 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.782 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.775 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.783 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.769 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.786 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.773 total time=   0.7s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.770 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.782 total time=   0.6s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.767 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.788 total time=   0.6s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.779 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.767 total time=   0.9s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.765 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.767 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.780 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.775 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.767 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.781 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.766 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.775 total time=   0.8s[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.762 total time=   7.1s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.762 total time=   1.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.758 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.758 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.773 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.773 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.771 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.768 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.768 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.769 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.769 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.772 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.757 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.780 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.772 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.775 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.764 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.781 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.763 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.781 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.774 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.776 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.766 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.765 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.778 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.771 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.777 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.765 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.778 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.775 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.781 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.762 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.778 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.772 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.772 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.781 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.763 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.779 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.777 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.769 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.775 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.759 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.777 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.776 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.772 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.782 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.760 total time=   0.6s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.776 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.768 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.776 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.768 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.779 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.784 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.780 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.768 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.786 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.778 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.785 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.780 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.783 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.783 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.768 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.788 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.784 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.783 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.781 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.768 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.789 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.780 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.779 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.783 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.786 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.779 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.780 total time=   0.7s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.771 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.781 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.779 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.785 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.773 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.775 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.772 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.784 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.769 total time=   0.8s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.788 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.783 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.773 total time=   0.7s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.776 total time=   0.8s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.780 total time=   0.6s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.770 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.786 total time=   0.6s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.778 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.779 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.767 total time=   0.6s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.779 total time=   0.8s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.766 total time=   1.8s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.784 total time=   1.7s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.778 total time=   1.0s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.776 total time=   1.2s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.786 total time=   1.1s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.767 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.784 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.776 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.782 total time=   0.3s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.777 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.787 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.781 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.767 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.786 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.784 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.782 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.777 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.787 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.783 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.777 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.778 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.768 total time=   0.6s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.770 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.782 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.780 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.779 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.767 total time=   0.5s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.788 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.783 total time=   0.5s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.781 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.769 total time=   0.7s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.789 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.774 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.771 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.784 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.767 total time=   0.6s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.782 total time=   0.6s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.780 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.770 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.776 total time=   0.9s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.778 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.781 total time=   0.8s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.776 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.770 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.781 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.764 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.783 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.766 total time=   0.9s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.767 total time=   1.0s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.777 total time=   0.9s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.767 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.782 total time=   1.0s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.777 total time=   1.0s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.763 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.782 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.774 total time=   1.1s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.762 total time=   1.0s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.782 total time=   0.9s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.761 total time=   1.1s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.770 total time=   1.1s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.763 total time=   1.2s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.768 total time=   0.9s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.769 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.763 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.782 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.767 total time=   0.8s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.782 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.783 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=2;, score=0.781 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.765 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.783 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.783 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.772 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.772 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.766 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.784 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.774 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.779 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.764 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.783 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.771 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.769 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.775 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.766 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.780 total time=   0.5s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.774 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.759 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.760 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.756 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.755 total time=   0.5s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.769 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.772 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.774 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.761 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.771 total time=   0.5s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.758 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.761 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.775 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.752 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.777 total time=   0.8s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.761 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.766 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.746 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.753 total time=   0.6s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.763 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.755 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.775 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.769 total time=   0.9s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.761 total time=   0.8s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.751 total time=   0.8s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.749 total time=   0.7s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.738 total time=   0.8s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.766 total time=   0.8s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.755 total time=   1.0s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.763 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.756 total time=   0.7s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.742 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.755 total time=   1.2s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.754 total time=   1.1s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.759 total time=   1.0s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.761 total time=   0.9s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.750 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.760 total time=   0.9s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.748 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.861 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.860 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.850 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.859 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.868 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.850 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.861 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.863 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.870 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.878 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.870 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.863 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.870 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.877 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.860 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.880 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.868 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.876 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.882 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.868 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.880 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.869 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.879 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.883 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.869 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.877 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.870 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.882 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.869 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.880 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.874 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.878 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.885 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.885 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.867 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.879 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.872 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.871 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.873 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.877 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.871 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.879 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.874 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.879 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.886 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.869 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.880 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.873 total time=   1.0s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.874 total time=   1.1s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.883 total time=   1.0s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.870 total time=   1.0s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.872 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.878 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.874 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.870 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.880 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.874 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.881 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.880 total time=   0.7s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.885 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.884 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.887 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.893 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.870 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.871 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.885 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.874 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.887 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.894 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.893 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.876 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.888 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.894 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.886 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.876 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.889 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.892 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.877 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.877 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.884 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.876 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.889 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.892 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.879 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.878 total time=   0.7s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.880 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.877 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.886 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.891 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.875 total time=   0.6s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.872 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.878 total time=   0.6s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.770 total time=   0.9s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.762 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.783 total time=   1.0s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.769 total time=   0.9s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.763 total time=   1.0s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.776 total time=   1.0s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.766 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.784 total time=   0.9s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.758 total time=   1.2s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.762 total time=   1.3s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.776 total time=   1.0s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.761 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.776 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.766 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.762 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.776 total time=   0.2s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.780 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=2;, score=0.778 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=2;, score=0.767 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.782 total time=   0.2s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.779 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.782 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.776 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.767 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.783 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.772 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.775 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.765 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.782 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.781 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.764 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.775 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.760 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.781 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.779 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.773 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.780 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.784 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.766 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.764 total time=   0.6s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.767 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.777 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.773 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.765 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.758 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.772 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.751 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.772 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.770 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.764 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.755 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.759 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.757 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.757 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.753 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.768 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.762 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.747 total time=   0.6s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.762 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.760 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.771 total time=   0.8s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.746 total time=   0.9s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.754 total time=   0.8s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.763 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.751 total time=   0.9s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.764 total time=   1.1s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.753 total time=   0.7s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.743 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.759 total time=   0.9s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.747 total time=   1.2s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.760 total time=   1.0s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.743 total time=   1.0s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.743 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.761 total time=   0.8s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.752 total time=   0.9s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.769 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.850 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.868 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.849 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.868 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.861 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.850 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.850 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.870 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.878 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.859 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.859 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.870 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.863 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.863 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.876 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.883 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.880 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.868 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.876 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.882 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.868 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.881 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.871 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.879 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.883 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.869 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.877 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.870 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.879 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.884 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.868 total time=   0.9s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.867 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.873 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.879 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.878 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.884 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.870 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.869 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.878 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.870 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.877 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.869 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.868 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.884 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.868 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.871 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.879 total time=   0.9s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.880 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.873 total time=   1.1s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.875 total time=   1.0s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.883 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.884 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.872 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.868 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.869 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.881 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.875 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.871 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.883 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.872 total time=   0.7s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.893 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.871 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.884 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.874 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.885 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.892 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.892 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.875 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.885 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.886 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.876 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.888 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.894 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.875 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.884 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.877 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.890 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.891 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.877 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.885 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.877 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.887 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.891 total time=   0.6s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.874 total time=   0.6s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.873 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.879 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.877 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.886 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.891 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.874 total time=   0.6s\n\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.766 total time=   1.1s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.764 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.777 total time=   1.1s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.761 total time=   0.9s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.780 total time=   1.0s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.776 total time=   1.0s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.758 total time=   1.3s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.769 total time=   1.1s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.763 total time=   1.2s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.774 total time=   0.9s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.771 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.766 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.777 total time=   0.8s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.760 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=2;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.781 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.766 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.771 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.783 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.777 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.776 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.766 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.782 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.775 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.775 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.758 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.779 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.775 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.768 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.773 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.763 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.766 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.781 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.765 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.764 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.771 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.759 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.778 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.773 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.755 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.772 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.765 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.768 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.755 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.767 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.777 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.772 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.754 total time=   0.6s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.761 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.752 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.769 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.754 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.754 total time=   0.9s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.745 total time=   0.8s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.753 total time=   1.0s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.750 total time=   0.8s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.756 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.765 total time=   0.9s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.760 total time=   0.8s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.744 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.773 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.751 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.760 total time=   1.0s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.751 total time=   1.1s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.750 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.760 total time=   1.0s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.751 total time=   1.0s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.755 total time=   0.9s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.760 total time=   1.0s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.850 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.859 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.868 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.860 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.850 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.861 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.868 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.850 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.871 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.863 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.870 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.877 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.859 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.870 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.870 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.878 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.867 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.867 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.880 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.869 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.876 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.882 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.868 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.880 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.871 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.879 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.883 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.869 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.877 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.870 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.879 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.885 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.879 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.879 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.870 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.885 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.867 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.885 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.880 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.882 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.870 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.879 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.871 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.877 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.885 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.870 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.879 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.873 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.873 total time=   0.9s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.882 total time=   1.1s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.870 total time=   1.0s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.882 total time=   1.0s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.878 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.864 total time=   0.9s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.875 total time=   0.9s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.877 total time=   0.9s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.872 total time=   0.8s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.879 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.878 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.875 total time=   0.7s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.886 total time=   0.7s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.884 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.874 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.886 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.893 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.870 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.884 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.883 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.875 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.888 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.892 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.874 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.875 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.886 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.876 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.888 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.891 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.884 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.877 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.890 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.876 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.885 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.876 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.889 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.889 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.879 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.887 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.890 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.874 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.881 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.884 total time=   0.6s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.887 total time=   0.7s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.873 total time=   0.7s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.877 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.874 total time=   0.5s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.773 total time=   1.0s\n[CV 2/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.763 total time=   0.9s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.767 total time=   1.2s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.763 total time=   1.2s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.778 total time=   1.2s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.765 total time=   0.9s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.765 total time=   0.8s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.778 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.763 total time=   0.8s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.779 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.768 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=2;, score=0.782 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.780 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.776 total time=   0.1s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.778 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.781 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.775 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.761 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.783 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.779 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.765 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.785 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.774 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.767 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.773 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.760 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.777 total time=   0.5s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.774 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.773 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.768 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.760 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.780 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.763 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.762 total time=   0.6s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.769 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.762 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.784 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.760 total time=   0.5s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.765 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.767 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.762 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.769 total time=   0.8s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.765 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.769 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.751 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.753 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.763 total time=   0.6s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.751 total time=   0.6s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.768 total time=   0.6s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.762 total time=   0.9s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.763 total time=   0.8s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.752 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.760 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.768 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.752 total time=   0.8s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.748 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.763 total time=   0.9s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.754 total time=   0.7s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.770 total time=   0.7s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.748 total time=   0.9s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.751 total time=   1.2s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.759 total time=   1.0s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.749 total time=   0.9s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.757 total time=   1.1s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.770 total time=   0.9s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.745 total time=   0.9s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.859 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.850 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.859 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.849 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.850 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.859 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.859 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.868 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.858 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.870 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.863 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.870 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.877 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.859 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.871 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.868 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.876 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.882 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.868 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.880 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.868 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.876 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.882 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.869 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.878 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.871 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.879 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.878 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.883 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.870 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.880 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.874 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.879 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.866 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.878 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.871 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.880 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.882 total time=   0.7s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.871 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.876 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.882 total time=   0.9s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.883 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.884 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.880 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.875 total time=   0.7s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.868 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.878 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.869 total time=   1.0s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.871 total time=   0.9s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.880 total time=   1.1s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.876 total time=   1.0s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.877 total time=   0.9s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.886 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.868 total time=   0.9s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.878 total time=   0.9s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.874 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.869 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.883 total time=   0.8s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.871 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.883 total time=   0.7s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.874 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.871 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.874 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.887 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.892 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.885 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.873 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.886 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.872 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.874 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.888 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.875 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.874 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.886 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.876 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.888 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.893 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.877 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.885 total time=   0.5s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.877 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.888 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.891 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.892 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.876 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.874 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.880 total time=   0.6s\n[CV 1/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.887 total time=   0.5s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.890 total time=   0.4s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.873 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.881 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.876 total time=   0.6s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.874 total time=   0.5s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.886 total time=   0.7s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.889 total time=   0.8s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.889 total time=   0.7s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.891 total time=   0.7s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.874 total time=   0.6s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.879 total time=   0.5s"
  },
  {
    "objectID": "visualization/cg/index.html#column-13",
    "href": "visualization/cg/index.html#column-13",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-14",
    "href": "visualization/cg/index.html#column-14",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nuplift_profit_xgb = prof_calc(uplift_tab_xgb, 14.99, 1.5)\n\npropensity_tab_xgb = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)\n\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)"
  },
  {
    "objectID": "visualization/cg/index.html#column-15",
    "href": "visualization/cg/index.html#column-15",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \n    \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)"
  },
  {
    "objectID": "visualization/cg/index.html#column-16",
    "href": "visualization/cg/index.html#column-16",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_xgb\", \"uplift_score_xgb\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "visualization/cg/index.html#column-17",
    "href": "visualization/cg/index.html#column-17",
    "title": "Machine Learning for Uplift",
    "section": "Column",
    "text": "Column\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_xgb\", \"uplift_score_xgb\"],\n    \"ad\",\n    1, qnt = 20\n)"
  },
  {
    "objectID": "projects/cg-uplift/index.html",
    "href": "projects/cg-uplift/index.html",
    "title": "Machine Learning for Uplift",
    "section": "",
    "text": "Logistic Regression\nRandom Forest\nXGBoost\nNeural Network\n\nThis project will provide insights into the relative strengths and weaknesses of each modeling approach in the context of direct marketing, with a particular focus on maximizing the return on investment for marketing campaigns."
  },
  {
    "objectID": "projects/cg-uplift/index.html#creative-gaming",
    "href": "projects/cg-uplift/index.html#creative-gaming",
    "title": "Machine Learning for Uplift",
    "section": "Creative gaming",
    "text": "Creative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)"
  },
  {
    "objectID": "projects/cg-uplift/index.html#creative-gaming-1",
    "href": "projects/cg-uplift/index.html#creative-gaming-1",
    "title": "Machine Learning for Uplift",
    "section": "Creative gaming",
    "text": "Creative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)\nrnd_30k: Dummy variable that randomly selects 30K customers (1) and the remaining 90K (0)"
  },
  {
    "objectID": "projects/cg-uplift/index.html#part-i-uplift-modeling-using-machine-learning",
    "href": "projects/cg-uplift/index.html#part-i-uplift-modeling-using-machine-learning",
    "title": "Machine Learning for Uplift",
    "section": "Part I: Uplift Modeling Using Machine Learning",
    "text": "Part I: Uplift Modeling Using Machine Learning\n\n1. Prepare the data\n\n\nCode\n# a. Add \"ad\" to cg_ad_random and set its value to 1 for all rows\ncg_ad_random[\"ad\"] = 1\n\n# b. Add \"ad\" to cg_organic_control and set its value to 0 for all rows\ncg_organic_control[\"ad\"] = 0\n\n# c. Create a stacked dataset by combining cg_ad_random and cg_organic_control\ncg_rct_stacked = pd.concat([cg_ad_random, cg_organic_control], axis=0)\n\ncg_rct_stacked['converted_yes']= rsm.ifelse(\n    cg_rct_stacked.converted == \"yes\", 1, rsm.ifelse(cg_rct_stacked.converted == \"no\", 0, np.nan)\n)\n\n# d. Create a training variable\ncg_rct_stacked['training'] = rsm.model.make_train(\n    data=cg_rct_stacked, test_size=0.3, strat_var=['converted', 'ad'], random_state = 1234)\n\n# Check the proportions of the training variable\ncg_rct_stacked.training.value_counts(normalize=True)\n\n\ntraining\n1.0    0.7\n0.0    0.3\nName: proportion, dtype: float64\n\n\n\n\nCode\npd.crosstab(cg_rct_stacked.converted, [cg_rct_stacked.ad, cg_rct_stacked.training]).round(2)\n\n\n\n\n\n\n\n\n\nad\n0\n1\n\n\ntraining\n0.0\n1.0\n0.0\n1.0\n\n\nconverted\n\n\n\n\n\n\n\n\nyes\n512\n1194\n1174\n2739\n\n\nno\n8488\n19806\n7826\n18261\n\n\n\n\n\n\n\n\n\n\nCode\nlen(cg_rct_stacked.query('training == 0 & ad == 0'))\n\n\n9000\n\n\n\n\nCode\nlen(cg_rct_stacked.query('training == 0 & ad == 1'))\n\n\n9000\n\n\n\n\nCode\n# e. Check if the proportion of the training variable is similar across the ad and control groups\npd.crosstab(\n    cg_rct_stacked.converted, [cg_rct_stacked.ad, cg_rct_stacked.training], normalize=\"columns\"\n).round(3)\n\n\n\n\n\n\n\n\n\nad\n0\n1\n\n\ntraining\n0.0\n1.0\n0.0\n1.0\n\n\nconverted\n\n\n\n\n\n\n\n\nyes\n0.057\n0.057\n0.13\n0.13\n\n\nno\n0.943\n0.943\n0.87\n0.87\n\n\n\n\n\n\n\n\n\n\nUsing Logistic Regression\n\n2. Train an uplift model\n\n\nCode\n# Assign variables to evar\nevar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ]\n\n\n\n\nCode\nlr_treatment = rsm.model.logistic(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n)\nlr_treatment.summary()\n\n\nLogistic regression (GLM)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nNull hyp.: There is no effect of x on converted\nAlt. hyp.: There is an effect of x on converted\n\n                                OR     OR%  coefficient  std.error  z.value p.value     \nIntercept                    0.030  -97.0%        -3.52      0.122  -28.987  &lt; .001  ***\nAcquiredSpaceship[yes]       1.088    8.8%         0.08      0.049    1.732   0.083    .\nAcquiredIonWeapon[yes]       0.917   -8.3%        -0.09      0.164   -0.533   0.594     \nPurchasedCoinPackSmall[yes]  1.045    4.5%         0.04      0.046    0.960   0.337     \nPurchasedCoinPackLarge[yes]  1.211   21.1%         0.19      0.049    3.930  &lt; .001  ***\nUserConsole[yes]             0.945   -5.5%        -0.06      0.058   -0.979   0.328     \nUserHasOldOS[yes]            0.799  -20.1%        -0.22      0.081   -2.752   0.006   **\nGameLevel                    1.059    5.9%         0.06      0.009    6.399  &lt; .001  ***\nNumGameDays                  1.015    1.5%         0.02      0.004    4.264  &lt; .001  ***\nNumGameDays4Plus             1.011    1.1%         0.01      0.006    1.674   0.094    .\nNumInGameMessagesSent        1.000    0.0%         0.00      0.000    0.205   0.838     \nNumFriends                   1.002    0.2%         0.00      0.000    9.255  &lt; .001  ***\nNumFriendRequestIgnored      1.000   -0.0%        -0.00      0.001   -0.484   0.628     \nNumSpaceHeroBadges           1.028    2.8%         0.03      0.009    2.968   0.003   **\nTimesLostSpaceship           0.993   -0.7%        -0.01      0.002   -2.964   0.003   **\nTimesKilled                  1.001    0.1%         0.00      0.006    0.201   0.841     \nTimesCaptain                 1.005    0.5%         0.01      0.002    2.054    0.04    *\nTimesNavigator               1.001    0.1%         0.00      0.003    0.205   0.838     \nNumAdsClicked                1.094    9.4%         0.09      0.003   33.156  &lt; .001  ***\nDaysUser                     1.000   -0.0%        -0.00      0.000   -0.469   0.639     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.096\nPseudo R-squared (McFadden adjusted): 0.094\nArea under the RO Curve (AUC): 0.712\nLog-likelihood: -7346.776, AIC: 14733.552, BIC: 14892.598\nChi-squared: 1568.873, df(19), p.value &lt; 0.001 \nNr obs: 21,000\n\n\n\n\nCode\nlr_control = rsm.model.logistic(\n    data={'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar\n)\nlr_control.summary()\n\n\nLogistic regression (GLM)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nNull hyp.: There is no effect of x on converted\nAlt. hyp.: There is an effect of x on converted\n\n                                OR     OR%  coefficient  std.error  z.value p.value     \nIntercept                    0.006  -99.4%        -5.18      0.193  -26.809  &lt; .001  ***\nAcquiredSpaceship[yes]       1.594   59.4%         0.47      0.072    6.472  &lt; .001  ***\nAcquiredIonWeapon[yes]       0.860  -14.0%        -0.15      0.267   -0.566   0.571     \nPurchasedCoinPackSmall[yes]  1.029    2.9%         0.03      0.069    0.415   0.678     \nPurchasedCoinPackLarge[yes]  1.338   33.8%         0.29      0.074    3.947  &lt; .001  ***\nUserConsole[yes]             1.148   14.8%         0.14      0.093    1.490   0.136     \nUserHasOldOS[yes]            0.832  -16.8%        -0.18      0.124   -1.479   0.139     \nGameLevel                    1.114   11.4%         0.11      0.014    7.527  &lt; .001  ***\nNumGameDays                  1.033    3.3%         0.03      0.005    5.954  &lt; .001  ***\nNumGameDays4Plus             1.047    4.7%         0.05      0.008    5.538  &lt; .001  ***\nNumInGameMessagesSent        1.001    0.1%         0.00      0.000    3.311  &lt; .001  ***\nNumFriends                   1.001    0.1%         0.00      0.000    4.809  &lt; .001  ***\nNumFriendRequestIgnored      0.989   -1.1%        -0.01      0.001   -8.415  &lt; .001  ***\nNumSpaceHeroBadges           1.523   52.3%         0.42      0.013   32.587  &lt; .001  ***\nTimesLostSpaceship           0.946   -5.4%        -0.06      0.006   -9.189  &lt; .001  ***\nTimesKilled                  1.006    0.6%         0.01      0.005    1.103    0.27     \nTimesCaptain                 0.998   -0.2%        -0.00      0.003   -0.487   0.626     \nTimesNavigator               0.989   -1.1%        -0.01      0.005   -2.365   0.018    *\nNumAdsClicked                1.031    3.1%         0.03      0.004    8.114  &lt; .001  ***\nDaysUser                     1.000    0.0%         0.00      0.000    2.335    0.02    *\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.202\nPseudo R-squared (McFadden adjusted): 0.198\nArea under the RO Curve (AUC): 0.831\nLog-likelihood: -3656.883, AIC: 7353.766, BIC: 7512.812\nChi-squared: 1851.927, df(19), p.value &lt; 0.001 \nNr obs: 21,000\n\n\n\n\nCreate predictions\n\n\nCode\ncg_rct_stacked[\"pred_treatment\"] = lr_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control\"] = lr_control.predict(cg_rct_stacked)[\"prediction\"]\n\n\n\n\nCode\npred_store = pd.DataFrame({\n    \"pred_treatment\": cg_rct_stacked.pred_treatment,\n    \"pred_control\": cg_rct_stacked.pred_control\n})\n\ncg_rct_stacked[\"uplift_score\"] = (\n    cg_rct_stacked.pred_treatment - cg_rct_stacked.pred_control\n)\n\n\n\n\n3. Calculate the Uplift and Incremental Uplift\nUplift Tab\n\n\nCode\nuplift_tab = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)\nuplift_tab\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\nuplift_score\n1\n0.05\n197\n450\n70\n634\n147.315457\n1.636838\n0.327368\n\n\n1\nuplift_score\n2\n0.10\n309\n900\n99\n1182\n233.619289\n2.595770\n0.195969\n\n\n2\nuplift_score\n3\n0.15\n428\n1350\n125\n1686\n327.911032\n3.643456\n0.212857\n\n\n3\nuplift_score\n4\n0.20\n528\n1800\n152\n2175\n402.206897\n4.468966\n0.167007\n\n\n4\nuplift_score\n5\n0.25\n594\n2250\n166\n2684\n454.842027\n5.053800\n0.119162\n\n\n5\nuplift_score\n6\n0.30\n642\n2700\n183\n3150\n485.142857\n5.390476\n0.070186\n\n\n6\nuplift_score\n7\n0.35\n681\n3150\n195\n3658\n513.080372\n5.700893\n0.063045\n\n\n7\nuplift_score\n8\n0.40\n719\n3600\n200\n4127\n544.539133\n6.050435\n0.073783\n\n\n8\nuplift_score\n9\n0.45\n756\n4050\n210\n4577\n570.179594\n6.335329\n0.060000\n\n\n9\nuplift_score\n10\n0.50\n791\n4500\n231\n5076\n586.212766\n6.513475\n0.035694\n\n\n10\nuplift_score\n11\n0.55\n831\n4950\n249\n5555\n609.118812\n6.767987\n0.051311\n\n\n11\nuplift_score\n12\n0.60\n859\n5400\n262\n6031\n624.412038\n6.937912\n0.034911\n\n\n12\nuplift_score\n13\n0.65\n892\n5850\n275\n6486\n643.965772\n7.155175\n0.044762\n\n\n13\nuplift_score\n14\n0.70\n937\n6300\n288\n6938\n675.483713\n7.505375\n0.071239\n\n\n14\nuplift_score\n15\n0.75\n980\n6750\n298\n7384\n707.586674\n7.862074\n0.073134\n\n\n15\nuplift_score\n16\n0.80\n1014\n7200\n312\n7805\n726.184497\n8.068717\n0.042301\n\n\n16\nuplift_score\n17\n0.85\n1047\n7650\n319\n8224\n750.264835\n8.336276\n0.056627\n\n\n17\nuplift_score\n18\n0.90\n1079\n8100\n360\n8621\n740.756177\n8.230624\n-0.032163\n\n\n18\nuplift_score\n19\n0.95\n1134\n8550\n432\n8831\n715.746122\n7.952735\n-0.220635\n\n\n19\nuplift_score\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n-0.384484\n\n\n\n\n\n\n\n\nGain Plot\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe curve starts at 0% uplift when 0% of the population is targeted (as expected, because no one has been exposed to the campaign).\nAs the percentage of the targeted population increases, the incremental uplift also increases, suggesting that targeting more of the population is yielding positive results.\nThe curve rises sharply at first, indicating that initially targeting the most responsive segments of the population yields significant uplift.\nAfter reaching a peak (which seems to be just under 80% of the population targeted), the incremental uplift begins to plateau or decrease slightly, suggesting that beyond this point, targeting additional segments of the population adds less value or could potentially include less responsive or non-responsive individuals.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe first decile (the top 10% predicted to be most responsive) shows the highest uplift, above 20%.\nThe uplift decreases across subsequent deciles, which is consistent with the expectation that the first deciles contain the most responsive individuals.\nThere is a noticeable decline in uplift as we move to higher deciles. The uplift becomes negative in the last deciles, indicating that targeting these segments would result in worse outcomes than if they were not targeted at all.\nNegative uplift in the later deciles could indicate that the campaign has a counterproductive effect on these individuals or that they would have been better or equally likely to take the desired action without the campaign intervention.\n\n\n\n4. Use the incremental_resp to calculate the profits\n\n\nCode\nprice = 14.99\ncost = 1.5\n\n\n\n\nCode\ntarget_row = uplift_tab[uplift_tab['cum_prop'] &lt;= 0.25].iloc[-1]\ntarget_row\n\n\npred                uplift_score\nbins                           5\ncum_prop                    0.25\nT_resp                       594\nT_n                         2250\nC_resp                       166\nC_n                         2684\nincremental_resp      454.842027\ninc_uplift                5.0538\nuplift                  0.119162\nName: 4, dtype: object\n\n\n\n\nCode\n# Define the function to calculate the profit\ndef prof_calc(data, price = 14.99, cost = 1.5):\n    # Given variables\n    target_customers = 30000\n    target_prop = 30000 / 120000\n\n    # Calculate the scale factor\n    scale_factor = 120000 / 9000\n\n    # Calculate the expected incremental customers and profits\n    target_row = data[data['cum_prop'] &lt;= target_prop].iloc[-1]\n    profit = (price*target_row['incremental_resp'] - cost * target_row['T_n']) * scale_factor\n    return profit\n\n\n\n\nCode\nuplift_profit_logit = prof_calc(uplift_tab, 14.99, 1.5)\nuplift_profit_logit\n\n\n45907.75976154993\n\n\n\n\n5. Calculate the uplift and Increatmental Uplift for Propensity Model\n\n\nCode\npropensity_tab = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)\npropensity_tab\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\npred_treatment\n1\n0.05\n204\n450\n80\n603\n144.298507\n1.603317\n0.320663\n\n\n1\npred_treatment\n2\n0.10\n326\n900\n112\n1131\n236.875332\n2.631948\n0.210505\n\n\n2\npred_treatment\n3\n0.15\n430\n1350\n159\n1605\n296.261682\n3.291796\n0.131955\n\n\n3\npred_treatment\n4\n0.20\n525\n1800\n206\n1994\n339.042126\n3.767135\n0.090288\n\n\n4\npred_treatment\n5\n0.25\n615\n2250\n239\n2344\n385.584471\n4.284272\n0.105714\n\n\n5\npred_treatment\n6\n0.30\n672\n2700\n285\n2807\n397.863912\n4.420710\n0.027315\n\n\n6\npred_treatment\n7\n0.35\n726\n3150\n316\n3162\n411.199241\n4.568880\n0.032676\n\n\n7\npred_treatment\n8\n0.40\n775\n3600\n336\n3603\n439.279767\n4.880886\n0.063537\n\n\n8\npred_treatment\n9\n0.45\n813\n4050\n361\n4044\n451.464392\n5.016271\n0.027755\n\n\n9\npred_treatment\n10\n0.50\n838\n4500\n386\n4527\n454.302187\n5.047802\n0.003796\n\n\n10\npred_treatment\n11\n0.55\n885\n4950\n409\n4991\n479.359848\n5.326221\n0.054875\n\n\n11\npred_treatment\n12\n0.60\n916\n5400\n436\n5479\n486.286549\n5.403184\n0.013561\n\n\n12\npred_treatment\n13\n0.65\n951\n5850\n455\n5880\n498.321429\n5.536905\n0.030396\n\n\n13\npred_treatment\n14\n0.70\n995\n6300\n464\n6332\n533.344915\n5.926055\n0.077866\n\n\n14\npred_treatment\n15\n0.75\n1028\n6750\n475\n6793\n556.006772\n6.177853\n0.049472\n\n\n15\npred_treatment\n16\n0.80\n1065\n7200\n483\n7256\n585.727674\n6.508085\n0.064944\n\n\n16\npred_treatment\n17\n0.85\n1091\n7650\n495\n7649\n595.935286\n6.621503\n0.027243\n\n\n17\npred_treatment\n18\n0.90\n1120\n8100\n499\n8093\n620.568392\n6.895204\n0.055435\n\n\n18\npred_treatment\n19\n0.95\n1148\n8550\n508\n8571\n641.244662\n7.124941\n0.043394\n\n\n19\npred_treatment\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n0.048454\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \n    \"converted\", \"yes\", \"pred_treatment\", \"ad\", 1, qnt = 20)\n\n\n\n\n\n\n\n\n\n\n\nCompare Uplift model and Propensity model\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment\", \"uplift_score\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nUplift Model Performance: The line for the uplift_score generally lies above the line for the pred_treatment, indicating that the uplift model consistently provides a higher incremental uplift across the different percentages of the population targeted.\nDiminishing Returns: Both lines show a trend of diminishing returns as more of the population is targeted, with the incremental uplift peaking and then plateauing or slightly decreasing, suggesting an optimal targeting point before 100%.\nComparison: The propensity model appears to perform better than random targeting (which would be a straight line from the origin), but the uplift model is more effective in achieving incremental gains. This suggests that while the propensity model can identify likely responders, the uplift model is better at identifying those for whom the treatment would make a difference in their behavior.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment\", \"uplift_score\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nUplift Distribution: Both sets of bars show a decrease in uplift as we move through the population segments, which is expected as the most responsive individuals are often targeted first.\nModel Comparison: In some segments, the uplift_score bars are higher than the pred_treatment bars, reinforcing the idea that the uplift model is more effective in certain segments.\nNegative Uplift: Towards the latter segments, both models show negative uplift, but the uplift_score model tends to have less severe negative values. The uplift model places customers with high incrementality in earlier deciles. The incrementality is lower for propensity model because it targets Persuadables and Sure Things whereas the uplift model targets only the former. This suggests that the uplift model may be better at minimizing the risk of targeting individuals who would have a negative response to the treatment.\n\nThat said, the propensity model still performs well here; this is because the customers who have the best propensity also tend to have the best uplift in this data:\n\n\nCode\ncm = rsm.correlation(\n    {\"cg_rct_stacked\": cg_rct_stacked.loc[cg_rct_stacked.training == 0, \"pred_treatment\": \"uplift_score\"]})\ncm.summary()\n\n\nCorrelation\nData     : cg_rct_stacked\nMethod   : pearson\nCutoff   : 0\nVariables: pred_treatment, pred_control, uplift_score\nNull hyp.: variables x and y are not correlated\nAlt. hyp.: variables x and y are correlated\n\nCorrelation matrix:\n             pred_treatment pred_control\npred_control           0.28             \nuplift_score           0.55        -0.65\n\np.values:\n             pred_treatment pred_control\npred_control            0.0             \nuplift_score            0.0          0.0\n\n\nThe positive correlation between pred_treatment and uplift_score is in line with what we would expect, as a higher predicted treatment response should correspond with a higher uplift score. The negative correlation between pred_control and uplift_score suggests that individuals who are likely to respond without any intervention (as predicted by the control model) are properly being identified as not contributing to uplift, which is a desirable feature of a good uplift model.\n\n\n6. Use the incremental_resp to calculate the profits for Propensity Model\n\n\nCode\npropensity_profit_logit = prof_calc(propensity_tab, 14.99, 1.5)\npropensity_profit_logit\n\n\n32065.482935153585\n\n\n\n\nCode\n# Difference in profits from using uplift model and propensity model\ndifference_logit = uplift_profit_logit - propensity_profit_logit\ndifference_logit\n\n\n13842.276826396348\n\n\n\n\n\nUsing Neural Network\n\n2. Train an uplift model\n\n\nCode\nclf_treatment = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    hidden_layer_sizes = (1, ),\n    alpha = 0.1\n)\nclf_treatment.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.1\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.712\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         5           15                 0                    179         362                       50                   0               yes                no                  22            0             4               4                     no                     no              2      1308         yes           no\n         4            4                 0                     36           0                        0                   0                no                no                   0            0             0               0                     no                     no              2      2922         yes           no\n         8           17                 0                    222          20                       63                  10               yes                no                  10            0             9               6                    yes                     no              4      2192         yes           no\n        10           18                 2                      0          56                        6                   2                no                no                   1            0             0               0                     no                    yes             13      2313         yes           no\n        10           20                 5                     36           0                       16                   0                no                no                   0            0             0               0                     no                    yes              9      1766         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n -0.480555     0.361225         -0.411200               0.968856    3.186178                 0.577047           -0.371464            1.269286    -0.081624      0.347884        0.390873      -0.988082 -1.999414                   True                  False                       False                       False             True             False\n -0.842392    -1.185248         -0.411200              -0.360274   -0.525098                -0.876726           -0.371464           -0.301827    -0.081624     -0.205861       -0.201936      -0.988082  0.439831                  False                  False                       False                       False             True             False\n  0.604958     0.642403         -0.411200               1.368525   -0.320055                 0.955027            4.091205            0.412315    -0.081624      1.040065        0.687277      -0.695079 -0.663421                   True                  False                        True                       False             True             False\n  1.328633     0.782991          0.170373              -0.694880    0.049022                -0.702273            0.521070           -0.230413    -0.081624     -0.205861       -0.201936       0.623433 -0.480554                  False                  False                       False                        True             True             False\n  1.328633     1.064168          1.042734              -0.360274   -0.525098                -0.411519           -0.371464           -0.301827    -0.081624     -0.205861       -0.201936       0.037428 -1.307237                  False                  False                       False                        True             True             False\n\n\n\n\nModel Tuning\n\n\nCode\nhls = [(1,), (2,), (3,), (3, 3), (4, 2), (5, 5), (5,), (10,), (5,10), (10,5)]\nalpha = [0.0001, 0.001, 0.01, 0.1, 1]\n\n\nparam_grid = {\"hidden_layer_sizes\": hls, \"alpha\": alpha}\nscoring = {\"AUC\": \"roc_auc\"}\n\nclf_cv_treatment = GridSearchCV(\n    clf_treatment.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5\n)\n\n\n\n\nCode\nclf_cv_treatment.fit(clf_treatment.data_onehot, clf_treatment.data.converted_yes)\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n\n\nGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', alpha=0.1,\n                                     hidden_layer_sizes=(1,), max_iter=10000,\n                                     random_state=1234, solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5), (5,), (10,),\n                                                (5, 10), (10, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', alpha=0.1,\n                                     hidden_layer_sizes=(1,), max_iter=10000,\n                                     random_state=1234, solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5), (5,), (10,),\n                                                (5, 10), (10, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: MLPClassifierMLPClassifier(activation='tanh', alpha=0.1, hidden_layer_sizes=(1,),\n              max_iter=10000, random_state=1234, solver='lbfgs') ¬†MLPClassifier?Documentation for MLPClassifierMLPClassifier(activation='tanh', alpha=0.1, hidden_layer_sizes=(1,),\n              max_iter=10000, random_state=1234, solver='lbfgs') \n\n\n\n\nCode\nclf_cv_treatment.best_params_\n\n\n{'alpha': 0.0001, 'hidden_layer_sizes': (4, 2)}\n\n\n\n\nCode\nclf_treatment = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    **clf_cv_treatment.best_params_\n)\nclf_treatment.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nHidden_layer_sizes   : (4, 2)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.792\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         5           15                 0                    179         362                       50                   0               yes                no                  22            0             4               4                     no                     no              2      1308         yes           no\n         4            4                 0                     36           0                        0                   0                no                no                   0            0             0               0                     no                     no              2      2922         yes           no\n         8           17                 0                    222          20                       63                  10               yes                no                  10            0             9               6                    yes                     no              4      2192         yes           no\n        10           18                 2                      0          56                        6                   2                no                no                   1            0             0               0                     no                    yes             13      2313         yes           no\n        10           20                 5                     36           0                       16                   0                no                no                   0            0             0               0                     no                    yes              9      1766         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n -0.480555     0.361225         -0.411200               0.968856    3.186178                 0.577047           -0.371464            1.269286    -0.081624      0.347884        0.390873      -0.988082 -1.999414                   True                  False                       False                       False             True             False\n -0.842392    -1.185248         -0.411200              -0.360274   -0.525098                -0.876726           -0.371464           -0.301827    -0.081624     -0.205861       -0.201936      -0.988082  0.439831                  False                  False                       False                       False             True             False\n  0.604958     0.642403         -0.411200               1.368525   -0.320055                 0.955027            4.091205            0.412315    -0.081624      1.040065        0.687277      -0.695079 -0.663421                   True                  False                        True                       False             True             False\n  1.328633     0.782991          0.170373              -0.694880    0.049022                -0.702273            0.521070           -0.230413    -0.081624     -0.205861       -0.201936       0.623433 -0.480554                  False                  False                       False                        True             True             False\n  1.328633     1.064168          1.042734              -0.360274   -0.525098                -0.411519           -0.371464           -0.301827    -0.081624     -0.205861       -0.201936       0.037428 -1.307237                  False                  False                       False                        True             True             False\n\n\n\n\nCode\nclf_control = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    hidden_layer_sizes = (1, ),\n    alpha = 0.0001\n)\nclf_control.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.841\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         7           18                 0                    124           0                       81                   0               yes                no                   8            0             0               4                     no                    yes              3      2101          no           no\n        10            3                 2                     60         479                       18                   0                no                no                  10            7             0               0                    yes                     no              7      1644         yes           no\n         2            1                 0                      0           0                        0                   0                no                no                   0            0             0               2                     no                     no              8      3197         yes          yes\n         8           15                 0                      0          51                        6                   0               yes                no                   0            0             2               1                    yes                     no             21      2009         yes           no\n        10           18                 0                      0           0                        0                   0                no                no                   0            0             0               0                    yes                     no              6      3288         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n  0.283862     0.806405         -0.399092               0.485069   -0.513099                 1.509762            -0.29461            0.343880    -0.075167     -0.180445        0.397524      -0.868677 -0.787337                   True                  False                       False                        True            False             False\n  1.373047    -1.292688          0.204178              -0.119399    4.411754                -0.342325            -0.29461            0.527329     1.452288     -0.180445       -0.209504      -0.333738 -1.475407                  False                  False                        True                       False             True             False\n -1.531446    -1.572567         -0.399092              -0.686088   -0.513099                -0.871493            -0.29461           -0.389917    -0.075167     -0.180445        0.094010      -0.200003  0.862827                  False                  False                       False                       False             True              True\n  0.646924     0.386586         -0.399092              -0.686088    0.011259                -0.695104            -0.29461           -0.389917    -0.075167      0.052946       -0.057747       1.538548 -0.925854                   True                  False                        True                       False             True             False\n  1.373047     0.806405         -0.399092              -0.686088   -0.513099                -0.871493            -0.29461           -0.389917    -0.075167     -0.180445       -0.209504      -0.467473  0.999839                  False                  False                        True                       False             True             False\n\n\n\n\nCode\n# Model tunning\nclf_cv_control = GridSearchCV(\n    clf_control.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5\n)\nclf_cv_control.fit(clf_control.data_onehot, clf_control.data.converted_yes)\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5), (5,), (10,),\n                                                (5, 10), (10, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5), (5,), (10,),\n                                                (5, 10), (10, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') ¬†MLPClassifier?Documentation for MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') \n\n\n\n\nCode\nclf_cv_control.best_params_\n\n\n{'alpha': 1, 'hidden_layer_sizes': (3, 3)}\n\n\n\n\nCode\nclf_control = rsm.model.mlp(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    **clf_cv_control.best_params_\n)\nclf_control.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nHidden_layer_sizes   : (3, 3)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 1\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.861\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         7           18                 0                    124           0                       81                   0               yes                no                   8            0             0               4                     no                    yes              3      2101          no           no\n        10            3                 2                     60         479                       18                   0                no                no                  10            7             0               0                    yes                     no              7      1644         yes           no\n         2            1                 0                      0           0                        0                   0                no                no                   0            0             0               2                     no                     no              8      3197         yes          yes\n         8           15                 0                      0          51                        6                   0               yes                no                   0            0             2               1                    yes                     no             21      2009         yes           no\n        10           18                 0                      0           0                        0                   0                no                no                   0            0             0               0                    yes                     no              6      3288         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n  0.283862     0.806405         -0.399092               0.485069   -0.513099                 1.509762            -0.29461            0.343880    -0.075167     -0.180445        0.397524      -0.868677 -0.787337                   True                  False                       False                        True            False             False\n  1.373047    -1.292688          0.204178              -0.119399    4.411754                -0.342325            -0.29461            0.527329     1.452288     -0.180445       -0.209504      -0.333738 -1.475407                  False                  False                        True                       False             True             False\n -1.531446    -1.572567         -0.399092              -0.686088   -0.513099                -0.871493            -0.29461           -0.389917    -0.075167     -0.180445        0.094010      -0.200003  0.862827                  False                  False                       False                       False             True              True\n  0.646924     0.386586         -0.399092              -0.686088    0.011259                -0.695104            -0.29461           -0.389917    -0.075167      0.052946       -0.057747       1.538548 -0.925854                   True                  False                        True                       False             True             False\n  1.373047     0.806405         -0.399092              -0.686088   -0.513099                -0.871493            -0.29461           -0.389917    -0.075167     -0.180445       -0.209504      -0.467473  0.999839                  False                  False                        True                       False             True             False\n\n\n\n\nCode\ncg_rct_stacked[\"pred_treatment_nn\"] = clf_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control_nn\"] = clf_control.predict(cg_rct_stacked)[\"prediction\"]\n\n\n\n\n3. Calculate the Uplift and Incremental Uplift\n\n\nCode\ncg_rct_stacked[\"uplift_score_nn\"] = (\n    cg_rct_stacked.pred_treatment_nn - cg_rct_stacked.pred_control_nn\n)\n\n\n\n\nCode\nuplift_tab_nn = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)\nuplift_tab_nn\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\nuplift_score_nn\n1\n0.05\n198\n450\n71\n597\n144.482412\n1.605360\n0.321072\n\n\n1\nuplift_score_nn\n2\n0.10\n354\n900\n113\n1138\n264.632689\n2.940363\n0.269033\n\n\n2\nuplift_score_nn\n3\n0.15\n476\n1350\n138\n1650\n363.090909\n4.034343\n0.222283\n\n\n3\nuplift_score_nn\n4\n0.20\n588\n1800\n174\n2164\n443.268022\n4.925200\n0.178850\n\n\n4\nuplift_score_nn\n5\n0.25\n673\n2250\n204\n2661\n500.508455\n5.561205\n0.128527\n\n\n5\nuplift_score_nn\n6\n0.30\n754\n2700\n233\n3172\n555.670870\n6.174121\n0.123249\n\n\n6\nuplift_score_nn\n7\n0.35\n811\n3150\n250\n3696\n597.931818\n6.643687\n0.094224\n\n\n7\nuplift_score_nn\n8\n0.40\n854\n3600\n259\n4186\n631.257525\n7.013973\n0.077188\n\n\n8\nuplift_score_nn\n9\n0.45\n893\n4050\n271\n4696\n659.279813\n7.325331\n0.063137\n\n\n9\nuplift_score_nn\n10\n0.50\n931\n4500\n282\n5210\n687.429942\n7.638110\n0.063044\n\n\n10\nuplift_score_nn\n11\n0.55\n969\n4950\n294\n5668\n712.242766\n7.913809\n0.058244\n\n\n11\nuplift_score_nn\n12\n0.60\n987\n5400\n299\n6090\n721.876847\n8.020854\n0.028152\n\n\n12\nuplift_score_nn\n13\n0.65\n1003\n5850\n304\n6539\n731.031809\n8.122576\n0.024420\n\n\n13\nuplift_score_nn\n14\n0.70\n1025\n6300\n306\n6926\n746.657522\n8.296195\n0.043721\n\n\n14\nuplift_score_nn\n15\n0.75\n1037\n6750\n311\n7387\n752.818329\n8.364648\n0.015821\n\n\n15\nuplift_score_nn\n16\n0.80\n1052\n7200\n319\n7799\n757.500705\n8.416675\n0.013916\n\n\n16\nuplift_score_nn\n17\n0.85\n1063\n7650\n326\n8249\n760.672445\n8.451916\n0.008889\n\n\n17\nuplift_score_nn\n18\n0.90\n1087\n8100\n353\n8631\n755.717414\n8.396860\n-0.017347\n\n\n18\nuplift_score_nn\n19\n0.95\n1138\n8550\n431\n8846\n721.421886\n8.015799\n-0.249457\n\n\n19\nuplift_score_nn\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n-0.445974\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe curve starts at the origin and increases as more of the population is targeted, reaching a peak before it starts to plateau. This indicates that the campaign has diminishing returns; after a certain point, targeting additional people results in smaller incremental gains.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_nn\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\nThis pattern indicates that the first segments are highly responsive to the campaign, while the later segments may have been negatively influenced by the campaign or would have been better off not being targeted at all.\n\n\n4. Using the incremental_resp to calculate the profits for Uplift model\n\n\nCode\nuplift_profit_nn = prof_calc(uplift_tab_nn, 14.99, 1.5)\nuplift_profit_nn\n\n\n55034.9566328448\n\n\n\n\n5. Calculate the uplift and Increatmental Uplift for Propensity Model\n\n\nCode\nprop_tab_nn = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_nn\", \"ad\", 1, qnt = 20\n)\nprop_tab_nn\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\npred_treatment_nn\n1\n0.05\n201\n450\n74\n586\n144.174061\n1.601934\n0.320387\n\n\n1\npred_treatment_nn\n2\n0.10\n351\n900\n140\n1076\n233.899628\n2.598885\n0.198639\n\n\n2\npred_treatment_nn\n3\n0.15\n475\n1350\n178\n1561\n321.060218\n3.567336\n0.197205\n\n\n3\npred_treatment_nn\n4\n0.20\n589\n1800\n220\n2044\n395.262231\n4.391803\n0.166377\n\n\n4\npred_treatment_nn\n5\n0.25\n686\n2250\n257\n2497\n454.422107\n5.049135\n0.133878\n\n\n5\npred_treatment_nn\n6\n0.30\n765\n2700\n293\n2962\n497.916948\n5.532411\n0.098136\n\n\n6\npred_treatment_nn\n7\n0.35\n833\n3150\n346\n3352\n507.850835\n5.642787\n0.015214\n\n\n7\npred_treatment_nn\n8\n0.40\n896\n3600\n396\n3698\n510.494321\n5.672159\n-0.004509\n\n\n8\npred_treatment_nn\n9\n0.45\n941\n4050\n444\n4013\n492.906305\n5.476737\n-0.052381\n\n\n9\npred_treatment_nn\n10\n0.50\n982\n4500\n477\n4398\n493.937244\n5.488192\n0.005397\n\n\n10\npred_treatment_nn\n11\n0.55\n1023\n4950\n486\n4870\n529.016427\n5.877960\n0.072043\n\n\n11\npred_treatment_nn\n12\n0.60\n1053\n5400\n491\n5383\n560.449378\n6.227215\n0.056920\n\n\n12\npred_treatment_nn\n13\n0.65\n1083\n5850\n496\n5908\n591.869330\n6.576326\n0.057143\n\n\n13\npred_treatment_nn\n14\n0.70\n1102\n6300\n503\n6352\n603.117758\n6.701308\n0.026456\n\n\n14\npred_treatment_nn\n15\n0.75\n1116\n6750\n505\n6835\n617.280176\n6.858669\n0.026970\n\n\n15\npred_treatment_nn\n16\n0.80\n1131\n7200\n506\n7272\n630.009901\n7.000110\n0.031045\n\n\n16\npred_treatment_nn\n17\n0.85\n1143\n7650\n507\n7657\n636.463497\n7.071817\n0.024069\n\n\n17\npred_treatment_nn\n18\n0.90\n1161\n8100\n509\n8123\n653.441216\n7.260458\n0.035708\n\n\n18\npred_treatment_nn\n19\n0.95\n1170\n8550\n510\n8541\n659.462592\n7.327362\n0.017608\n\n\n19\npred_treatment_nn\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n0.004532\n\n\n\n\n\n\n\n\n\n\nCompare Uplift model and Propensity model\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_nn\", \"uplift_score_nn\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe uplift_score_nn line generally lies above the pred_treatment_nn line, indicating that the uplift model predicts a higher incremental uplift across the different segments of the targeted population.\nBoth lines show a rise in incremental uplift with an increase in the targeted population, reaching a peak, and then beginning to plateau, suggesting a point of diminishing returns.\nThe uplift model‚Äôs curve suggests that targeting based on its scores leads to higher incremental gains compared to the propensity model, which is likely predicting the likelihood of response to the treatment without considering the control group‚Äôs response.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_nn\", \"uplift_score_nn\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe uplift decreases from the first to the last segment, which suggests that the initial segments are the most responsive to the targeting. The treatment model shows positive uplift in the early segments but drops off more sharply than the uplift model in later segments, indicating that the treatment model might be less effective at distinguishing between those who will respond due to the treatment and those who would have responded anyway.\nThe uplift model has a more gradual decline in uplift across segments and less negative uplift in the lower segments, which could imply that it is more effective at targeting the right individuals.\nThe negative values in later segments for both models suggest that certain individuals are either not influenced by or negatively influenced by the treatment. This could represent individuals who might purchase or respond anyway, so the propensity might have been an unnecessary expense for this group, or it could represent a group for whom the treatment had an adverse effect.\n\n\n\n6. Using the Incremental_resp to calculate the profits for Propensity Model\n\n\nCode\npropensity_profit_nn = prof_calc(prop_tab_nn, 14.99, 1.5)\npropensity_profit_nn\n\n\n45823.831691362975\n\n\n\n\nCode\n# Different profit between Uplift model and Propensity model\ndifference_nn = uplift_profit_nn - propensity_profit_nn\ndifference_nn\n\n\n9211.124941481823\n\n\n\n\n\nUsing Random Forest Model\n\n2. Train an uplift model\n\n\nCode\nrf_treatment = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n)\nrf_treatment.summary()\n\n\nRandom Forest\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nmax_features         : sqrt (4)\nn_estimators         : 100\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.761\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         5           15                 0                    179         362                       50                   0                  22            0             4               4              2      1308                   True                  False                       False                       False             True             False\n         4            4                 0                     36           0                        0                   0                   0            0             0               0              2      2922                  False                  False                       False                       False             True             False\n         8           17                 0                    222          20                       63                  10                  10            0             9               6              4      2192                   True                  False                        True                       False             True             False\n        10           18                 2                      0          56                        6                   2                   1            0             0               0             13      2313                  False                  False                       False                        True             True             False\n        10           20                 5                     36           0                       16                   0                   0            0             0               0              9      1766                  False                  False                       False                        True             True             False\n\n\n\n\nModel Tuning\n\n\nCode\nmax_features = [None, 'auto', 'sqrt', 'log2', 0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0]\nn_estimators = [10, 50, 100, 200, 500, 1000]\n\n\nparam_grid = {\"max_features\": max_features, \"n_estimators\": n_estimators}\nscoring = {\"AUC\": \"roc_auc\"}\n\nrf_cv_treatment = GridSearchCV(rf_treatment.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5)\n\n\n\n\nCode\nrf_cv_treatment.fit(rf_treatment.data_onehot, rf_treatment.data.converted_yes)\n\n\nFitting 5 folds for each of 66 candidates, totalling 330 fits\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.2s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.755) total time=   1.3s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.755) total time=   7.4s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.773) total time=  14.7s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.777) total time=  14.7s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.766) total time=  20.0s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.769) total time=  13.5s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.791) total time=   2.5s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.759) total time=   2.1s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.712) total time=   2.8s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.752) total time= 1.5min\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.726) total time=  37.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.699) total time=   0.1s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.737) total time=   1.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.753) total time=   3.4s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.755) total time=  10.0s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.777) total time=  33.6s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.772) total time=  15.7s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.779) total time=  21.0s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.763) total time=   1.5s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.774) total time=   1.0s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.774) total time=   5.8s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.768) total time=   2.5s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.743) total time= 1.2min\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.749) total time=  36.9s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.749) total time=   1.3s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.757) total time=  13.4s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.762) total time=  14.7s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.775) total time=   7.1s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.776) total time=  22.2s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.766) total time=   1.3s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.769) total time=   1.4s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.773) total time=   2.2s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.788) total time=   2.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.771) total time=   2.2s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.760) total time=   2.1s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.744) total time=   2.0s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.728) total time=   3.7s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.749) total time=  53.4s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.740) total time=  23.3s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.703) total time=  11.6s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.759) total time=   8.4s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.778) total time=  17.7s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.771) total time=  13.3s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.772) total time=  22.4s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.734) total time=   1.6s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.750) total time=   2.6s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.786) total time=  33.8s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.748) total time=   9.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.719) total time=   0.1s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.703) total time=   0.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.699) total time=   0.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.741) total time=   0.3s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.734) total time=   0.2s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.738) total time=   0.4s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.740) total time=   0.2s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.730) total time=   0.4s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.744) total time=   0.8s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.741) total time=   1.5s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.770) total time=   2.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.774) total time=   5.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.763) total time=   9.2s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.758) total time=   1.0s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.767) total time=   0.8s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.753) total time=   1.6s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.755) total time=   2.6s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.786) total time=  18.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.753) total time=   6.7s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.737) total time=  10.5s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.829) total time=   0.1s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.836) total time=   0.3s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.851) total time=   0.4s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.834) total time=   1.4s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.823) total time=   1.3s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.813) total time=   1.7s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.795) total time=   3.8s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.835) total time=  14.0s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.791) total time=   7.3s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.852) total time=  34.9s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.792) total time=  15.5s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.777) total time=  10.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.798) total time=   9.0s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.821) total time=   6.9s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.845) total time=   0.1s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.829) total time=   0.1s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.823) total time=   0.2s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.839) total time=   0.2s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.826) total time=   0.3s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.844) total time=   0.4s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.832) total time=   0.2s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.851) total time=   0.5s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.840) total time=   2.0s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.843) total time=   2.2s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.852) total time=   5.7s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.797) total time=   4.0s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.853) total time=   0.3s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.819) total time=   1.2s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.798) total time=   1.4s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.813) total time=  13.6s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.824) total time=   9.6s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.790) total time=   5.1s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.845) total time=   0.1s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.829) total time=   0.1s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.826) total time=   0.1s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.823) total time=   0.1s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.849) total time=   0.1s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.834) total time=   0.3s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.839) total time=   0.2s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.835) total time=   0.2s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.826) total time=   0.4s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.853) total time=   0.3s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.845) total time=   0.3s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.836) total time=   0.3s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.833) total time=   0.2s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.823) total time=   0.5s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.851) total time=   0.3s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.840) total time=   1.6s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.846) total time=   1.3s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.844) total time=   1.8s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.832) total time=   0.4s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.856) total time=   1.3s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.827) total time=   1.9s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.842) total time=   2.3s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.850) total time=   0.5s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.835) total time=   0.5s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.835) total time=   1.4s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.827) total time=   0.6s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.852) total time=   0.5s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.856) total time=   1.6s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.823) total time=   1.5s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.864) total time=   2.5s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.840) total time=  21.7s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.836) total time=   0.2s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.855) total time=   0.3s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.835) total time=   0.3s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.837) total time=   0.5s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.854) total time=   0.8s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.854) total time=   2.3s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.846) total time=   2.4s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.825) total time=   2.3s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.853) total time=   0.5s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.848) total time=   0.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.847) total time=   1.2s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.825) total time=   1.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.825) total time=   3.9s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.839) total time=   4.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.818) total time=   2.6s\n[CV 3/5] END max_features=None, n_estimators=10; AUC: (test=0.719) total time=   0.5s\n[CV 1/5] END max_features=None, n_estimators=50; AUC: (test=0.749) total time=   3.0s\n[CV 1/5] END max_features=None, n_estimators=100; AUC: (test=0.757) total time=   6.6s\n[CV 5/5] END max_features=None, n_estimators=100; AUC: (test=0.752) total time=   6.5s\n[CV 4/5] END max_features=None, n_estimators=200; AUC: (test=0.766) total time=  14.8s\n[CV 3/5] END max_features=None, n_estimators=500; AUC: (test=0.765) total time=  31.9s\n[CV 2/5] END max_features=None, n_estimators=1000; AUC: (test=0.763) total time= 1.1min\n[CV 1/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.721) total time=   0.2s\n[CV 2/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.714) total time=   0.2s\n[CV 3/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.711) total time=   0.2s\n[CV 4/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.697) total time=   0.2s\n[CV 5/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.703) total time=   0.2s\n[CV 1/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.770) total time=   0.9s\n[CV 2/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.755) total time=   0.9s\n[CV 3/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.772) total time=   0.9s\n[CV 4/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.757) total time=   0.9s\n[CV 5/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.750) total time=   0.9s\n[CV 3/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.780) total time=   1.7s\n[CV 4/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.769) total time=   1.7s\n[CV 2/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.774) total time=   3.4s\n[CV 3/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.782) total time=   3.3s\n[CV 1/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.780) total time=   8.5s\n[CV 4/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.778) total time=   8.6s\n[CV 2/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.776) total time=  18.1s\n[CV 5/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.766) total time=  17.5s\n[CV 1/5] END max_features=log2, n_estimators=500; AUC: (test=0.780) total time=   9.4s\n[CV 5/5] END max_features=log2, n_estimators=500; AUC: (test=0.765) total time=   8.9s\n[CV 4/5] END max_features=log2, n_estimators=1000; AUC: (test=0.779) total time=  17.3s\n[CV 2/5] END max_features=0.25, n_estimators=200; AUC: (test=0.774) total time=   3.8s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.703) total time=   0.2s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.758) total time=   2.7s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.731) total time=   0.4s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.761) total time=   1.0s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.750) total time=   3.3s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.760) total time=  17.8s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.780) total time=  16.6s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.775) total time=  26.1s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.761) total time=   2.1s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.767) total time=   1.0s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.773) total time=   1.6s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.770) total time=   1.6s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.761) total time=   3.0s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.764) total time=  37.7s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.746) total time=  55.1s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.738) total time=  26.8s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.706) total time=   8.9s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.719) total time=   0.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.703) total time=   0.1s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.756) total time=   1.8s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.756) total time=   1.4s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.753) total time=   1.8s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.730) total time=   0.5s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.761) total time=   0.7s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.718) total time=   1.7s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.750) total time=   4.3s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.760) total time=  25.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.781) total time=  19.3s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.772) total time=  32.8s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.773) total time=   1.5s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.758) total time=   3.4s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.720) total time=   2.8s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.727) total time= 1.3min\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.742) total time=  13.2s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.739) total time=  13.2s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.719) total time=   0.1s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.712) total time=   0.1s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.703) total time=   0.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.699) total time=   0.1s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.756) total time=   1.0s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.737) total time=   0.5s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.760) total time=   1.7s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.731) total time=   0.6s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.758) total time=   0.6s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.719) total time=   0.8s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.751) total time=   7.5s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.760) total time=  10.9s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.775) total time=   7.8s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.787) total time=  20.3s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.781) total time=  32.8s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.753) total time=  40.0s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.746) total time=  35.2s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.755) total time=   2.9s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.724) total time=   0.4s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.744) total time=   3.4s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.774) total time=  10.2s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.771) total time=   8.9s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.788) total time=  20.2s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.778) total time=  20.7s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.763) total time=   2.6s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.756) total time=  22.9s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.748) total time=  20.1s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.716) total time=   4.5s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.739) total time=   0.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.773) total time=   2.9s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.755) total time=   4.4s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.775) total time=   3.5s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.773) total time=  11.0s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.749) total time=   2.2s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.768) total time=  18.0s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.762) total time=  14.5s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.823) total time=   0.1s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.839) total time=   0.2s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.832) total time=   0.2s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.824) total time=   0.2s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.839) total time=   2.3s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.847) total time=   0.7s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.872) total time=   4.0s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.812) total time=   4.6s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.845) total time=   0.5s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.861) total time=   1.4s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.794) total time=   1.7s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.871) total time=   2.1s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.802) total time=  12.5s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.797) total time=  15.1s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.771) total time=  13.1s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.817) total time=   0.9s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.806) total time=   2.9s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.815) total time=   7.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.848) total time=   0.5s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.815) total time=   1.6s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.867) total time=   2.2s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.807) total time=  12.5s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.797) total time=   7.7s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.792) total time=  24.4s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.817) total time=   3.0s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.847) total time=   0.4s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.833) total time=   0.5s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.817) total time=   0.6s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.801) total time=   1.7s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.795) total time=  11.8s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.847) total time=  44.3s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.801) total time=   4.0s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.838) total time=   8.6s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.800) total time=   4.2s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.804) total time=   4.4s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.849) total time=   1.5s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.833) total time=   1.6s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.849) total time=   0.8s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.846) total time=   3.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.852) total time=   1.0s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.833) total time=   2.8s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.835) total time=   4.2s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.813) total time=   2.6s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.777) total time=   5.0s\n[CV 4/5] END max_features=None, n_estimators=10; AUC: (test=0.712) total time=   0.5s\n[CV 3/5] END max_features=None, n_estimators=50; AUC: (test=0.751) total time=   3.0s\n[CV 5/5] END max_features=None, n_estimators=50; AUC: (test=0.741) total time=   3.1s\n[CV 4/5] END max_features=None, n_estimators=100; AUC: (test=0.765) total time=   7.2s\n[CV 3/5] END max_features=None, n_estimators=200; AUC: (test=0.760) total time=  14.0s\n[CV 2/5] END max_features=None, n_estimators=500; AUC: (test=0.761) total time=  32.1s\n[CV 1/5] END max_features=None, n_estimators=1000; AUC: (test=0.763) total time= 1.1min\n[CV 5/5] END max_features=None, n_estimators=1000; AUC: (test=0.751) total time=  60.0s\n[CV 2/5] END max_features=log2, n_estimators=50; AUC: (test=0.755) total time=   0.9s\n[CV 4/5] END max_features=log2, n_estimators=50; AUC: (test=0.757) total time=   0.9s\n[CV 2/5] END max_features=log2, n_estimators=100; AUC: (test=0.766) total time=   1.7s\n[CV 4/5] END max_features=log2, n_estimators=100; AUC: (test=0.769) total time=   1.8s\n[CV 1/5] END max_features=log2, n_estimators=200; AUC: (test=0.779) total time=   3.8s\n[CV 3/5] END max_features=log2, n_estimators=200; AUC: (test=0.782) total time=   3.7s\n[CV 2/5] END max_features=log2, n_estimators=500; AUC: (test=0.776) total time=   9.1s\n[CV 1/5] END max_features=log2, n_estimators=1000; AUC: (test=0.780) total time=  17.0s\n[CV 5/5] END max_features=log2, n_estimators=1000; AUC: (test=0.766) total time=  17.0s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.719) total time=   0.2s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.699) total time=   0.1s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.753) total time=   3.2s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.718) total time=   1.7s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.759) total time=   8.8s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.775) total time=  18.4s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.772) total time=  14.6s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.769) total time=  27.4s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.754) total time=   2.6s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.756) total time=   3.3s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.778) total time=  45.9s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.739) total time= 1.5min\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.759) total time=   5.6s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.776) total time=  26.5s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.787) total time=  17.4s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.761) total time=  44.8s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.780) total time= 1.8min\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.762) total time=   5.6s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.758) total time=  18.7s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.766) total time=  24.2s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.774) total time=  13.5s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.763) total time=   3.9s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.748) total time=   2.6s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.739) total time=  59.8s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.731) total time=  13.9s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.724) total time=  13.5s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.759) total time=  32.2s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.770) total time=  20.5s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.768) total time=   2.2s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.765) total time=   1.4s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.768) total time=   2.2s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.783) total time=   2.2s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.740) total time=   2.3s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.746) total time=  33.6s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.723) total time=   9.8s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.733) total time=   6.5s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.755) total time=   3.8s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.758) total time=   3.2s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.771) total time=   2.8s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.780) total time=   6.1s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.767) total time=   1.0s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.758) total time=   1.6s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.748) total time=   0.7s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.741) total time=   1.6s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.739) total time=   1.9s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.769) total time=  24.0s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.749) total time=   4.0s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.845) total time=   0.1s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.849) total time=   0.2s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.826) total time=   0.2s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.854) total time=   0.4s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.811) total time=   3.8s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.852) total time=   4.0s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.826) total time=   1.0s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.825) total time=   1.0s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.820) total time=   1.0s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5,); AUC: (test=0.815) total time=   0.9s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.803) total time=   1.6s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(10,); AUC: (test=0.804) total time=   1.9s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.796) total time=  22.9s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.788) total time=   8.3s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.845) total time=   0.1s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.829) total time=   0.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.826) total time=   0.1s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.823) total time=   0.1s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(1,); AUC: (test=0.849) total time=   0.1s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.842) total time=   0.2s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.838) total time=   0.1s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.836) total time=   0.1s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.826) total time=   0.1s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(2,); AUC: (test=0.853) total time=   0.5s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.844) total time=   0.3s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.836) total time=   0.3s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.832) total time=   0.3s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.851) total time=   0.4s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.838) total time=   3.7s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.852) total time=   4.9s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.826) total time=   1.3s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.812) total time=   6.8s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.836) total time=   0.6s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.831) total time=   0.7s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.819) total time=   0.9s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.816) total time=   0.9s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.804) total time=   1.6s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.791) total time=  13.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.840) total time=  29.3s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.831) total time=   1.0s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.826) total time=   1.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.867) total time=   1.9s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.809) total time=   3.8s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5,); AUC: (test=0.826) total time=   0.6s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.856) total time=   1.3s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(10,); AUC: (test=0.860) total time=   2.4s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.771) total time=  13.7s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.792) total time=   9.0s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.796) total time=  43.5s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.803) total time=   5.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.845) total time=   0.1s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.830) total time=   0.1s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.826) total time=   0.1s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.823) total time=   0.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(1,); AUC: (test=0.849) total time=   0.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.842) total time=   0.4s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.840) total time=   0.2s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(2,); AUC: (test=0.828) total time=   0.5s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.847) total time=   0.6s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.823) total time=   0.5s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.858) total time=   1.8s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.857) total time=   2.2s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.816) total time=   1.2s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.839) total time=   2.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.841) total time=   3.1s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.841) total time=   1.8s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.808) total time=   5.1s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.824) total time=   4.6s\n[CV 2/5] END max_features=None, n_estimators=10; AUC: (test=0.702) total time=   0.5s\n[CV 5/5] END max_features=None, n_estimators=10; AUC: (test=0.684) total time=   0.5s\n[CV 4/5] END max_features=None, n_estimators=50; AUC: (test=0.761) total time=   3.0s\n[CV 3/5] END max_features=None, n_estimators=100; AUC: (test=0.756) total time=   6.7s\n[CV 2/5] END max_features=None, n_estimators=200; AUC: (test=0.759) total time=  13.4s\n[CV 1/5] END max_features=None, n_estimators=500; AUC: (test=0.763) total time=  33.7s\n[CV 5/5] END max_features=None, n_estimators=500; AUC: (test=0.750) total time=  31.6s\n[CV 4/5] END max_features=None, n_estimators=1000; AUC: (test=0.767) total time= 1.0min\n[CV 3/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.782) total time=   8.5s\n[CV 1/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.780) total time=  17.7s\n[CV 4/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.779) total time=  17.4s\n[CV 4/5] END max_features=log2, n_estimators=200; AUC: (test=0.774) total time=   3.9s\n[CV 3/5] END max_features=log2, n_estimators=500; AUC: (test=0.782) total time=   9.1s\n[CV 2/5] END max_features=log2, n_estimators=1000; AUC: (test=0.776) total time=  17.4s\n[CV 2/5] END max_features=0.25, n_estimators=10; AUC: (test=0.714) total time=   0.2s\n[CV 4/5] END max_features=0.25, n_estimators=10; AUC: (test=0.697) total time=   0.2s\n[CV 1/5] END max_features=0.25, n_estimators=50; AUC: (test=0.770) total time=   0.9s\n[CV 3/5] END max_features=0.25, n_estimators=50; AUC: (test=0.772) total time=   0.9s\n[CV 5/5] END max_features=0.25, n_estimators=50; AUC: (test=0.750) total time=   0.9s\n[CV 2/5] END max_features=0.25, n_estimators=100; AUC: (test=0.766) total time=   1.7s\n[CV 4/5] END max_features=0.25, n_estimators=100; AUC: (test=0.769) total time=   1.9s\n[CV 1/5] END max_features=0.25, n_estimators=200; AUC: (test=0.779) total time=   3.8s\n[CV 4/5] END max_features=0.25, n_estimators=200; AUC: (test=0.774) total time=   3.4s\n[CV 2/5] END max_features=0.25, n_estimators=500; AUC: (test=0.776) total time=   8.1s\n[CV 1/5] END max_features=0.25, n_estimators=1000; AUC: (test=0.780) total time=  20.1s\n[CV 5/5] END max_features=0.25, n_estimators=1000; AUC: (test=0.766) total time=  18.4s\n[CV 5/5] END max_features=0.5, n_estimators=200; AUC: (test=0.759) total time=   6.3s\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning:\n\nA worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning:\n\n\n120 fits failed out of a total of 330.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 2.0 instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 3.0 instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 4.0 instead.\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_search.py:1051: UserWarning:\n\nOne or more of the test scores are non-finite: [0.7054542  0.75040485 0.75711448 0.75888311 0.76132513 0.76196086\n        nan        nan        nan        nan        nan        nan\n 0.70889806 0.76061223 0.76902557 0.77397572 0.77621517 0.7770484\n 0.70889806 0.76061223 0.76902557 0.77397572 0.77621517 0.7770484\n 0.70889806 0.76061223 0.76902557 0.77397572 0.77621517 0.7770484\n 0.7220481  0.75976207 0.76435102 0.76833309 0.77083125 0.7711869\n 0.71471084 0.75111362 0.75801595 0.76259915 0.76559531 0.76627665\n 0.7054542  0.75040485 0.75711448 0.75888311 0.76132513 0.76196086\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan]\n\n\n\nGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) ¬†RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) \n\n\n\n\nCode\nrf_cv_treatment.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 1000}\n\n\n\n\nCode\nrf_treatment = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 1\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    **rf_cv_treatment.best_params_\n)\nrf_treatment.summary()\n\n\nRandom Forest\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nmax_features         : sqrt (4)\nn_estimators         : 1000\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.775\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         5           15                 0                    179         362                       50                   0                  22            0             4               4              2      1308                   True                  False                       False                       False             True             False\n         4            4                 0                     36           0                        0                   0                   0            0             0               0              2      2922                  False                  False                       False                       False             True             False\n         8           17                 0                    222          20                       63                  10                  10            0             9               6              4      2192                   True                  False                        True                       False             True             False\n        10           18                 2                      0          56                        6                   2                   1            0             0               0             13      2313                  False                  False                       False                        True             True             False\n        10           20                 5                     36           0                       16                   0                   0            0             0               0              9      1766                  False                  False                       False                        True             True             False\n\n\n\n\nCode\nrf_control = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n)\nrf_control.summary()\n\n\nRandom Forest\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nmax_features         : sqrt (4)\nn_estimators         : 100\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.851\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         7           18                 0                    124           0                       81                   0                   8            0             0               4              3      2101                   True                  False                       False                        True            False             False\n        10            3                 2                     60         479                       18                   0                  10            7             0               0              7      1644                  False                  False                        True                       False             True             False\n         2            1                 0                      0           0                        0                   0                   0            0             0               2              8      3197                  False                  False                       False                       False             True              True\n         8           15                 0                      0          51                        6                   0                   0            0             2               1             21      2009                   True                  False                        True                       False             True             False\n        10           18                 0                      0           0                        0                   0                   0            0             0               0              6      3288                  False                  False                        True                       False             True             False\n\n\n\n\nModel Tuning\n\n\nCode\nrf_cv_control = GridSearchCV(rf_control.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5)\nrf_cv_control.fit(rf_control.data_onehot, rf_control.data.converted_yes)\n\n\nFitting 5 folds for each of 66 candidates, totalling 330 fits\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning:\n\n\n120 fits failed out of a total of 330.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 2.0 instead.\n\n--------------------------------------------------------------------------------\n22 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 3.0 instead.\n\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 3.0 instead.\n\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 4.0 instead.\n\n--------------------------------------------------------------------------------\n12 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 4.0 instead.\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_search.py:1051: UserWarning:\n\nOne or more of the test scores are non-finite: [0.79399629 0.84770861 0.85644669 0.85794913 0.86166784 0.86328154\n        nan        nan        nan        nan        nan        nan\n 0.79976758 0.86098559 0.86567103 0.87050655 0.87364622 0.8743026\n 0.79976758 0.86098559 0.86567103 0.87050655 0.87364622 0.8743026\n 0.79976758 0.86098559 0.86567103 0.87050655 0.87364622 0.8743026\n 0.80333525 0.85486483 0.86299353 0.8676931  0.86846026 0.86937865\n 0.79893787 0.84862995 0.85982764 0.86258705 0.86599233 0.8667254\n 0.79399629 0.84770861 0.85644669 0.85794913 0.86166784 0.86328154\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan]\n\n\n\nGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) ¬†RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) \n\n\n\n\nCode\nrf_cv_control.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 1000}\n\n\n\n\nCode\nrf_control = rsm.model.rforest(\n    data = {'cg_rct_stacked': cg_rct_stacked.query(\"training == 1 & ad == 0\")},\n    rvar = 'converted',\n    lev = 'yes',\n    evar = evar,\n    **rf_cv_control.best_params_\n)\nrf_control.summary()\n\n\nRandom Forest\nData                 : cg_rct_stacked\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 21,000\nmax_features         : sqrt (4)\nn_estimators         : 1000\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.873\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         7           18                 0                    124           0                       81                   0                   8            0             0               4              3      2101                   True                  False                       False                        True            False             False\n        10            3                 2                     60         479                       18                   0                  10            7             0               0              7      1644                  False                  False                        True                       False             True             False\n         2            1                 0                      0           0                        0                   0                   0            0             0               2              8      3197                  False                  False                       False                       False             True              True\n         8           15                 0                      0          51                        6                   0                   0            0             2               1             21      2009                   True                  False                        True                       False             True             False\n        10           18                 0                      0           0                        0                   0                   0            0             0               0              6      3288                  False                  False                        True                       False             True             False\n\n\n\n\nCode\n# Predictions\ncg_rct_stacked[\"pred_treatment_rf\"] = rf_treatment.predict(cg_rct_stacked)[\"prediction\"]\ncg_rct_stacked[\"pred_control_rf\"] = rf_control.predict(cg_rct_stacked)[\"prediction\"]\n\n\n\n\nCode\ncg_rct_stacked[\"uplift_score_rf\"] = (\n    cg_rct_stacked.pred_treatment_rf - cg_rct_stacked.pred_control_rf\n)\n\n\n\n\n3. Calculate the Uplift and Incremental Uplift\n\n\nCode\nuplift_tab_rf = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)\nuplift_tab_rf\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\nuplift_score_rf\n1\n0.05\n210\n449\n59\n579\n164.246978\n1.824966\n0.365806\n\n\n1\nuplift_score_rf\n2\n0.10\n357\n898\n96\n1113\n279.544474\n3.106050\n0.258106\n\n\n2\nuplift_score_rf\n3\n0.15\n478\n1347\n126\n1686\n377.334520\n4.192606\n0.217132\n\n\n3\nuplift_score_rf\n4\n0.20\n576\n1795\n156\n2199\n448.660300\n4.985114\n0.160270\n\n\n4\nuplift_score_rf\n5\n0.25\n666\n2237\n182\n2707\n515.599557\n5.728884\n0.152439\n\n\n5\nuplift_score_rf\n6\n0.30\n732\n2700\n203\n3201\n560.772259\n6.230803\n0.100038\n\n\n6\nuplift_score_rf\n7\n0.35\n777\n3146\n229\n3708\n582.708198\n6.474536\n0.049615\n\n\n7\nuplift_score_rf\n8\n0.40\n824\n3590\n239\n4186\n619.028667\n6.878096\n0.084935\n\n\n8\nuplift_score_rf\n9\n0.45\n855\n4040\n252\n4704\n638.571429\n7.095238\n0.043792\n\n\n9\nuplift_score_rf\n10\n0.50\n888\n4474\n258\n5157\n664.169866\n7.379665\n0.062792\n\n\n10\nuplift_score_rf\n11\n0.55\n916\n4930\n270\n5632\n679.654119\n7.551712\n0.036140\n\n\n11\nuplift_score_rf\n12\n0.60\n942\n5376\n279\n6138\n697.636364\n7.751515\n0.040509\n\n\n12\nuplift_score_rf\n13\n0.65\n968\n5818\n288\n6550\n712.185649\n7.913174\n0.036979\n\n\n13\nuplift_score_rf\n14\n0.70\n989\n6269\n290\n7006\n729.506709\n8.105630\n0.042177\n\n\n14\nuplift_score_rf\n15\n0.75\n1009\n6750\n292\n7499\n746.164955\n8.290722\n0.037523\n\n\n15\nuplift_score_rf\n16\n0.80\n1017\n7184\n299\n7898\n745.030387\n8.278115\n0.000889\n\n\n16\nuplift_score_rf\n17\n0.85\n1031\n7644\n303\n8322\n752.685652\n8.363174\n0.021001\n\n\n17\nuplift_score_rf\n18\n0.90\n1087\n8098\n353\n8662\n756.984530\n8.410939\n-0.023711\n\n\n18\nuplift_score_rf\n19\n0.95\n1139\n8548\n432\n8842\n721.364171\n8.015157\n-0.323333\n\n\n19\nuplift_score_rf\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n-0.428895\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe curve starts at 0% uplift when 0% of the population is targeted, which is expected as no one has yet been exposed to the campaign.\nAs the percentage of the targeted population increases, the incremental uplift also increases. This indicates that targeting more of the population is initially resulting in a higher incremental gain.\nThe curve shows a steep initial growth in uplift as the targeting begins, suggesting that the early segments of the population targeted are highly responsive to the campaign.\nAfter a certain point, the rate of increase in incremental uplift starts to diminish. This is seen as the curve begins to flatten, suggesting that the additional gains from targeting more of the population are decreasing.\nThe curve reaches a peak and then plateaus, indicating that there is an optimal point of targeting beyond which the incremental benefits do not increase significantly. This is typically where the marketer would aim to stop targeting additional customers to maximize efficiency and return on investment.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_rf\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nThe first few bars show a positive uplift, with the first bar indicating an uplift of approximately 30%. This suggests that the first segment of the population (likely the top 5% or 10%) responded very well to the intervention.\nAs moving right along the x-axis, the uplift decreases. This is expected as typically, the individuals most likely to respond are targeted first, and as moving through the population, the less responsive individuals are included.\nEventually, the uplift drops to 0% and then becomes negative. The negative bars at the end suggest that targeting those segments of the population may have been counterproductive, either because the intervention had an adverse effect or because it was an unnecessary expense for those individuals who might have taken the desired action without any intervention.\nThe most negative bar, located towards the right end of the chart, indicates a significant negative impact on that segment. This might represent a group that was either deterred by the campaign or where the cost of targeting outweighed the benefits.\n\n\n\n4. Using the incremental_resp to calculate the profits for Uplift model\n\n\nCode\nuplift_profit_rf = prof_calc(uplift_tab_rf, 14.99, 1.5)\nuplift_profit_rf\n\n\n58311.16473340722\n\n\n\n\n5. Calculate the uplift and Increatmental Uplift for Propensity Model\n\n\nCode\nprop_tab_rf = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_rf\", \"ad\", 1, qnt = 20\n)\n\n\n\n\nCompare Uplift model and Propensity model\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_rf\", \"uplift_score_rf\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nUplift Model: The line representing uplift_score_rf is consistently above the pred_treatment_rf line, suggesting that the uplift model identifies individuals who will respond to the treatment more effectively than the propensity model alone. This indicates that using the uplift model, the campaign would yield a higher incremental gain across the targeted population segments.\nPropensity Model: The pred_treatment_rf line shows that the propensity model does predict some level of uplift, but it is lower compared to the uplift model. This implies that while the propensity model identifies individuals likely to respond to the treatment, it does not do so as effectively as the uplift model when it comes to maximizing incremental uplift.\nDiminishing Returns: Both models show an increase in incremental uplift as a larger percentage of the population is targeted, but the rate of increase slows down, and both lines begin to plateau. This indicates diminishing returns; beyond a certain point, targeting additional segments of the population yields progressively smaller increases in uplift.\nOptimal Targeting Point: The point at which the curves start to plateau suggests the optimal targeting point for the campaign. Beyond this point, the additional cost of targeting more individuals may not be justified by the incremental gains.\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_rf\", \"uplift_score_rf\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nInitial Segments: The first few segments, representing the most responsive parts of the population, show a significant positive uplift for both models. This suggests that both models are effective at identifying the individuals most likely to be influenced by the campaign.\nDecreasing Uplift: As moving to the right, representing a larger share of the population being targeted, the uplift for both models decreases. This is typical in targeted marketing as the most responsive individuals are usually the first ones targeted.\nNegative Uplift: Towards the end segments, both models show negative uplift, which can indicate that targeting these individuals may have a counterproductive effect. It could mean that the campaign is reaching individuals who either would have made a purchase without the campaign or who may be turned off by the campaign.\nComparison Between Models: In most segments, the uplift_score_rf bars are higher than the pred_treatment_rf bars, suggesting that the uplift model is more effective at identifying which segments of the population will provide a higher incremental uplift when targeted.\n\n\n\n6. Using the Incremental_resp to calculate the profits for Propensity Model\n\n\nCode\npropensity_profit_rf = prof_calc(prop_tab_rf, 14.99, 1.5)\npropensity_profit_rf\n\n\n48633.38627925748\n\n\n\n\nCode\n# Difference in profits from using uplift model and propensity model\ndifference_rf = uplift_profit_rf - propensity_profit_rf\ndifference_rf\n\n\n9677.778454149739\n\n\n\n\nUsing XGBoost Model\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \nimport xgboost as xgb\n# Create X_train, X_test, y_train, y_test for treatment group\nX_train_treatment = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 1) & (cg_rct_stacked[\"ad\"] == 1), evar]\ny_train_treatment = cg_rct_stacked.query(\"training == 1 & ad == 1\").converted_yes\n\nX_test_treatment = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 0) & (cg_rct_stacked[\"ad\"] == 1), evar]\ny_test_treatment = cg_rct_stacked.query(\"training == 0 & ad == 1\").converted_yes\n\n\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\n# Setup model\nxgbc_treatment = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_treatment.fit(X_train_treatment, y_train_treatment)\n\n# Set up and fit GridSearchCV\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n    'min_child_weight': [1, 2, 3, 4, 5, 6]\n}\n\nxgbc_cv_treatment = GridSearchCV(xgbc_treatment, param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=5)\nxgbc_cv_treatment.fit(X_train_treatment, y_train_treatment)\n\n# Retrieve the best parameters and retrain the model\nbest_params_treatment = xgbc_cv_treatment.best_params_\nxgbc_treatment = xgb.XGBClassifier(**best_params_treatment, use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_treatment.fit(X_train_treatment, y_train_treatment)\n\n\nFitting 5 folds for each of 192 candidates, totalling 960 fits\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.2s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.737) total time=   0.7s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.754) total time=   3.6s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.778) total time=   6.8s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(3, 3); AUC: (test=0.759) total time=  17.7s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.794) total time=  25.6s\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.775) total time=  24.7s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.729) total time= 1.8min\n[CV 4/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.719) total time=  29.2s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.778) total time=   4.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.757) total time=  17.6s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.777) total time=  16.1s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.768) total time=  27.3s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.757) total time=  11.3s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(5,); AUC: (test=0.788) total time=   3.2s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.752) total time=   4.4s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.748) total time=   2.8s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.764) total time=  34.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.747) total time=  37.6s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.721) total time=  23.0s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.691) total time=  10.8s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.746) total time=   5.8s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.775) total time=  10.9s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.778) total time=  14.7s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.772) total time=  12.7s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.766) total time=  46.6s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.764) total time=  43.2s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.732) total time=  18.9s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.711) total time=   0.1s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.719) total time=   0.1s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.712) total time=   0.1s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.703) total time=   0.1s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(1,); AUC: (test=0.699) total time=   0.1s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.751) total time=   0.4s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.734) total time=   0.3s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.747) total time=   2.2s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.751) total time=   0.7s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(2,); AUC: (test=0.738) total time=   2.9s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.732) total time=   0.5s\n[CV 3/5] END alpha=0.1, hidden_layer_sizes=(3,); AUC: (test=0.756) total time=   1.2s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.778) total time=   4.5s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.755) total time=  22.0s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.751) total time=   3.0s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.775) total time=  19.1s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.765) total time=  12.1s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5,); AUC: (test=0.769) total time=   2.0s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.753) total time=   2.0s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.777) total time=  20.3s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.750) total time=  27.9s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.730) total time=   0.3s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(3,); AUC: (test=0.730) total time=   1.1s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.769) total time=   6.1s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.771) total time=   2.8s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.761) total time=   5.3s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.773) total time=   7.6s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.777) total time=  20.4s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.744) total time=   5.6s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(10, 5); AUC: (test=0.754) total time=   5.0s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(1,); AUC: (test=0.826) total time=   0.2s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(2,); AUC: (test=0.836) total time=   0.2s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.839) total time=   0.3s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(3,); AUC: (test=0.836) total time=   0.3s\n[CV 1/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.820) total time=   2.8s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(4, 2); AUC: (test=0.818) total time=   0.6s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.821) total time=   7.8s\n[CV 3/5] END alpha=0.0001, hidden_layer_sizes=(5, 5); AUC: (test=0.816) total time=   5.4s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(5, 10); AUC: (test=0.822) total time=  15.7s\n[CV 2/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.779) total time=  12.4s\n[CV 5/5] END alpha=0.0001, hidden_layer_sizes=(10, 5); AUC: (test=0.809) total time=   4.1s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(3,); AUC: (test=0.819) total time=   0.6s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.839) total time=   1.8s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.846) total time=   0.9s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(3, 3); AUC: (test=0.814) total time=   4.2s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.820) total time=   2.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(4, 2); AUC: (test=0.875) total time=   2.8s\n[CV 3/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.820) total time=   3.2s\n[CV 5/5] END alpha=0.001, hidden_layer_sizes=(5, 5); AUC: (test=0.825) total time=   4.0s\n[CV 1/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.864) total time=   1.4s\n[CV 4/5] END alpha=0.001, hidden_layer_sizes=(10,); AUC: (test=0.783) total time=   1.8s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(5, 10); AUC: (test=0.816) total time=  14.5s\n[CV 2/5] END alpha=0.001, hidden_layer_sizes=(10, 5); AUC: (test=0.776) total time=  19.5s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.826) total time=   0.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(1,); AUC: (test=0.849) total time=   0.1s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.841) total time=   0.3s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.836) total time=   0.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(2,); AUC: (test=0.853) total time=   0.3s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.837) total time=   0.3s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(3,); AUC: (test=0.819) total time=   0.3s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.846) total time=   2.8s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(3, 3); AUC: (test=0.816) total time=   3.6s\n[CV 2/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.814) total time=   1.2s\n[CV 4/5] END alpha=0.01, hidden_layer_sizes=(4, 2); AUC: (test=0.798) total time=   1.9s\n[CV 1/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.825) total time=   2.9s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(5, 5); AUC: (test=0.838) total time=   5.2s\n[CV 3/5] END alpha=0.01, hidden_layer_sizes=(5, 10); AUC: (test=0.823) total time=  27.1s\n[CV 5/5] END alpha=0.01, hidden_layer_sizes=(10, 5); AUC: (test=0.819) total time=   5.3s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.839) total time=   1.1s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(3, 3); AUC: (test=0.815) total time=   1.1s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.829) total time=   1.2s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.818) total time=   0.6s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(4, 2); AUC: (test=0.810) total time=   1.0s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.826) total time=   2.6s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.808) total time=   1.9s\n[CV 5/5] END alpha=0.1, hidden_layer_sizes=(5, 5); AUC: (test=0.845) total time=   4.5s\n[CV 2/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.818) total time=   1.1s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(10,); AUC: (test=0.800) total time=   2.4s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.796) total time=   7.3s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(5, 10); AUC: (test=0.777) total time=   7.6s\n[CV 1/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.809) total time=   5.9s\n[CV 4/5] END alpha=0.1, hidden_layer_sizes=(10, 5); AUC: (test=0.791) total time=   5.3s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(3, 3); AUC: (test=0.823) total time=   1.8s\n[CV 3/5] END alpha=1, hidden_layer_sizes=(4, 2); AUC: (test=0.827) total time=   1.2s\n[CV 1/5] END alpha=1, hidden_layer_sizes=(5, 5); AUC: (test=0.842) total time=   3.3s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.845) total time=   0.4s\n[CV 4/5] END alpha=1, hidden_layer_sizes=(5,); AUC: (test=0.834) total time=   0.9s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(10,); AUC: (test=0.838) total time=   2.2s\n[CV 2/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.826) total time=   3.4s\n[CV 5/5] END alpha=1, hidden_layer_sizes=(5, 10); AUC: (test=0.832) total time=   4.8s\n[CV 1/5] END max_features=None, n_estimators=10; AUC: (test=0.710) total time=   0.5s\n[CV 2/5] END max_features=None, n_estimators=50; AUC: (test=0.751) total time=   3.1s\n[CV 2/5] END max_features=None, n_estimators=100; AUC: (test=0.756) total time=   6.6s\n[CV 1/5] END max_features=None, n_estimators=200; AUC: (test=0.760) total time=  13.5s\n[CV 5/5] END max_features=None, n_estimators=200; AUC: (test=0.750) total time=  13.6s\n[CV 4/5] END max_features=None, n_estimators=500; AUC: (test=0.767) total time=  32.4s\n[CV 3/5] END max_features=None, n_estimators=1000; AUC: (test=0.766) total time= 1.1min\n[CV 1/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.773) total time=   1.7s\n[CV 2/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.766) total time=   1.7s\n[CV 5/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.756) total time=   1.7s\n[CV 1/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.779) total time=   3.3s\n[CV 4/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.774) total time=   3.4s\n[CV 5/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.761) total time=   3.3s\n[CV 2/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.776) total time=   8.4s\n[CV 5/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.765) total time=   9.1s\n[CV 3/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.783) total time=  17.2s\n[CV 1/5] END max_features=log2, n_estimators=10; AUC: (test=0.721) total time=   0.2s\n[CV 2/5] END max_features=log2, n_estimators=10; AUC: (test=0.714) total time=   0.2s\n[CV 3/5] END max_features=log2, n_estimators=10; AUC: (test=0.711) total time=   0.2s\n[CV 4/5] END max_features=log2, n_estimators=10; AUC: (test=0.697) total time=   0.2s\n[CV 5/5] END max_features=log2, n_estimators=10; AUC: (test=0.703) total time=   0.2s\n[CV 1/5] END max_features=log2, n_estimators=50; AUC: (test=0.770) total time=   0.9s\n[CV 3/5] END max_features=log2, n_estimators=50; AUC: (test=0.772) total time=   0.8s\n[CV 5/5] END max_features=log2, n_estimators=50; AUC: (test=0.750) total time=   0.9s\n[CV 1/5] END max_features=log2, n_estimators=100; AUC: (test=0.773) total time=   1.7s\n[CV 3/5] END max_features=log2, n_estimators=100; AUC: (test=0.780) total time=   1.7s\n[CV 5/5] END max_features=log2, n_estimators=100; AUC: (test=0.756) total time=   1.7s\n[CV 2/5] END max_features=log2, n_estimators=200; AUC: (test=0.774) total time=   3.8s\n[CV 5/5] END max_features=log2, n_estimators=200; AUC: (test=0.761) total time=   3.9s\n[CV 4/5] END max_features=log2, n_estimators=500; AUC: (test=0.778) total time=   8.7s\n[CV 3/5] END max_features=log2, n_estimators=1000; AUC: (test=0.783) total time=  17.0s\n[CV 1/5] END max_features=0.25, n_estimators=10; AUC: (test=0.721) total time=   0.2s\n[CV 3/5] END max_features=0.25, n_estimators=10; AUC: (test=0.711) total time=   0.2s\n[CV 5/5] END max_features=0.25, n_estimators=10; AUC: (test=0.703) total time=   0.2s\n[CV 2/5] END max_features=0.25, n_estimators=50; AUC: (test=0.755) total time=   0.9s\n[CV 4/5] END max_features=0.25, n_estimators=50; AUC: (test=0.757) total time=   1.0s\n[CV 1/5] END max_features=0.25, n_estimators=100; AUC: (test=0.773) total time=   1.7s\n[CV 3/5] END max_features=0.25, n_estimators=100; AUC: (test=0.780) total time=   1.8s\n[CV 5/5] END max_features=0.25, n_estimators=100; AUC: (test=0.756) total time=   1.9s\n[CV 3/5] END max_features=0.25, n_estimators=200; AUC: (test=0.782) total time=   3.6s\n[CV 5/5] END max_features=0.25, n_estimators=200; AUC: (test=0.761) total time=   3.3s\n[CV 3/5] END max_features=0.25, n_estimators=500; AUC: (test=0.782) total time=   8.0s\n[CV 2/5] END max_features=0.25, n_estimators=1000; AUC: (test=0.776) total time=  19.9s\n[CV 1/5] END max_features=0.5, n_estimators=10; AUC: (test=0.727) total time=   0.4s\n[CV 2/5] END max_features=0.5, n_estimators=10; AUC: (test=0.723) total time=   0.4s\n[CV 3/5] END max_features=0.5, n_estimators=10; AUC: (test=0.713) total time=   0.4s\n[CV 4/5] END max_features=0.5, n_estimators=10; AUC: (test=0.729) total time=   0.3s\n[CV 5/5] END max_features=0.5, n_estimators=10; AUC: (test=0.717) total time=   0.3s\n[CV 1/5] END max_features=0.5, n_estimators=50; AUC: (test=0.764) total time=   1.7s\n[CV 3/5] END max_features=0.5, n_estimators=50; AUC: (test=0.764) total time=   1.7s\n[CV 5/5] END max_features=0.5, n_estimators=50; AUC: (test=0.751) total time=   1.6s\n[CV 3/5] END max_features=0.5, n_estimators=100; AUC: (test=0.767) total time=   3.3s\n[CV 1/5] END max_features=0.5, n_estimators=200; AUC: (test=0.768) total time=   6.2s\n[CV 4/5] END max_features=0.5, n_estimators=200; AUC: (test=0.772) total time=   6.4s\n[CV 3/5] END max_features=0.5, n_estimators=500; AUC: (test=0.774) total time=  15.7s\n[CV 2/5] END max_features=0.5, n_estimators=1000; AUC: (test=0.771) total time=  30.5s\n[CV 1/5] END max_features=0.75, n_estimators=10; AUC: (test=0.726) total time=   0.5s\n[CV 2/5] END max_features=0.75, n_estimators=10; AUC: (test=0.711) total time=   0.5s\n[CV 3/5] END max_features=0.75, n_estimators=10; AUC: (test=0.718) total time=   0.5s\n[CV 4/5] END max_features=0.75, n_estimators=10; AUC: (test=0.716) total time=   0.5s\n[CV 5/5] END max_features=0.75, n_estimators=10; AUC: (test=0.702) total time=   0.4s\n[CV 1/5] END max_features=0.75, n_estimators=50; AUC: (test=0.756) total time=   2.2s\n[CV 3/5] END max_features=0.75, n_estimators=50; AUC: (test=0.751) total time=   2.1s\n[CV 5/5] END max_features=0.75, n_estimators=50; AUC: (test=0.745) total time=   2.2s\n[CV 2/5] END max_features=0.75, n_estimators=100; AUC: (test=0.760) total time=   4.2s\n[CV 5/5] END max_features=0.75, n_estimators=100; AUC: (test=0.751) total time=   4.4s\n[CV 3/5] END max_features=0.75, n_estimators=200; AUC: (test=0.767) total time=   8.8s\n[CV 2/5] END max_features=0.75, n_estimators=500; AUC: (test=0.767) total time=  25.9s\n[CV 5/5] END max_features=0.75, n_estimators=500; AUC: (test=0.755) total time=  23.7s\n[CV 4/5] END max_features=0.75, n_estimators=1000; AUC: (test=0.771) total time=  51.3s\n[CV 5/5] END max_features=1.0, n_estimators=100; AUC: (test=0.752) total time=   6.6s\n[CV 2/5] END max_features=1.0, n_estimators=200; AUC: (test=0.759) total time=  11.6s\n[CV 1/5] END max_features=1.0, n_estimators=500; AUC: (test=0.763) total time=  28.8s\n[CV 5/5] END max_features=1.0, n_estimators=500; AUC: (test=0.750) total time=  31.8s\n[CV 4/5] END max_features=1.0, n_estimators=1000; AUC: (test=0.767) total time= 1.0min\n[CV 2/5] END max_features=None, n_estimators=10; AUC: (test=0.785) total time=   0.5s\n[CV 1/5] END max_features=None, n_estimators=50; AUC: (test=0.856) total time=   2.2s\n[CV 5/5] END max_features=None, n_estimators=50; AUC: (test=0.853) total time=   2.4s\n[CV 4/5] END max_features=None, n_estimators=100; AUC: (test=0.848) total time=   5.1s\n[CV 3/5] END max_features=None, n_estimators=200; AUC: (test=0.858) total time=   9.4s\n[CV 2/5] END max_features=None, n_estimators=500; AUC: (test=0.859) total time=  25.0s\n[CV 1/5] END max_features=None, n_estimators=1000; AUC: (test=0.866) total time=  52.0s\n[CV 5/5] END max_features=None, n_estimators=1000; AUC: (test=0.871) total time= 1.0min\n[CV 1/5] END max_features=log2, n_estimators=10; AUC: (test=0.810) total time=   0.2s\n[CV 2/5] END max_features=log2, n_estimators=10; AUC: (test=0.786) total time=   0.2s\n[CV 3/5] END max_features=log2, n_estimators=10; AUC: (test=0.810) total time=   0.2s\n[CV 4/5] END max_features=log2, n_estimators=10; AUC: (test=0.783) total time=   0.2s\n[CV 5/5] END max_features=log2, n_estimators=10; AUC: (test=0.810) total time=   0.2s\n[CV 1/5] END max_features=log2, n_estimators=50; AUC: (test=0.870) total time=   0.8s\n[CV 3/5] END max_features=log2, n_estimators=50; AUC: (test=0.865) total time=   0.8s\n[CV 5/5] END max_features=log2, n_estimators=50; AUC: (test=0.862) total time=   0.8s\n[CV 2/5] END max_features=log2, n_estimators=100; AUC: (test=0.864) total time=   1.7s\n[CV 4/5] END max_features=log2, n_estimators=100; AUC: (test=0.856) total time=   1.5s\n[CV 1/5] END max_features=log2, n_estimators=200; AUC: (test=0.875) total time=   3.6s\n[CV 3/5] END max_features=log2, n_estimators=200; AUC: (test=0.872) total time=   3.2s\n[CV 2/5] END max_features=log2, n_estimators=500; AUC: (test=0.872) total time=   8.3s\n[CV 1/5] END max_features=log2, n_estimators=1000; AUC: (test=0.874) total time=  18.9s\n[CV 5/5] END max_features=log2, n_estimators=1000; AUC: (test=0.882) total time=  17.3s\n[CV 4/5] END max_features=0.25, n_estimators=500; AUC: (test=0.867) total time=   9.0s\n[CV 3/5] END max_features=0.25, n_estimators=1000; AUC: (test=0.876) total time=  18.1s\n[CV 3/5] END max_features=0.5, n_estimators=50; AUC: (test=0.861) total time=   1.4s\n[CV 5/5] END max_features=0.5, n_estimators=50; AUC: (test=0.863) total time=   1.4s\n[CV 2/5] END max_features=0.5, n_estimators=100; AUC: (test=0.860) total time=   3.0s\n[CV 5/5] END max_features=0.5, n_estimators=100; AUC: (test=0.877) total time=   2.7s\n[CV 3/5] END max_features=0.5, n_estimators=200; AUC: (test=0.869) total time=   5.4s\n[CV 2/5] END max_features=0.5, n_estimators=500; AUC: (test=0.866) total time=  14.6s\n[CV 1/5] END max_features=0.5, n_estimators=1000; AUC: (test=0.871) total time=  31.1s\n[CV 5/5] END max_features=0.5, n_estimators=1000; AUC: (test=0.880) total time=  30.6s\n[CV 2/5] END max_features=0.75, n_estimators=500; AUC: (test=0.863) total time=  26.7s\n[CV 1/5] END max_features=0.75, n_estimators=1000; AUC: (test=0.869) total time=  41.9s\n[CV 5/5] END max_features=0.75, n_estimators=1000; AUC: (test=0.875) total time=  43.2s\n[CV 2/5] END max_features=1.0, n_estimators=500; AUC: (test=0.859) total time=  34.2s\n[CV 1/5] END max_features=1.0, n_estimators=1000; AUC: (test=0.866) total time=  48.3s\n[CV 5/5] END max_features=1.0, n_estimators=1000; AUC: (test=0.871) total time=  45.3s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.773 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.762 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.771 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.753 total time=   0.1s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=4;, score=0.773 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.762 total time=   0.1s\n[CV 2/5] END learning_rate=0.01, max_depth=3, min_child_weight=6;, score=0.771 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.768 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.757 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=3;, score=0.768 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.774 total time=   0.1s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.757 total time=   0.1s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.779 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.768 total time=   0.1s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.777 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=2;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.774 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.758 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.780 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=5;, score=0.780 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.772 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.773 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.776 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.774 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.763 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=4;, score=0.780 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.777 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.776 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.767 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.782 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=4;, score=0.776 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.765 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.779 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=1;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.773 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.774 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.780 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.764 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.778 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=6;, score=0.779 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.773 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.772 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.764 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=3;, score=0.779 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.775 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.774 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.772 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.759 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=2;, score=0.776 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.776 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=4;, score=0.772 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.781 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=10, min_child_weight=5;, score=0.761 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=6;, score=0.780 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=1;, score=0.768 total time=   0.1s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=2;, score=0.780 total time=   0.1s\n[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=3;, score=0.779 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=4;, score=0.783 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.775 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=5;, score=0.767 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=6;, score=0.781 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=4, min_child_weight=1;, score=0.780 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=4, min_child_weight=2;, score=0.785 total time=   0.1s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.778 total time=   0.1s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=3;, score=0.768 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=4;, score=0.769 total time=   0.1s\n[CV 4/5] END learning_rate=0.05, max_depth=4, min_child_weight=5;, score=0.786 total time=   0.1s\n[CV 1/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.779 total time=   0.1s\n[CV 5/5] END learning_rate=0.05, max_depth=4, min_child_weight=6;, score=0.768 total time=   0.1s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=1;, score=0.788 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=2;, score=0.782 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.780 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=3;, score=0.769 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=4;, score=0.789 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=5;, score=0.782 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.782 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=6;, score=0.769 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=1;, score=0.789 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=2;, score=0.782 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.779 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=6, min_child_weight=3;, score=0.769 total time=   0.2s\n[CV 4/5] END learning_rate=0.05, max_depth=6, min_child_weight=4;, score=0.788 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=6, min_child_weight=5;, score=0.786 total time=   0.2s\n[CV 2/5] END learning_rate=0.05, max_depth=6, min_child_weight=6;, score=0.779 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.780 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=1;, score=0.771 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=2;, score=0.788 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=3;, score=0.781 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=7, min_child_weight=4;, score=0.782 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.776 total time=   0.2s\n[CV 5/5] END learning_rate=0.05, max_depth=7, min_child_weight=5;, score=0.773 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=7, min_child_weight=6;, score=0.788 total time=   0.2s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=1;, score=0.775 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=2;, score=0.772 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.782 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=8, min_child_weight=3;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=8, min_child_weight=4;, score=0.787 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=8, min_child_weight=5;, score=0.780 total time=   0.3s\n[CV 2/5] END learning_rate=0.05, max_depth=8, min_child_weight=6;, score=0.775 total time=   0.2s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.778 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=1;, score=0.768 total time=   0.5s\n[CV 4/5] END learning_rate=0.05, max_depth=9, min_child_weight=2;, score=0.789 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=9, min_child_weight=3;, score=0.774 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=9, min_child_weight=4;, score=0.774 total time=   0.3s\n[CV 1/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.783 total time=   0.4s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=5;, score=0.769 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=9, min_child_weight=6;, score=0.766 total time=   0.4s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=1;, score=0.769 total time=   0.5s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=2;, score=0.770 total time=   0.4s\n[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=3;, score=0.772 total time=   0.4s\n[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.782 total time=   0.3s\n[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4;, score=0.767 total time=   0.3s\n[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=5;, score=0.785 total time=   0.3s\n[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=6;, score=0.779 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=1;, score=0.784 total time=   0.1s\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.775 total time=   0.1s\n[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=2;, score=0.769 total time=   0.1s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=3;, score=0.784 total time=   0.1s\n[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=4;, score=0.778 total time=   0.1s\n[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=5;, score=0.786 total time=   0.1s\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nFor control group\n\n\nCode\n# Create X_train, X_test, y_train, y_test for control group\nX_train_control = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 1) & (cg_rct_stacked[\"ad\"] == 0), evar]\ny_train_control = cg_rct_stacked.query(\"training == 1 & ad == 0\").converted_yes\n\nX_test_control = cg_rct_stacked.loc[(cg_rct_stacked[\"training\"] == 0) & (cg_rct_stacked[\"ad\"] == 0), evar]\ny_test_control = cg_rct_stacked.query(\"training == 0 & ad == 0\").converted_yes\n\n\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n# Use the same param_grid as the treatment group\nxgbc_control = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\n\n# Set up and fit GridSearchCV\nxgbc_cv_control = GridSearchCV(xgbc_control, param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=5)\nxgbc_cv_control.fit(X_train_control, y_train_control)\n\n# Retrieve the best parameters and retrain the model\nbest_params_control = xgbc_cv_control.best_params_\nxgbc_control = xgb.XGBClassifier(**best_params_control, use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\nxgbc_control.fit(X_train_control, y_train_control)\n\n\nFitting 5 folds for each of 192 candidates, totalling 960 fits\n[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.777 total time=   0.1s\n[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=6;, score=0.785 total time=   0.1s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=1;, score=0.782 total time=   0.1s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=2;, score=0.780 total time=   0.1s\n[CV 1/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.780 total time=   0.1s\n[CV 5/5] END learning_rate=0.1, max_depth=4, min_child_weight=3;, score=0.769 total time=   0.2s\n[CV 4/5] END learning_rate=0.1, max_depth=4, min_child_weight=4;, score=0.783 total time=   0.2s\n[CV 3/5] END learning_rate=0.1, max_depth=4, min_child_weight=5;, score=0.783 total time=   0.1s\n[CV 2/5] END learning_rate=0.1, max_depth=4, min_child_weight=6;, score=0.782 total time=   0.1s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.779 total time=   0.2s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=1;, score=0.767 total time=   0.2s\n[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=2;, score=0.788 total time=   0.2s\n[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=3;, score=0.779 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=4;, score=0.777 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=5;, score=0.779 total time=   0.2s\n[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.780 total time=   0.2s\n[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=6;, score=0.771 total time=   0.2s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=1;, score=0.787 total time=   0.2s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=2;, score=0.781 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=6, min_child_weight=3;, score=0.775 total time=   0.2s\n[CV 1/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.785 total time=   0.2s\n[CV 5/5] END learning_rate=0.1, max_depth=6, min_child_weight=4;, score=0.767 total time=   0.2s\n[CV 4/5] END learning_rate=0.1, max_depth=6, min_child_weight=5;, score=0.786 total time=   0.2s\n[CV 3/5] END learning_rate=0.1, max_depth=6, min_child_weight=6;, score=0.784 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=1;, score=0.769 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.782 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=7, min_child_weight=2;, score=0.769 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=7, min_child_weight=3;, score=0.783 total time=   0.2s\n[CV 3/5] END learning_rate=0.1, max_depth=7, min_child_weight=4;, score=0.778 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=5;, score=0.773 total time=   0.2s\n[CV 2/5] END learning_rate=0.1, max_depth=7, min_child_weight=6;, score=0.770 total time=   0.2s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.776 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=8, min_child_weight=1;, score=0.765 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=8, min_child_weight=2;, score=0.781 total time=   0.6s\n[CV 3/5] END learning_rate=0.1, max_depth=8, min_child_weight=3;, score=0.776 total time=   0.7s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=4;, score=0.770 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=8, min_child_weight=5;, score=0.781 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=8, min_child_weight=6;, score=0.768 total time=   0.3s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=1;, score=0.770 total time=   0.4s\n[CV 1/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.777 total time=   0.3s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=2;, score=0.763 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=3;, score=0.767 total time=   0.5s\n[CV 5/5] END learning_rate=0.1, max_depth=9, min_child_weight=4;, score=0.761 total time=   0.3s\n[CV 4/5] END learning_rate=0.1, max_depth=9, min_child_weight=5;, score=0.780 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=9, min_child_weight=6;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=1;, score=0.758 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.769 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=2;, score=0.778 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=3;, score=0.765 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=4;, score=0.765 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=5;, score=0.766 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=6;, score=0.762 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.776 total time=   0.1s\n[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.780 total time=   0.1s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=1;, score=0.768 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=3;, score=0.774 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.777 total time=   0.1s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=4;, score=0.783 total time=   0.2s\n[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=5;, score=0.771 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=6;, score=0.783 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=1;, score=0.783 total time=   0.2s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=2;, score=0.776 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=4, min_child_weight=3;, score=0.772 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.775 total time=   0.2s\n[CV 5/5] END learning_rate=0.2, max_depth=4, min_child_weight=4;, score=0.765 total time=   0.1s\n[CV 4/5] END learning_rate=0.2, max_depth=4, min_child_weight=5;, score=0.782 total time=   0.1s\n[CV 3/5] END learning_rate=0.2, max_depth=4, min_child_weight=6;, score=0.781 total time=   0.1s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=1;, score=0.764 total time=   0.2s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=2;, score=0.767 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.773 total time=   0.2s\n[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=3;, score=0.760 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=4;, score=0.777 total time=   0.2s\n[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=5;, score=0.774 total time=   0.2s\n[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=6;, score=0.773 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.768 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=1;, score=0.760 total time=   0.2s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=2;, score=0.756 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=6, min_child_weight=3;, score=0.777 total time=   0.2s\n[CV 3/5] END learning_rate=0.2, max_depth=6, min_child_weight=4;, score=0.769 total time=   0.2s\n[CV 2/5] END learning_rate=0.2, max_depth=6, min_child_weight=5;, score=0.765 total time=   0.2s\n[CV 1/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.774 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=6, min_child_weight=6;, score=0.761 total time=   0.2s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=1;, score=0.771 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=7, min_child_weight=2;, score=0.758 total time=   0.3s\n[CV 2/5] END learning_rate=0.2, max_depth=7, min_child_weight=3;, score=0.761 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=4;, score=0.775 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.767 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=7, min_child_weight=5;, score=0.759 total time=   0.3s\n[CV 4/5] END learning_rate=0.2, max_depth=7, min_child_weight=6;, score=0.772 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=1;, score=0.751 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=8, min_child_weight=2;, score=0.753 total time=   0.5s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=3;, score=0.763 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.763 total time=   0.3s\n[CV 5/5] END learning_rate=0.2, max_depth=8, min_child_weight=4;, score=0.755 total time=   0.4s\n[CV 4/5] END learning_rate=0.2, max_depth=8, min_child_weight=5;, score=0.775 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=8, min_child_weight=6;, score=0.763 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=1;, score=0.752 total time=   0.9s\n[CV 1/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.760 total time=   0.9s\n[CV 5/5] END learning_rate=0.2, max_depth=9, min_child_weight=2;, score=0.756 total time=   0.8s\n[CV 4/5] END learning_rate=0.2, max_depth=9, min_child_weight=3;, score=0.765 total time=   0.4s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=4;, score=0.760 total time=   0.3s\n[CV 3/5] END learning_rate=0.2, max_depth=9, min_child_weight=5;, score=0.753 total time=   0.6s\n[CV 2/5] END learning_rate=0.2, max_depth=9, min_child_weight=6;, score=0.743 total time=   0.7s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.759 total time=   0.7s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=1;, score=0.747 total time=   0.5s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=2;, score=0.754 total time=   0.5s\n[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=3;, score=0.759 total time=   0.5s\n[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=4;, score=0.757 total time=   0.4s\n[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=5;, score=0.750 total time=   0.4s\n[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.760 total time=   0.4s\n[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=6;, score=0.748 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=1;, score=0.861 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=2;, score=0.860 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.860 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=3;, score=0.849 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.861 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=3, min_child_weight=5;, score=0.850 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.870 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=1;, score=0.863 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.859 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=2;, score=0.878 total time=   0.1s\n[CV 2/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.863 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=4, min_child_weight=4;, score=0.870 total time=   0.1s\n[CV 5/5] END learning_rate=0.01, max_depth=4, min_child_weight=5;, score=0.877 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=4, min_child_weight=6;, score=0.870 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.880 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=1;, score=0.867 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.876 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=3;, score=0.868 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.868 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=5, min_child_weight=4;, score=0.882 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.869 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=5, min_child_weight=6;, score=0.881 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=1;, score=0.883 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=2;, score=0.879 total time=   0.2s\n[CV 3/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.877 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=3;, score=0.869 total time=   0.2s\n[CV 1/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.878 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=6, min_child_weight=5;, score=0.870 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.870 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=6, min_child_weight=6;, score=0.884 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=1;, score=0.885 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=2;, score=0.878 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.879 total time=   0.2s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=3;, score=0.866 total time=   0.3s\n[CV 1/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.878 total time=   0.2s\n[CV 2/5] END learning_rate=0.01, max_depth=7, min_child_weight=5;, score=0.871 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.870 total time=   0.2s\n[CV 5/5] END learning_rate=0.01, max_depth=7, min_child_weight=6;, score=0.885 total time=   0.3s\n[CV 2/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.873 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=2;, score=0.878 total time=   0.3s\n[CV 5/5] END learning_rate=0.01, max_depth=8, min_child_weight=3;, score=0.883 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=8, min_child_weight=4;, score=0.877 total time=   0.3s\n[CV 3/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.880 total time=   0.3s\n[CV 4/5] END learning_rate=0.01, max_depth=8, min_child_weight=5;, score=0.868 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.868 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=1;, score=0.868 total time=   0.5s\n[CV 4/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.869 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=2;, score=0.879 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.873 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=9, min_child_weight=4;, score=0.880 total time=   0.5s\n[CV 5/5] END learning_rate=0.01, max_depth=9, min_child_weight=5;, score=0.883 total time=   0.5s\n[CV 1/5] END learning_rate=0.01, max_depth=9, min_child_weight=6;, score=0.877 total time=   0.5s\n[CV 3/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.878 total time=   0.7s\n[CV 4/5] END learning_rate=0.01, max_depth=10, min_child_weight=1;, score=0.868 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.870 total time=   0.5s\n[CV 2/5] END learning_rate=0.01, max_depth=10, min_child_weight=3;, score=0.874 total time=   0.4s\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n              max_leaves=None, min_child_weight=5, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n              max_leaves=None, min_child_weight=5, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nCode\nX_full = cg_rct_stacked[evar]\ncg_rct_stacked[\"pred_treatment_xgb\"] = xgbc_treatment.predict_proba(X_full)[:, 1]\ncg_rct_stacked[\"pred_control_xgb\"] = xgbc_control.predict_proba(X_full)[:, 1]\n\n\n\n\n3. Calculate the Uplift and Incremental Uplift\n\n\nCode\ncg_rct_stacked[\"uplift_score_xgb\"] = (\n    cg_rct_stacked.pred_treatment_xgb - cg_rct_stacked.pred_control_xgb\n)\ncg_rct_stacked['uplift_score_xgb']\n\n\n0        0.042117\n1        0.043018\n2        0.004007\n3        0.034363\n4       -0.059559\n           ...   \n29995    0.004342\n29996   -0.001307\n29997    0.225445\n29998    0.159661\n29999    0.121746\nName: uplift_score_xgb, Length: 60000, dtype: float32\n\n\n\n\nCode\ncg_rct_stacked['uplift_score_xgb'].describe()\n\n\ncount    60000.000000\nmean         0.065285\nstd          0.159799\nmin         -0.842148\n25%          0.018705\n50%          0.053788\n75%          0.125166\nmax          0.792246\nName: uplift_score_xgb, dtype: float64\n\n\n\n\nCode\nuplift_tab_xgb = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)\nuplift_tab_xgb\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\nuplift_score_xgb\n1\n0.05\n217\n450\n81\n625\n158.680000\n1.763111\n0.352622\n\n\n1\nuplift_score_xgb\n2\n0.10\n357\n900\n110\n1118\n268.449016\n2.982767\n0.252288\n\n\n2\nuplift_score_xgb\n3\n0.15\n475\n1350\n136\n1639\n362.980476\n4.033116\n0.212318\n\n\n3\nuplift_score_xgb\n4\n0.20\n597\n1800\n167\n2159\n457.768874\n5.086321\n0.211496\n\n\n4\nuplift_score_xgb\n5\n0.25\n681\n2250\n199\n2674\n513.554226\n5.706158\n0.124531\n\n\n5\nuplift_score_xgb\n6\n0.30\n747\n2700\n209\n3214\n571.424393\n6.349160\n0.128148\n\n\n6\nuplift_score_xgb\n7\n0.35\n800\n3150\n222\n3723\n612.167607\n6.801862\n0.092238\n\n\n7\nuplift_score_xgb\n8\n0.40\n850\n3600\n238\n4254\n648.589563\n7.206551\n0.080979\n\n\n8\nuplift_score_xgb\n9\n0.45\n895\n4050\n252\n4777\n681.351266\n7.570570\n0.073231\n\n\n9\nuplift_score_xgb\n10\n0.50\n939\n4500\n267\n5252\n710.230008\n7.891445\n0.066199\n\n\n10\nuplift_score_xgb\n11\n0.55\n962\n4950\n279\n5710\n720.134851\n8.001498\n0.024910\n\n\n11\nuplift_score_xgb\n12\n0.60\n978\n5400\n283\n6161\n729.955851\n8.110621\n0.026686\n\n\n12\nuplift_score_xgb\n13\n0.65\n1002\n5850\n288\n6571\n745.600670\n8.284452\n0.041138\n\n\n13\nuplift_score_xgb\n14\n0.70\n1011\n6300\n290\n6982\n749.327127\n8.325857\n0.015134\n\n\n14\nuplift_score_xgb\n15\n0.75\n1026\n6750\n296\n7451\n757.848074\n8.420534\n0.020540\n\n\n15\nuplift_score_xgb\n16\n0.80\n1037\n7200\n301\n7884\n762.114155\n8.467935\n0.012897\n\n\n16\nuplift_score_xgb\n17\n0.85\n1049\n7650\n307\n8307\n766.280607\n8.514229\n0.012482\n\n\n17\nuplift_score_xgb\n18\n0.90\n1097\n8100\n350\n8656\n769.481516\n8.549795\n-0.016543\n\n\n18\nuplift_score_xgb\n19\n0.95\n1141\n8550\n435\n8851\n720.793244\n8.008814\n-0.338120\n\n\n19\nuplift_score_xgb\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n-0.443445\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"uplift_score_xgb\", \"ad\", 1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\n\n4. Use the incremental_resp to calculate the profits\n\n\nCode\nuplift_profit_xgb = prof_calc(uplift_tab_xgb, 14.99, 1.5)\nuplift_profit_xgb\n\n\n57642.371278982806\n\n\n\n\n5. Calculate the uplift and Increatmental Uplift for Propensity Model\n\n\nCode\npropensity_tab_xgb = rsm.uplift_tab(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)\npropensity_tab_xgb\n\n\n\n\n\n\n\n\n\n\npred\nbins\ncum_prop\nT_resp\nT_n\nC_resp\nC_n\nincremental_resp\ninc_uplift\nuplift\n\n\n\n\n0\npred_treatment_xgb\n1\n0.05\n216\n450\n82\n587\n153.137990\n1.701533\n0.340307\n\n\n1\npred_treatment_xgb\n2\n0.10\n358\n900\n131\n1067\n247.503280\n2.750036\n0.213472\n\n\n2\npred_treatment_xgb\n3\n0.15\n505\n1350\n182\n1522\n343.567674\n3.817419\n0.214579\n\n\n3\npred_treatment_xgb\n4\n0.20\n602\n1800\n228\n2002\n397.004995\n4.411167\n0.119722\n\n\n4\npred_treatment_xgb\n5\n0.25\n700\n2250\n277\n2463\n446.954933\n4.966166\n0.111487\n\n\n5\npred_treatment_xgb\n6\n0.30\n782\n2700\n332\n2893\n472.148635\n5.246096\n0.054315\n\n\n6\npred_treatment_xgb\n7\n0.35\n844\n3150\n362\n3312\n499.706522\n5.552295\n0.066179\n\n\n7\npred_treatment_xgb\n8\n0.40\n899\n3600\n398\n3733\n515.180016\n5.724222\n0.036712\n\n\n8\npred_treatment_xgb\n9\n0.45\n954\n4050\n427\n4162\n538.490630\n5.983229\n0.054623\n\n\n9\npred_treatment_xgb\n10\n0.50\n993\n4500\n452\n4614\n552.167750\n6.135197\n0.031357\n\n\n10\npred_treatment_xgb\n11\n0.55\n1035\n4950\n473\n5008\n567.478035\n6.305312\n0.040034\n\n\n11\npred_treatment_xgb\n12\n0.60\n1067\n5400\n489\n5468\n584.081200\n6.489791\n0.036329\n\n\n12\npred_treatment_xgb\n13\n0.65\n1096\n5850\n500\n5929\n602.662169\n6.696246\n0.040583\n\n\n13\npred_treatment_xgb\n14\n0.70\n1109\n6300\n504\n6407\n613.417044\n6.815745\n0.020521\n\n\n14\npred_treatment_xgb\n15\n0.75\n1122\n6750\n506\n6828\n621.780316\n6.908670\n0.024138\n\n\n15\npred_treatment_xgb\n16\n0.80\n1136\n7200\n506\n7212\n630.841930\n7.009355\n0.031111\n\n\n16\npred_treatment_xgb\n17\n0.85\n1145\n7650\n507\n7646\n637.734763\n7.085942\n0.017696\n\n\n17\npred_treatment_xgb\n18\n0.90\n1158\n8100\n509\n8103\n649.188449\n7.213205\n0.024513\n\n\n18\npred_treatment_xgb\n19\n0.95\n1167\n8550\n510\n8561\n657.655297\n7.307281\n0.017817\n\n\n19\npred_treatment_xgb\n20\n1.00\n1174\n9000\n512\n9000\n662.000000\n7.355556\n0.011000\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)\n\n\n\n\n\n\n\n\n\n\nThe curve starts at 0% uplift when 0% of the population is targeted, which makes sense as no one has yet been exposed to the campaign.\nAs the percentage of the targeted population increases, the incremental uplift also increases. This indicates that targeting more of the population is initially resulting in higher incremental gains.\nThe curve shows a steep rise in incremental uplift at the beginning, suggesting that the early segments of the population targeted are highly responsive to the campaign.\nThe rate of increase in incremental uplift begins to slow down as the curve extends towards the right, which suggests diminishing returns; the incremental gains from targeting additional portions of the population decrease.\nThe curve eventually starts to plateau, indicating that there is a point at which the incremental benefits do not significantly increase with additional targeting. This plateau represents the point of maximum efficiency in targeting efforts.\n\nThe graph is indicative of a typical response to a well-targeted marketing campaign, where the most responsive individuals are targeted first, leading to higher uplifts early on. As less responsive individuals are targeted, the incremental uplift decreases\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"), \n    \"converted\", \"yes\", \"pred_treatment_xgb\", \"ad\", 1, qnt = 20)\n\n\n\n\n\n\n\n\n\n\nHigh Responsiveness in Early Segments: The first few bars are the tallest, indicating the highest uplift. This suggests that the first segment of the population (likely the top 5% or 10%) is the most responsive to the campaign.\nDiminishing Returns: As we move right across the x-axis to include larger percentages of the population, the size of the uplift decreases. This pattern is typical in targeted marketing, where you see the greatest response rate increases in the first few population segments.\nConsistent Drop: The consistent drop-off in uplift as the population segments progress suggests that the most responsive individuals are targeted first, followed by progressively less responsive segments.\nPositive Uplift Across Segments: All the bars are above the 0% line, which indicates that each segment experienced a positive uplift due to the campaign. This means that even the least responsive segments targeted still had a positive response, although it was not as strong as the initial segments.\n\n\n\nCompare Uplift model and Propensity model\n\n\nCode\nfig = rsm.inc_uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_xgb\", \"uplift_score_xgb\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nUplift Model Performance: The uplift_score_xgb curve is consistently above the pred_treatment_xgb curve. This indicates that the uplift model is predicting a higher incremental uplift across the different segments of the targeted population compared to the treatment model.\nPropensity Model: The pred_treatment_xgb curve shows that the propensity model also predicts an increase in incremental uplift with more of the population targeted. However, the uplift is not as high as the one predicted by the uplift model, suggesting that the propensity model might be useful, but not optimal.\nDiminishing Returns: Both models show an initial rapid increase in incremental uplift, which slows and eventually plateaus as the percentage of the population targeted increases. This is indicative of diminishing returns; beyond a certain point, targeting additional people results in smaller increases in incremental uplift.\nOptimal Targeting Point: The curves suggest that there is an optimal point for targeting, after which the incremental benefits of targeting additional individuals begin to decrease. Identifying this point can help optimize marketing efforts and budget allocation.\n\nIn short, the uplift_score_xgb model appears to be more effective for targeting the right individuals, likely because it accounts for the incremental effect of the treatment, identifying those who are influenced by the campaign as opposed to those who would respond without it.\n\n\nCode\nfig = rsm.uplift_plot(\n    cg_rct_stacked.query(\"training == 0\"),\n    \"converted\",\n    \"yes\",\n    [\"pred_treatment_xgb\", \"uplift_score_xgb\"],\n    \"ad\",\n    1, qnt = 20\n)\n\n\n\n\n\n\n\n\n\n\nHigh Responsiveness in Early Segments: The initial segments show a higher uplift for both models. This indicates that these segments of the population are more responsive to the marketing campaign.\nDecreasing Uplift: As the segments progress from left to right (targeting a larger share of the population), the uplift decreases. This trend is typical in targeted marketing, where the most responsive individuals are usually targeted first.\nNegative Uplift in Later Segments: For the segments on the right, the uplift becomes negative. This could mean that targeting these individuals might be counterproductive, potentially leading to a negative reaction to the marketing campaign or an unnecessary cost for individuals who would have made a purchase without any intervention.\nComparison Between Models: In most segments, the uplift_score_xgb bars appear to have a higher uplift than the pred_treatment_xgb bars, suggesting that the uplift model is more effective at identifying which segments of the population will provide a higher incremental uplift when targeted.\n\n\n\n6. Use the incremental_resp to calculate the profits for Propensity Model\n\n\nCode\npropensity_profit_xgb = prof_calc(propensity_tab_xgb, 14.99, 1.5)\npropensity_profit_xgb\n\n\n44331.39261063743\n\n\n\n\nCode\n# Difference in profits from using uplift model and propensity model\ndifference_xgb = uplift_profit_xgb - propensity_profit_xgb\ndifference_xgb\n\n\n13310.978668345379\n\n\n\n\nResults\n\n\nCode\nmod_perf = pd.DataFrame({\n    \"model\": [\"Logistic\", \"Neural Network\", \"Random Forest\", \"XGBoost\"],\n    \"incremental_profit_uplift\": [uplift_profit_logit, uplift_profit_nn, uplift_profit_rf, uplift_profit_xgb],\n    \"incremental_profit_propensity\": [propensity_profit_logit, propensity_profit_nn, propensity_profit_rf, propensity_profit_xgb],\n    \"difference\": [difference_logit, difference_nn, difference_rf, difference_xgb]\n})\nmod_perf.sort_values(\"difference\", ascending=False)\n\n\n\n\n\n\n\n\n\n\nmodel\nincremental_profit_uplift\nincremental_profit_propensity\ndifference\n\n\n\n\n0\nLogistic\n45907.759762\n32065.482935\n13842.276826\n\n\n3\nXGBoost\n57642.371279\n44331.392611\n13310.978668\n\n\n2\nRandom Forest\n58311.164733\n48633.386279\n9677.778454\n\n\n1\nNeural Network\n55034.956633\n45823.831691\n9211.124941\n\n\n\n\n\n\n\n\n\n\nCode\nincremental_profit_dct = {\n    \"Logistic\": uplift_profit_logit,\n    \"Neural Network\": uplift_profit_nn,\n    \"Random Forest\": uplift_profit_rf,\n    \"XGBoost\": uplift_profit_xgb\n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(incremental_profit_dct.items()), columns=['Model', 'IncrementalProfit'])\nplt.figure(figsize=(12, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"IncrementalProfit\", data=df, palette=\"magma\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.IncrementalProfit, f'${row.IncrementalProfit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Incremental Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Incremental Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndifference_dct = {\n    \"Logistic\": difference_logit,\n    \"Neural Network\": difference_nn,\n    \"Random Forest\": difference_rf,\n    \"XGBoost\": difference_xgb\n}\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(difference_dct.items()), columns=['Model', 'IncrementalProfit'])\nplt.figure(figsize=(12, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"IncrementalProfit\", data=df, palette=\"magma\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.IncrementalProfit, f'${row.IncrementalProfit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Incremental Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Difference by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPart II\n\n1. What formula would you use to select customers to target using a propensity model if your goal is to maximize expected profits?\nTo select customers to target using a propensity to buy model with the goal of maximizing our expected profits, we want to use a formula that incorporates both the propensity to buy and the expected profit from each customer if they do purchase. This approach allows the prioritization of customers not just based on their likelihood to buy, but also on the value they bring. The propensity model is a more general approach that can be used to predict the likelihood of a customer to purchase a product or service. The formula to select customers to target using a propensity model if the goal is to maximize expected profits is as follows:\n\\[\n\\text{Customers to target} = (\\text{price}* {pred_i} - \\text{cost}) &gt; 0\n\\]\n\n\nCode\nprice = 14.99\ncost = 1.50\n\n\n\n\nCode\ndef round_to_nearest_5(x):\n    return round(x*20) / 20\n\n\n\n\nCode\n# Run the function accross all models and predictions\nmodels = ['logit', 'nn', 'rf', 'xgb']\npredictions = ['pred_treatment', 'pred_treatment_nn', 'pred_treatment_rf', 'pred_treatment_xgb']\n\n# Define optimization function for profit\ndef propensity_prof_opt(data, price = 14.99, cost = 1.5):\n    total_customers = 120000\n    result = []\n    for model, prediction in zip(models, predictions):\n        data['EP'] = data[prediction] * price - cost\n        data['ads_logit'] = data['EP'] &gt; 0\n        perc = np.nanmean(data['ads_logit'])\n        target_customers = perc * total_customers\n        rounded_perc = round_to_nearest_5(perc)\n        result.append([model, perc, rounded_perc, target_customers])\n    return pd.DataFrame(result, columns = ['model','perc_customer', 'rounded_perc', 'target_customers']).sort_values('perc_customer', ascending = False)\n\npropensity_prof_opt = propensity_prof_opt(cg_rct_stacked.query(\"training == 1 & ad == 1\"))\npropensity_prof_opt\n\n\n\n\n\n\n\n\n\n\nmodel\nperc_customer\nrounded_perc\ntarget_customers\n\n\n\n\n3\nxgb\n0.485571\n0.50\n58268.571429\n\n\n0\nlogit\n0.478524\n0.50\n57422.857143\n\n\n1\nnn\n0.439286\n0.45\n52714.285714\n\n\n2\nrf\n0.223524\n0.20\n26822.857143\n\n\n\n\n\n\n\n\nTherefore, we want to target customers using logit propensity model for each tuned model to maximize the expected profits.\n\nLogistic Regression Model: 57,422 customers\nNeural Network Model: 54,005 customers\nRandom Forest Model: 27222 customers\nXGBoost Model: 58,268 customers\n\n\n\nUplift Expected Profit\nIf we wanted to target customers using an uplift model, we will select customers with the highest predicted uplift scores, where uplift is the incremental impact of the treatment (e.g., receiving an ad) on the customer‚Äôs probability of making a purchase.\n The expected incremental profit can be calculated by multiplying the uplift score by the profit per conversion minus the cost of targeting the client. The formula is listed below:\n\\[\n\\text{Customers to target} = ({\\text{Uplift Score} * \\text{profit per conversion}) - \\text{Cost per conversion}}\n\\]\nThe uplift score is calculated by subtracting the probability of the customer to purchase without the treatment from the probability of the customer to purchase with the treatment.\n\\[\n\\text{Uplift Score} = \\text{P(Outcome|Treatment)} - \\text{P(Outcome|Control)}\n\\]\n\n\nCode\n# Optimizing the uplift model profit\n# Run the function accross all models and uplift scores\nmodels = ['logit', 'nn', 'rf', 'xgb']\nuplift_scores = ['uplift_score', 'uplift_score_nn', 'uplift_score_rf', 'uplift_score_xgb']\n\n# Define optimization function for profit\ndef uplift_prof_opt(data, price = 14.99, cost = 1.5):\n    total_customers = 120000\n    result = []\n    for model, uplift_score in zip(models,uplift_scores):\n        data['EIP'] = (data[uplift_score] * price) - cost\n        data['ads_logit'] = data['EIP'] &gt; 0\n        perc = np.nanmean(data['ads_logit'])\n        target_customers = perc * total_customers\n        rounded_perc = round_to_nearest_5(perc)\n        result.append([model, perc, rounded_perc, target_customers])\n    return pd.DataFrame(result, columns = ['model','perc_customer', 'rounded_perc', 'target_customers']).sort_values('perc_customer', ascending = False)\n\nuplift_prof_opt = uplift_prof_opt(cg_rct_stacked.query(\"training == 1 & ad == 1\"))\nuplift_prof_opt\n\n\n\n\n\n\n\n\n\n\nmodel\nperc_customer\nrounded_perc\ntarget_customers\n\n\n\n\n1\nnn\n0.295000\n0.30\n35400.000000\n\n\n3\nxgb\n0.283048\n0.30\n33965.714286\n\n\n0\nlogit\n0.221905\n0.20\n26628.571429\n\n\n2\nrf\n0.138810\n0.15\n16657.142857\n\n\n\n\n\n\n\n\nTherefore, we want to target customers using uplift model for each tuned model to maximize the expected profits.\n\nLogistic Regression Model: 26,628 customers\nNeural Network Model: 36,068 customers\nRandom Forest Model: 16,982 customers\nXGBoost Model: 33,965 customers\n\n\n\n\nLogistic Model\n\n3. Uplift and Propensity Expected Profit\nIn questions 1 and 2, we calculated the optimal target of customers. for the propensity and uplift models. When we round to the nearest 5%, we will then use these percentages to calculate the number of customers to target out of the 120K customer base as well as calculate the expected incremental profits.\nTo calculate the incremental profits we will follow the general steps of: 1. Determining the incremental response. As we can see, the incremental_resp column shows the additional responses attributed to the treatment of the ad for each bin of customers. 2. We will then want to calculate the incremental profits for each bin. To do this, you will multiply the incremental response by the average profit per response, and we will assume an average profit per response value if not provided. 3. Finally, we will sum the incremental profits across all the bins to get the total incremental profit.\n\n\nCode\n# Define the function to calculate the profit\ndef prof_calc_opt(data, total_customers, percent_target, price = 14.99, cost = 1.5):\n    # Given variables\n    target_customers = total_customers * percent_target\n    target_prop = percent_target\n    # Calculate the scale factor\n    scale_factor = 120000 / 9000\n\n    # Calculate the expected incremental customers and profits\n    target_row = data[data['cum_prop'] &lt;= target_prop].iloc[-1]  # Ensure data is sorted if necessary\n    profit = (price * target_row['incremental_resp'] - cost * target_row['T_n']) * scale_factor\n    return profit\n\n\n\n\nCode\n# Propensity Model\npropensity_profits_logit = prof_calc_opt(propensity_tab, 120000, 0.5, 14.99, 1.5)\npropensity_profits_logit\n\n\n799.8637508283598\n\n\n\n\nCode\n# Uplift Model\nuplift_profits_logit = prof_calc_opt(uplift_tab, 120000, 0.2, 14.99, 1.5)\nuplift_profits_logit\n\n\n44387.751724137925\n\n\n\n\nCode\nlogit_mod = pd.DataFrame({\n    \"model\": [\"Uplift\", \"Propensity\"],\n    \"incremental_profits\": [uplift_profits_logit, propensity_profits_logit]\n})\nlogit_mod.sort_values(\"incremental_profits\", ascending=False)\nlogit_mod\n\n\n\n\n\n\n\n\n\n\nmodel\nincremental_profits\n\n\n\n\n0\nUplift\n44387.751724\n\n\n1\nPropensity\n799.863751\n\n\n\n\n\n\n\n\n\n\n\nNeural Network\n\n\nCode\n# Propensity Model\npropensity_profits_nn = prof_calc_opt(prop_tab_nn, 120000, 0.45, 14.99, 1.5)\npropensity_profits_nn\n\n\n17515.540061466905\n\n\n\n\nCode\n# Uplift Model\nuplift_profits_nn = prof_calc_opt(uplift_tab_nn, 120000, 0.3, 14.99, 1.5)\nuplift_profits_nn\n\n\n57060.084573350156\n\n\n\n\nCode\nnn_mod = pd.DataFrame({\n    \"model\": [\"Uplift\", \"Propensity\"],\n    \"incremental_profits\": [uplift_profits_nn, propensity_profits_nn]\n})\nnn_mod.sort_values(\"incremental_profits\", ascending=False)\nnn_mod\n\n\n\n\n\n\n\n\n\n\nmodel\nincremental_profits\n\n\n\n\n0\nUplift\n57060.084573\n\n\n1\nPropensity\n17515.540061\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\nCode\n# Propensity Model\npropensity_profits_rf = prof_calc_opt(prop_tab_rf, 120000, 0.25, 14.99, 1.5)\npropensity_profits_rf\n\n\n48633.38627925748\n\n\n\n\nCode\n# Uplift Model\nuplift_profits_rf = prof_calc_opt(uplift_tab_rf, 120000, 0.15, 14.99, 1.5)\nuplift_profits_rf\n\n\n48476.592645314355\n\n\n\n\nCode\nrf_mod = pd.DataFrame({\n    \"model\": [\"Uplift\", \"Propensity\"],\n    \"incremental_profits\": [uplift_profits_rf, propensity_profits_rf]\n})\nrf_mod.sort_values(\"incremental_profits\", ascending=False)\nrf_mod\n\n\n\n\n\n\n\n\n\n\nmodel\nincremental_profits\n\n\n\n\n0\nUplift\n48476.592645\n\n\n1\nPropensity\n48633.386279\n\n\n\n\n\n\n\n\n\n\nXGBoost Model\n\n\nCode\n# Propensity Model\npropensity_profits_xgb = prof_calc_opt(propensity_tab_xgb, 120000, 0.5, 14.99, 1.5)\npropensity_profits_xgb\n\n\n20359.9276983095\n\n\n\n\nCode\n# Uplift Model\nuplift_profits_xgb = prof_calc_opt(uplift_tab_xgb, 120000, 0.3, 14.99, 1.5)\nuplift_profits_xgb\n\n\n60208.6887367766\n\n\n\n\nCode\nxgb_mod = pd.DataFrame({\n    \"model\": [\"Uplift\", \"Propensity\"],\n    \"incremental_profits\": [uplift_profits_xgb, propensity_profits_xgb]\n})\nxgb_mod.sort_values(\"incremental_profits\", ascending=False)\nxgb_mod\n\n\n\n\n\n\n\n\n\n\nmodel\nincremental_profits\n\n\n\n\n0\nUplift\n60208.688737\n\n\n1\nPropensity\n20359.927698\n\n\n\n\n\n\n\n\n\n\nCode\nuplift_profit_dct = {\n    \"Logit_Uplift\": uplift_profits_logit,\n    \"NN_Uplift\": uplift_profits_nn, \n    \"RF_Uplift\": uplift_profits_rf,\n    \"XGB_Uplift\": uplift_profits_xgb}\n\n\n\n\nCode\npropensity_profit_dct = {\n    \"Logit_Propensity\": propensity_profits_logit,\n    \"NN_Propensity\": propensity_profits_nn, \n    \"RF_Propensity\": propensity_profits_nn,\n    \"XGB_Propensity\": propensity_profits_xgb}\n\n\n\n\nCode\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(uplift_profit_dct.items()), columns=['Model', 'IncrementalProfit'])\nplt.figure(figsize=(12, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"IncrementalProfit\", data=df, palette=\"magma\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.IncrementalProfit, f'${row.IncrementalProfit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Incremental Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Uplift Incremental Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(propensity_profit_dct.items()), columns=['Model', 'IncrementalProfit'])\nplt.figure(figsize=(12, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"IncrementalProfit\", data=df, palette=\"magma\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.IncrementalProfit, f'${row.IncrementalProfit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Incremental Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Propensity Incremental Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this situation where the goal is to maximize the total number of conversions, a propensity model shows to be more effective. It targets a broader audience, including those who are already likely to convert without any intervention. The uplift model, on the other hand, targets only those who are likely to convert because of the intervention. This is why the propensity model is more effective in this case.\nThe propensity model is also used for broad targeting and segmentation, helping to focus resources on high-probability events. The uplift model is used for optimizing marketing campaigns and other interventions by targeting those individuals whose behavior can be positively changed by the intervention, thus maximizing ROI and reducing wastage. This causes the propensity model to capture a higher profit, where as the uplift model captures a higher ROI.\nIn essence, while both models are used to enhance decision-making in marketing and other fields, they serve different strategic purposes: propensity models for predicting actions based on existing propensities, and uplift models for identifying the additional impact of a particular action or intervention."
  },
  {
    "objectID": "projects/cg-uplift/cg_organic_control_description.html",
    "href": "projects/cg-uplift/cg_organic_control_description.html",
    "title": "Website",
    "section": "",
    "text": "Game telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\n\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)"
  },
  {
    "objectID": "projects/cg-uplift/cg_organic_control_description.html#creative-gaming",
    "href": "projects/cg-uplift/cg_organic_control_description.html#creative-gaming",
    "title": "Website",
    "section": "",
    "text": "Game telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\n\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)"
  },
  {
    "objectID": "visualization/cg/index.html#tab-1",
    "href": "visualization/cg/index.html#tab-1",
    "title": "Machine Learning for Uplift",
    "section": "",
    "text": "Uplift and Propensity using Logistic Regression"
  },
  {
    "objectID": "visualization/cg/index.html#tab-2",
    "href": "visualization/cg/index.html#tab-2",
    "title": "Machine Learning for Uplift",
    "section": "Tab 2",
    "text": "Tab 2\n\nRow\nNeural Network Model\n\nColumn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn"
  },
  {
    "objectID": "visualization/cg/index.html#tab-3",
    "href": "visualization/cg/index.html#tab-3",
    "title": "Machine Learning for Uplift",
    "section": "Tab 3",
    "text": "Tab 3\n\nRow\nRandom Forest Model\n\nColumn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn"
  },
  {
    "objectID": "projects/project3/index.html",
    "href": "projects/project3/index.html",
    "title": "Conjoint",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/index.html#estimating-yogurt-preferences",
    "href": "projects/project3/index.html#estimating-yogurt-preferences",
    "title": "Conjoint",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\nData Overview\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nanonymized consumer identifiers.\n\n\ny1, y2, y3, y4\na vector indicating the chosen product.\n\n\nf1, f2, f3, f4\na vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising\n\n\np1, p2, p3, p4\nthe products‚Äô prices\n\n\n\n\n\n\n\n\nCode\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head()\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\n\n\nCode\nyogurt_data.describe(include='all')\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\ncount\n2430.0000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n\n\nmean\n1215.5000\n0.341975\n0.401235\n0.029218\n0.227572\n0.055556\n0.039506\n0.037449\n0.037449\n0.106248\n0.081532\n0.053622\n0.079507\n\n\nstd\n701.6249\n0.474469\n0.490249\n0.168452\n0.419351\n0.229109\n0.194836\n0.189897\n0.189897\n0.020587\n0.011047\n0.008054\n0.007714\n\n\nmin\n1.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-0.012000\n0.000000\n0.025000\n0.004000\n\n\n25%\n608.2500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.103000\n0.081000\n0.050000\n0.079000\n\n\n50%\n1215.5000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108000\n0.086000\n0.054000\n0.079000\n\n\n75%\n1822.7500\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.115000\n0.086000\n0.061000\n0.086000\n\n\nmax\n2430.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.193000\n0.111000\n0.086000\n0.104000\n\n\n\n\n\n\n\n\nStatistics Summary:\n\nThere are 2,430 records.\nBinary fields (y1, y2, y3, y4, f1, f2, f3, f4) indicate varying levels of frequency with which different yogurts were chosen or conditions were met.\nPrice or index fields (p1, p2, p3, p4) show distributions with differing means, minima, and maxima, suggesting variability in yogurt pricing or attributes across the samples.\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we‚Äôll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts‚Äô prices:\n\\[\nx_j' = \\left[ \\mathbb{1}(\\text{Yogurt 1}), \\mathbb{1}(\\text{Yogurt 2}), \\mathbb{1}(\\text{Yogurt 3}), X_f, X_p \\right]\n\\]\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a ‚Äúwide‚Äù shape with \\(n\\) rows and multiple columns for each covariate, to a ‚Äúlong‚Äù shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we‚Äôll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be ‚Äúpivoted‚Äù or ‚Äúmelted‚Äù from wide to long.\n\nReshape and prep the data\n\n\nCode\n# Melt the data into a long format\nlong_data = pd.melt(yogurt_data, id_vars=['id'], \n                    value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],\n                    var_name='product_feature', value_name='value')\n\n# Extract product and feature types from the 'product_feature' column\nlong_data['product'] = long_data['product_feature'].str.extract('(\\d)').astype(int)\nlong_data['feature'] = long_data['product_feature'].str.extract('([a-z]+)')\n\n# Pivot the table to get one row per consumer per product\nreshaped_yogurt = long_data.pivot_table(index=['id', 'product'], columns='feature', values='value', aggfunc='first').reset_index()\n\n# Add the binary indicators for the first three yogurts\nfor j in range(1, 4):\n    reshaped_yogurt[f'Yogurt{j}'] = (reshaped_yogurt['product'] == j).astype(int)\n\n# Ensure the resulting DataFrame is correctly structured\nreshaped_yogurt\n\n\n\n\n\n\n\n\n\nfeature\nid\nproduct\nf\np\ny\nYogurt1\nYogurt2\nYogurt3\n\n\n\n\n0\n1\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n1\n1\n2\n0.0\n0.081\n0.0\n0\n1\n0\n\n\n2\n1\n3\n0.0\n0.061\n0.0\n0\n0\n1\n\n\n3\n1\n4\n0.0\n0.079\n1.0\n0\n0\n0\n\n\n4\n2\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9715\n2429\n4\n0.0\n0.086\n1.0\n0\n0\n0\n\n\n9716\n2430\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n9717\n2430\n2\n0.0\n0.086\n0.0\n0\n1\n0\n\n\n9718\n2430\n3\n0.0\n0.043\n0.0\n0\n0\n1\n\n\n9719\n2430\n4\n0.0\n0.079\n1.0\n0\n0\n0\n\n\n\n\n9720 rows √ó 8 columns\n\n\n\n\n\n\n\n\nEstimation\n\nCode up the log-likelihood function.\n\n\nCode\ndef log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return np.sum(log_likelihood)\n\n\nUse optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)).\n\n\nCode\ndef negative_log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return -np.sum(log_likelihood)\n\n# Prepare the input matrix X and the choice vector\nn_products = 4  # There are 4 products\nfeatures = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'f', 'p']\nX = reshaped_yogurt[features].values\nchoices = reshaped_yogurt['y'].values\n\n# Define initial guesses for the parameters\ninitial_beta = np.zeros(len(features))\n\n# Rerun the optimization with numpy properly imported and initial_beta defined\nresult = minimize(negative_log_likelihood, initial_beta, args=(X, choices))\nCoef = result.x\n\n\nCoef_table = pd.DataFrame({\n    'Variables': features,\n    'Coeficient': Coef\n})\n\nCoef_table\n\n\n\n\n\n\n\n\n\n\nVariables\nCoeficient\n\n\n\n\n0\nYogurt1\n1.387751\n\n\n1\nYogurt2\n0.643505\n\n\n2\nYogurt3\n-3.086113\n\n\n3\nf\n0.487415\n\n\n4\np\n-37.057828\n\n\n\n\n\n\n\n\n\n\nCode\n# unique_choices = reshaped_yogurt['product'].nunique()\n\n\n# X = reshaped_yogurt[features]\n# X = sm.add_constant(reshaped_yogurt[features])\n# choices = reshaped_yogurt['y']\n\n\n\n# mnl_model = sm.MNLogit(choices, X).fit()\n# mnl_summary = mnl_model.summary()\n# mnl_results_table = mnl_summary.tables[1]\n\n# # To display or print out the table\n# print(mnl_results_table)\n\n\n\n\n\nDiscussion\nThe estimated parameters for the three yogurt product intercepts are:\n\\(\\beta_1\\) = 1.39\n\\(\\beta_2\\) = 0.64\n\\(\\beta_3\\) = - 3.09\nThese coefficients represent the intrinsic utilities (or preferences) of the three yogurt products when all other variables (such as price and whether the product was featured) are held constant. Here‚Äôs how to interpret these intercepts in the context of consumer preferences:\n\\(\\beta_1\\) (Yogurt 1): The positive and highest value among the three suggests that Yogurt 1 is the most preferred when no other attributes (like price or features) are considered. It has the highest intrinsic utility.\n\\(\\beta_2\\) (Yogurt 2): This is also positive but lower than \\(\\beta_1\\) , indicating that Yogurt 2 is less preferred than Yogurt 1 but still has a positive intrinsic appeal compared to a baseline (which could be another product not included in these three, like Yogurt 4 in this analysis).\n\\(\\beta_3\\) (Yogurt 3): The negative value here suggests that Yogurt 3 is least preferred among the three, having a lower intrinsic utility relative to the others.\nGiven these interpretations, Yogurt 1 appears to be the most preferred option among the first three, followed by Yogurt 2, with Yogurt 3 being the least preferred under the assumption that other factors are equal. This intrinsic preference could be driven by factors not explicitly modeled but captured by the intercepts, such as brand affinity, flavor preferences, or other unobserved attributes associated with each product.\nUse the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\n\nCode\n# Extracted beta values for Yogurt 1 and Yogurt 3 and the price coefficient\nbeta_1 = 1.39\nbeta_3 = -3.09\nbeta_p = -37.06  # The negative price coefficient\n\n# Calculate utility difference\nutility_difference = beta_1 - beta_3\n\n# Convert utility difference to dollar benefit using the price coefficient\ndollar_benefit = utility_difference / abs(beta_p)\n\nprint(\"Per-unit monetary measure of brand value is \", round(dollar_benefit, 4))\n\n\nPer-unit monetary measure of brand value is  0.1209\n\n\nThe per-unit monetary measure of brand value between the most-preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is approximately $0.12 per unit. This means that, in terms of brand value, consumers might be willing to pay an extra 12 cents per unit for Yogurt 1 compared to Yogurt 3, based solely on their preference (utility difference) as captured by the model. This is a useful way to quantify the monetary value of consumer preferences in this context\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nCalculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities). Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?\n\n\nCode\n#reload the function cuz the disconnecting between tasks\ndef negative_log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return -np.sum(log_likelihood)\n\n# Prepare the input matrix X and the choice vector\nn_products = 4  # There are 4 products\nfeatures = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'f', 'p']\nX = reshaped_yogurt[features].values\nchoices = reshaped_yogurt['y'].values\n\n# Define initial guesses for the parameters\ninitial_beta = np.zeros(len(features))\n\n# Rerun the optimization with numpy properly imported and initial_beta defined\nresult = minimize(negative_log_likelihood, initial_beta, args=(X, choices))\n\n\n\n\nCode\ndef calculate_probabilities(beta, X):\n    utility = X.dot(beta)\n    exp_util = np.exp(utility)\n    sum_exp_util = np.sum(exp_util.reshape(-1, n_products), axis=1)\n    probabilities = exp_util / np.repeat(sum_exp_util, n_products)\n    return probabilities.reshape(-1, n_products)\n\nestimated_beta = result.x\n# Calculate the initial choice probabilities for all products and all consumers\ninitial_probabilities = calculate_probabilities(estimated_beta, X)\n\n# Calculate the current market shares by taking the mean of probabilities across all consumers for each product\ncurrent_market_shares = np.mean(initial_probabilities, axis=0)\n\n# Display the new market shares\ncurrent_market_shares_df = pd.DataFrame({\n    'Product': ['Yogurt 1', 'Yogurt 2', 'Yogurt 3', 'Yogurt 4'],\n    'Current Market Share': current_market_shares\n})\n\ncurrent_market_shares_df\n\n\n\n\n\n\n\n\n\n\nProduct\nCurrent Market Share\n\n\n\n\n0\nYogurt 1\n0.341975\n\n\n1\nYogurt 2\n0.401235\n\n\n2\nYogurt 3\n0.029218\n\n\n3\nYogurt 4\n0.227572\n\n\n\n\n\n\n\n\nThe current market shares for the four yogurt products are approximately:\nYogurt 1: 34.2%\nYogurt 2: 40.1%\nYogurt 3: 2.9%\nYogurt 4: 22.8%\nNext, let‚Äôs increase the price of Yogurt 1 by $0.10 and then use the fitted model to predict the new choice probabilities. We‚Äôll see how the market shares change, particularly for Yogurt 1, as a result of this price increase.\n\n\nCode\n# Increase the price of Yogurt 1 by $0.10\n# First, create a new X matrix with the updated price for Yogurt 1\nX_new_prices = X.copy()\nprice_increase = 0.10\nX_new_prices[:, 4][X_new_prices[:, 0] == 1] += price_increase  # Only increase the price in the entries for Yogurt 1\n\n# Calculate the new choice probabilities with the increased price of Yogurt 1\nnew_probabilities = calculate_probabilities(estimated_beta, X_new_prices)\n\n# Calculate the new market shares by taking the mean of new probabilities across all consumers for each product\nnew_market_shares = np.mean(new_probabilities, axis=0)\n\n# Display the new market shares\nnew_market_shares_df = pd.DataFrame({\n    'Product': ['Yogurt 1', 'Yogurt 2', 'Yogurt 3', 'Yogurt 4'],\n    'New Market Share': new_market_shares\n})\n\nnew_market_shares_df\n\n\n\n\n\n\n\n\n\n\nProduct\nNew Market Share\n\n\n\n\n0\nYogurt 1\n0.021118\n\n\n1\nYogurt 2\n0.591145\n\n\n2\nYogurt 3\n0.044040\n\n\n3\nYogurt 4\n0.343697\n\n\n\n\n\n\n\n\nThe new market shares for the four yogurt products after increasing the price of Yogurt 1 by $0.10 are approximately:\nYogurt 1: 2.1%\nYogurt 2: 59.1%\nYogurt 3: 4.4%\nYogurt 4: 34.4%\nYogurt 1‚Äôs market share dramatically decreases from 34.2% to 2.1% due to the price increase.\nYogurt 2‚Äôs market share significantly increases, absorbing most of the share lost by Yogurt 1.\nYogurt 3 and Yogurt 4 also see some increase in their market shares. This demonstrates the sensitivity of market share to price changes in competitive markets, especially under the assumption of a Multinomial Logit model where the relative utilities directly affect the choice probabilities. Yogurt 1‚Äôs substantial price increase leads consumers to switch to the more affordable alternatives, illustrating the impact of price elasticity on consumer choice behavior"
  },
  {
    "objectID": "projects/project3/index.html#estimating-minivan-preferences",
    "href": "projects/project3/index.html#estimating-minivan-preferences",
    "title": "Conjoint",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\nLoad data and Describe the data\n\n\nCode\nminivan = pd.read_csv(\"rintro-chapter13conjoint.csv\")\nminivan\n\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8995\n200\n14\n2\nno\n7\n3ft\ngas\n35\n1\n\n\n8996\n200\n14\n3\nno\n7\n3ft\nhyb\n35\n0\n\n\n8997\n200\n15\n1\nno\n7\n2ft\ngas\n35\n0\n\n\n8998\n200\n15\n2\nno\n8\n3ft\nelec\n40\n0\n\n\n8999\n200\n15\n3\nno\n6\n3ft\ngas\n35\n1\n\n\n\n\n9000 rows √ó 9 columns\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nresp.id\nIdentifier for the respondent.\n\n\nques\nQuestion or choice task number.\n\n\nalt\nAlternative within each choice task.\n\n\ncarpool, seat, cargo, eng\nAttributes of each alternative, such as carpool availability, seating capacity, cargo space, and engine type.\n\n\nprice\nPrice associated with each alternative.\n\n\nchoice\nWhether the alternative was chosen (1) or not (0).\n\n\n\n\n\n\n\n\nCode\n# Number of respondents\nnum_respondents = minivan['resp.id'].nunique()\n\n# Number of choice tasks completed by each respondent\ntasks_per_respondent = minivan.groupby('resp.id')['ques'].nunique()\n\n# Number of alternatives per choice task\n# Assuming the structure is consistent across the dataset\nalternatives_per_task = minivan.groupby(['resp.id', 'ques'])['alt'].nunique().max()\n\nprint(\"Number of Respondents:\", num_respondents)\nprint(tasks_per_respondent.describe())\nprint(\"Number of Alternatives per Choice Task:\",alternatives_per_task)\n\n\nNumber of Respondents: 200\ncount    200.0\nmean      15.0\nstd        0.0\nmin       15.0\n25%       15.0\n50%       15.0\n75%       15.0\nmax       15.0\nName: ques, dtype: float64\nNumber of Alternatives per Choice Task: 3\n\n\nHere‚Äôs a summary of the conjoint survey data:\n\nNumber of Respondents: There are 200 respondents who participated in the survey.\nNumber of Choice Tasks per Respondent: Each respondent completed 15 choice tasks. This number is consistent across all respondents.\nNumber of Alternatives per Choice Task: Each choice task presented 3 alternatives.\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\nEstimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine) and show the table of Coefficients and Standard Errors\n\n\nCode\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Create dummy variables for the categorical attributes, excluding base levels\nminivan['seat_7'] = (minivan['seat'] == 7).astype(int)\nminivan['seat_8'] = (minivan['seat'] == 8).astype(int)\nminivan['cargo_3ft'] = (minivan['cargo'] == '3ft').astype(int)\nminivan['eng_hybrid'] = (minivan['eng'] == 'hyb').astype(int)\nminivan['eng_electric'] = (minivan['eng'] == 'elec').astype(int)\nminivan = minivan.apply(pd.to_numeric, errors='coerce')\n\nX = minivan[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hybrid', 'eng_electric']]\nX = sm.add_constant(X)  # Add intercept\ny = minivan['choice']  # Make sure 'choice' is coded appropriately for multinomial\n\nmnl_model = sm.MNLogit(y, X).fit()\nmnl_summary = mnl_model.summary()\nmnl_results_table = mnl_summary.tables[1]\n\n# To display or print out the table\nprint(mnl_results_table)\n\n\nOptimization terminated successfully.\n         Current function value: 0.558663\n         Iterations 6\n================================================================================\n    choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            5.5322      0.224     24.677      0.000       5.093       5.972\nprice           -0.1591      0.006    -25.616      0.000      -0.171      -0.147\nseat_7          -0.5248      0.060     -8.800      0.000      -0.642      -0.408\nseat_8          -0.2931      0.059     -5.009      0.000      -0.408      -0.178\ncargo_3ft        0.4385      0.049      9.004      0.000       0.343       0.534\neng_hybrid      -0.7605      0.057    -13.361      0.000      -0.872      -0.649\neng_electric    -1.4347      0.062    -23.217      0.000      -1.556      -1.314\n================================================================================\n\n\n\n\nResults\nInterpretation:\nPrice: The negative coefficient (-0.1591) suggests that as the price increases by one thousand dollars, the log odds of choosing a particular car decrease, indicating a typical negative relationship between price and purchase probability.\nSeat 7: Having 7 seats, compared to the baseline of 6 seats, is associated with lower odds of the car being chosen.\nSeat 8: Similarly, having 8 seats is also less preferable compared to 6 seats but less so than 7 seats.\nCargo 3ft: More cargo space (3ft) increases the odds of choosing the car compared to the base level of 2ft. This feature is preferred over the baseline of 2ft cargo space, as indicated by the positive coefficient. Consumers prefer more cargo space, all else being equal.\nEngine Hybrid and Electric: Both hybrid and electric engines are less preferred compared to a traditional gas engine, with electric being the least preferred among the options.\nUse the price coefficient as a dollar-per-util conversion factor. We could find the dollar value of 3ft of cargo space as compared to 2ft of cargo space:\n\n\nCode\n # Coefficients from the model results\ncargo_coeff = 0.4385\nprice_coeff = -0.1591\n\n# Calculate the dollar value of having 3ft of cargo space compared to 2ft\ndollar_value_cargo = (cargo_coeff / price_coeff) * (-1)\nprint(\"The Dolla Value of having 3ft cargo space compared to 2ft:\", round(dollar_value_cargo, 3))\n\n\nThe Dolla Value of having 3ft cargo space compared to 2ft: 2.756\n\n\nThe dollar value of having 3 feet of cargo space compared to 2 feet, based on the model, is approximately $2,756. This amount represents the additional value that respondents place on having an extra foot of cargo space in their vehicle choice.\nAssume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n\nCode\n# Coefficients from the MNL model\ncoef_const = 5.5322\ncoef_price = -0.1591\ncoef_seat_7 = -0.5248\ncoef_seat_8 = -0.2931\ncoef_cargo_3ft = 0.4385\ncoef_eng_elec = -1.4347\ncoef_eng_hyb = -0.7605\n\n# Define the attributes of each minivan\nminivans = [\n    {\"seats\": 7, \"cargo\": 2, \"engine\": \"Hyb\", \"price\": 30},\n    {\"seats\": 6, \"cargo\": 2, \"engine\": \"Gas\", \"price\": 30},\n    {\"seats\": 8, \"cargo\": 2, \"engine\": \"Gas\", \"price\": 30},\n    {\"seats\": 7, \"cargo\": 3, \"engine\": \"Gas\", \"price\": 40},\n    {\"seats\": 6, \"cargo\": 2, \"engine\": \"Elec\", \"price\": 40},\n    {\"seats\": 7, \"cargo\": 2, \"engine\": \"Hyb\", \"price\": 35}\n]\n\n# Function to calculate utility\ndef calculate_utility(minivan):\n    utility = coef_const\n    utility += coef_price * minivan[\"price\"]\n    if minivan[\"seats\"] == 7:\n        utility += coef_seat_7\n    elif minivan[\"seats\"] == 8:\n        utility += coef_seat_8\n    if minivan[\"cargo\"] == 3:\n        utility += coef_cargo_3ft\n    if minivan[\"engine\"] == \"Elec\":\n        utility += coef_eng_elec\n    elif minivan[\"engine\"] == \"Hyb\":\n        utility += coef_eng_hyb\n    return utility\n\n# Calculate utilities for each minivan\nutilities = [calculate_utility(minivan) for minivan in minivans]\n\n# Calculate the market shares using the softmax function\nexp_utilities = np.exp(utilities)\nmarket_shares = exp_utilities / np.sum(exp_utilities)\n\n# Create a DataFrame for the results\nminivan_names = ['A', 'B', 'C', 'D', 'E', 'F']\nmarket_shares_df = pd.DataFrame({\n    'Minivan': minivan_names,\n    'Market Share': market_shares\n})\n\nmarket_shares_df\n\n\n\n\n\n\n\n\n\n\nMinivan\nMarket Share\n\n\n\n\n0\nA\n0.116071\n\n\n1\nB\n0.419684\n\n\n2\nC\n0.313062\n\n\n3\nD\n0.078430\n\n\n4\nE\n0.020365\n\n\n5\nF\n0.052389\n\n\n\n\n\n\n\n\nHigh Market Share\nMinivan B (41.97%): 6 seats, 2ft cargo, gas engine, $30. This minivan has the highest market share, suggesting that consumers highly value the combination of a lower price ($30) with standard features (6 seats, 2ft cargo, gas engine). This option appears to be the most cost-effective and appeals to the majority of consumers.\nMinivan C (31.31%): 8 seats, 2ft cargo, gas engine, $30. This minivan also has a significant market share, indicating that some consumers are willing to opt for a vehicle with more seating capacity at the same price. The additional seats (8 seats) add utility, making it an attractive option despite not being the top choice.\nModerate Market Share Minivan A (11.61%): 7 seats, 2ft cargo, hybrid engine, $30. The hybrid engine in this minivan reduces its market share compared to the purely gas-powered options at the same price point. While hybrid engines are generally valued for their efficiency, the preference in this case seems to be towards conventional gas engines at a lower price.\nLow Market Share Minivan F (5.24%): 7 seats, 2ft cargo, hybrid engine, $35. The increase in price to $35, coupled with similar features as Minivan A, significantly lowers its market share. This suggests that consumers are sensitive to price increases and are less inclined to pay an extra $5 for a similar hybrid vehicle with the same number of seats and cargo space.\nMinivan D (7.84%): 7 seats, 3ft cargo, gas engine, $40. Despite offering more cargo space (3ft), the higher price of $40 detracts from its attractiveness. This indicates that the additional cargo space does not compensate for the higher price for most consumers.\nMinivan E (2.04%): 6 seats, 2ft cargo, electric engine, $40. This minivan has the lowest market share, suggesting that consumers place a relatively low value on electric engines in this context, especially when paired with a high price. The cost does not justify the perceived benefits of the electric engine, resulting in minimal consumer interest.\nKey Takeaways:\nPrice Sensitivity: Consumers show a strong preference for lower-priced options. Minivans priced at $30 dominate the market shares, indicating high price sensitivity.\nEngine Type Preferences: Gas engines are favored over hybrid and electric engines, reflecting either cost concerns or possibly a lack of perceived additional value from alternative engine types at higher prices.\nFeature Trade-offs: Additional features like more seats or cargo space are valued but have a diminishing return when paired with higher prices. Consumers appear to balance their preferences for additional features with their willingness to pay more."
  },
  {
    "objectID": "projects/project3/trial.html",
    "href": "projects/project3/trial.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "In this article we use the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/trial.html#estimating-yogurt-preferences",
    "href": "projects/project3/trial.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\nInteractive Raw Yogurt Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      id\n      y1\n      y2\n      y3\n      y4\n      f1\n      f2\n      f3\n      f4\n      p1\n      p2\n      p3\n      p4\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we‚Äôll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts‚Äô prices:\n\\[\nx_j' = [\\mathbf{1}(\\text{Yogurt 1}), \\mathbf{1}(\\text{Yogurt 2}), \\mathbf{1}(\\text{Yogurt 3}), X_f, X_p]\n\\]\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a ‚Äúwide‚Äù shape with \\(n\\) rows and multiple columns for each covariate, to a ‚Äúlong‚Äù shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we‚Äôll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be ‚Äúpivoted‚Äù or ‚Äúmelted‚Äù from wide to long.\n\n\n\n\n\n\nInteractive Cleaned Yogurt Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      id\n      product\n      chosen\n      featured\n      price\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\n\n\n\n\n\n\nlog_likelihood()\n\n\n\n\n\n\n\nCode\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculate the log-likelihood of the MNL model.\n\n    Parameters:\n    beta (array): Array of coefficients [Œ≤1, Œ≤2, Œ≤3, Œ≤f, Œ≤p].\n    data (DataFrame): The reshaped long format data with columns ['id', 'product', 'chosen', 'featured', 'price'].\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    data['yogurt1'] = (data['product'] == 1).astype(int)\n    data['yogurt2'] = (data['product'] == 2).astype(int)\n    data['yogurt3'] = (data['product'] == 3).astype(int)\n    data['utility'] = (beta1 * data['yogurt1'] + \n                       beta2 * data['yogurt2'] + \n                       beta3 * data['yogurt3'] + \n                       beta_f * data['featured'] + \n                       beta_p * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['chosen'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\n\n\n\n\n\n\nCode\ninitial_beta = np.zeros(5)\nlog_likelihood(initial_beta, yogurt_long)\n\n\n3368.6952975213344\n\n\nUsing optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)).\n\n\nCode\nresult = minimize(log_likelihood, initial_beta, args=(yogurt_long,), method='BFGS')\nestimated_beta = result.x\nestimated_beta\n\n\narray([  1.38775332,   0.64350491,  -3.08611501,   0.48741354,\n       -37.05792291])\n\n\n\n\nDiscussion\nWe learn the following‚Ä¶\n\nProduct Intercepts:\n\n(\\(\\beta_1\\)) (Yogurt 1): The positive coefficient suggests that Yogurt 1 is relatively preferred.\n(\\(\\beta_2\\)) (Yogurt 2): This positive coefficient also indicates a relative preference for Yogurt 2, but it‚Äôs lower than Yogurt 1.\n(\\(\\beta_3\\)) (Yogurt 3): The negative coefficient suggests that Yogurt 3 is less preferred compared to the omitted category (Yogurt 4).\n\nFeatured (\\(\\beta_f\\)): The positive coefficient 0.487 implies that featuring a yogurt increases its utility and thus its probability of being chosen.\nPrice (\\(\\beta_p\\)): The large negative coefficient -37.058 indicates a strong negative effect of price on the utility, meaning higher prices significantly reduce the likelihood of a yogurt being chosen.\n\nUsing the estimated price coefficient as a dollar-per-util conversion to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\n\nCode\nconversion_factor = -1 / estimated_beta[4]\nutility_difference = estimated_beta[0] - estimated_beta[2]\nmonetary_value = utility_difference * conversion_factor\nmonetary_value\n\n\n0.12072636520970716\n\n\n‚ùó The monetary benefit between the most-preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is approximately $0.12 per unit. This means consumers value Yogurt 1 about $0.12 more per unit than Yogurt 3, based on the estimated utilities. ‚ùó\n\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nCalculating market shares in the market at the time the data were collected. Then, increasing the price of yogurt 1 by $0.10 and using the fitted model to predict p(y|x) for each consumer and each product.\n\n\n\n\n\n\npredict_market_shares()\n\n\n\n\n\n\n\nCode\ndef predict_market_shares(beta, data):\n    \"\"\"\n    Predict market shares using the estimated beta coefficients.\n\n    Parameters:\n    beta (array): Array of coefficients [Œ≤1, Œ≤2, Œ≤3, Œ≤f, Œ≤p].\n    data (DataFrame): The reshaped long format data with columns ['id', 'product', 'chosen', 'featured', 'price'].\n\n    Returns:\n    DataFrame: The predicted market shares for each product.\n    \"\"\"\n    data['yogurt1'] = (data['product'] == 1).astype(int)\n    data['yogurt2'] = (data['product'] == 2).astype(int)\n    data['yogurt3'] = (data['product'] == 3).astype(int)\n    data['utility'] = (beta[0] * data['yogurt1'] + \n                       beta[1] * data['yogurt2'] + \n                       beta[2] * data['yogurt3'] + \n                       beta[3] * data['featured'] + \n                       beta[4] * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    market_shares = data.groupby('product')['probability'].mean().reset_index()\n    market_shares.columns = ['product', 'market_share']\n    return market_shares\n\n\n\n\n\n\n\n(   product  market_share\n 0        1      0.341975\n 1        2      0.401235\n 2        3      0.029218\n 3        4      0.227572,\n    product  market_share\n 0        1      0.021118\n 1        2      0.591145\n 2        3      0.044040\n 3        4      0.343697)\n\n\nMarket Shares Analysis\nThe market shares before and after the price increase of Yogurt 1 are as follows:\nOriginal Market Shares:\n\nYogurt 1: 34.20%\nYogurt 2: 40.12%\nYogurt 3: 2.92%\nYogurt 4: 22.76%\n\nAdjusted Market Shares (after $0.10 price increase for Yogurt 1):\n\nYogurt 1: 2.11%\nYogurt 2: 59.11%\nYogurt 3: 4.40%\nYogurt 4: 34.37%\n\nIncreasing the price of Yogurt 1 by $0.10 drastically decreases its market share from 34.20% to 2.11%. Meanwhile, the market shares for Yogurt 2, Yogurt 3, and Yogurt 4 increase, with Yogurt 2 seeing the most significant rise from 40.12% to 59.11%."
  },
  {
    "objectID": "projects/project3/trial.html#estimating-minivan-preferences",
    "href": "projects/project3/trial.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\n\n\n\n\n\n\nInteractive Conjoint Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      resp.id\n      ques\n      alt\n      carpool\n      seat\n      cargo\n      eng\n      price\n      choice\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCojoint Variables\n\n\n\n\n\n\nresp.id: Respondent identifier.\nques: Choice task number.\nalt: Alternative number within the choice task.\ncarpool: Carpool option (yes/no).\nseat: Number of seats (6, 7, 8).\ncargo: Cargo space (2ft, 3ft).\neng: Engine type (gas, hybrid).\nprice: Price in thousands of dollars.\nchoice: Indicator for whether the alternative was chosen (1 if chosen, 0 otherwise).\n\n\n\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\nNumber of respondents: 200\nNumber of choice tasks per respondent: 15\nNumber of alternatives presented in each choice task: 3\n\nEach respondent in the survey completed 15 choice tasks, with each task presenting 3 different alternatives to choose from.\n\n\nModel\nWe‚Äôll estimate an MNL model omitting the following levels to avoid multicollinearity:\n\n6 seats\n2ft cargo\nGas engine\n\nThe variables we will include in our model are:\n\nseat_7: Dummy variable for 7 seats.\nseat_8: Dummy variable for 8 seats.\ncargo_3ft: Dummy variable for 3ft cargo space.\neng_hyb: Dummy variable for hybrid engine.\nprice: Continuous variable for price in thousands of dollars.\n\n\n\n\n\n\n\nconjoint_log_likelihood()\n\n\n\n\n\n\n\nCode\ndef conjoint_log_likelihood(beta, data):\n    \"\"\"\n    Calculate the log-likelihood of the MNL model for the conjoint data.\n\n    Parameters:\n    beta (array): Array of coefficients [Œ≤_seat_7, Œ≤_seat_8, Œ≤_cargo_3ft, Œ≤_eng_hyb, Œ≤_price].\n    data (DataFrame): The conjoint data with dummy variables.\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta_seat_7, beta_seat_8, beta_cargo_3ft, beta_eng_hyb, beta_price = beta\n    data['utility'] = (beta_seat_7 * data['seat_7'] +\n                       beta_seat_8 * data['seat_8'] +\n                       beta_cargo_3ft * data['cargo_3ft'] +\n                       beta_eng_hyb * data['eng_hyb'] +\n                       beta_price * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby(['resp.id', 'ques'])['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['choice'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\n\n\n\n\n\n\nCode\ninitial_beta_conjoint = np.zeros(5)\nconjoint_result = minimize(conjoint_log_likelihood, initial_beta_conjoint, args=(conjoint_data,), method='BFGS')\nestimated_beta_conjoint = conjoint_result.x\nestimated_beta_conjoint\n\n\narray([-0.48592307, -0.28346544,  0.41191849, -0.10548881, -0.15573405])\n\n\nThe estimated coefficients for the MNL model are as follows:\n\nbeta seat 7: -0.486\nbeta seat 8: -0.283\nbeta cargo 3ft: 0.412\nbeta hybrid engine: -0.105\nbeta sprice: -0.156\n\n\n\nResults\n\nSeats:\n\n(7 seats): The negative coefficient suggests that 7 seats are less preferred compared to the baseline category (6 seats).\n(8 seats): The negative coefficient suggests that 8 seats are also less preferred compared to 6 seats, but less so than 7 seats.\n\nCargo Space:\n\n(3ft cargo): The positive coefficient indicates that 3ft of cargo space is preferred over 2ft of cargo space.\n\nEngine:\n\n(Hybrid Engine): The negative coefficient suggests that hybrid engines are less preferred compared to gas engines.\n\nPrice:\n\n(Price): The negative coefficient indicates that higher prices decrease the utility of the minivan, making it less likely to be chosen.\n\n\n\n\nCode\nconversion_factor_conjoint = -1 / estimated_beta_conjoint[4]\nutility_difference_cargo = estimated_beta_conjoint[2]\nmonetary_value_cargo = utility_difference_cargo * conversion_factor_conjoint\nmonetary_value_cargo\n\n\n2.645012364801245\n\n\nThe dollar value of having 3ft of cargo space compared to 2ft of cargo space is approximately $2,645. This means that, on average, consumers value the additional cargo space at $2,645.\nLet‚Äôs assume the market consists of the following 6 minivans.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nWe will use the estimated model to predict the market shares of these six minivans.\n\n\n\n\n\n\nCode block\n\n\n\n\n\n\n\nCode\nmarket_configurations = pd.DataFrame({\n    'minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n})\nmarket_configurations['seat_7'] = (market_configurations['seat'] == 7).astype(int)\nmarket_configurations['seat_8'] = (market_configurations['seat'] == 8).astype(int)\nmarket_configurations['cargo_3ft'] = (market_configurations['cargo'] == '3ft').astype(int)\nmarket_configurations['eng_hyb'] = (market_configurations['eng'] == 'hyb').astype(int)\nmarket_configurations['utility'] = (estimated_beta_conjoint[0] * market_configurations['seat_7'] +\n                                    estimated_beta_conjoint[1] * market_configurations['seat_8'] +\n                                    estimated_beta_conjoint[2] * market_configurations['cargo_3ft'] +\n                                    estimated_beta_conjoint[3] * market_configurations['eng_hyb'] +\n                                    estimated_beta_conjoint[4] * market_configurations['price'])\nmarket_configurations['exp_utility'] = np.exp(market_configurations['utility'])\nmarket_configurations['market_share'] = market_configurations['exp_utility'] / market_configurations['exp_utility'].sum()\n\n\n\n\n\nNote: Our professor took this example from the ‚ÄúR 4 Marketing Research‚Äù book by Chapman and Feit. üôÇ\nThe predicted market shares for the six minivan configurations are as follows:\n\n\n\nMinivan\nMarket Share\n\n\n\n\nA\n18.66%\n\n\nB\n33.70%\n\n\nC\n25.38%\n\n\nD\n6.59%\n\n\nE\n7.10%\n\n\nF\n8.56%\n\n\n\n\nMinivan B (6 seats, 2ft cargo, gas engine, $30k) has the highest predicted market share at 33.70%.\nMinivan C (8 seats, 2ft cargo, gas engine, $30k) and Minivan A (7 seats, 2ft cargo, hybrid engine, $30k) also have substantial market shares at 25.38% and 18.66%, respectively.\nMinivans with higher prices or different engine types (like hybrid or electric) tend to have lower market shares."
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Duyen Tran",
    "section": "",
    "text": "Grad Student in Business Analytics at University of California, San Diego."
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Duyen Tran",
    "section": "",
    "text": "Product Manager (Feb 2020 - Dec 2023)\nDigital Marketing Analyst (Jul 2017 - Feb 2020)\nAppraiser Intern (Jan 2017 - Jun 2017)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Duyen Tran",
    "section": "",
    "text": "MS in Business Analytics - University of California, San Diego.\nMBA in International Marketing - Management Development Institute of Singapore.\nBA in Real Estates - National Economics University, Viet Nam."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Hello, I‚Äôm Duyen Tran üëã - Business Analyst Extraordinaire",
    "section": "",
    "text": "I bridge the gap between IT and business, translating complex business needs into actionable technical requirements. With a background in diverse job experience, I pride myself on being the lynchpin that ensures project success.\n\nüìä Currently leveraging data analytics to drive business improvements.\nüìò Continuing my learning journey in data science to enrich my BA toolkit at UC San Diego.\nüíº Previously collaborated with cross-functional teams in the finance and healthcare sectors.\nüì´ Reach out to me at: duyen.tran@rady.ucsd.edu\n‚ö° Fun fact: In my free time, I enjoy making handmade things and exploring local coffee shops.\n\n\n\n\nMS in Business Analytics - University of California, San Diego (2023 - 2024)\nMBA in International Marketing - Management Development Institute of Singapore (2018 - 2019)\nBA in Real Estates - National Economics University, Viet Nam (2013 - 2017)\n\n\n\n\n\nDocumentation: User Stories, Use Cases\nData Analysis: Excel, SQL, PostgreSQL, Python(Especially: Pandas, Numpy, Scipy, Matplotlib), R(Especially: Tidyverse, Simmer)\n\n\n\n\n\nProduct Manager/Analyst (Feb 2020 - Dec 2023)\nDigital Marketing Analyst (Jul 2017 - Feb 2020)\nAppraiser Intern (Jan 2017 - Jun 2017)\n\n\n\n\n\nFinancial Forecasting Tool: Collaborated with finance experts to develop a tool that predicts quarterly revenue.\nE-Commerce Recommendation Engine: Worked with the marketing department to integrate a product recommendation system into an online marketplace.\n\n\n\n\n\n\n\nLinkedIn Badge\n\n\n\n\n\nI‚Äôm keen on integrating more data science techniques into my BA toolkit. If you have any course or book recommendations or any collaboration opportunities, please drop me a message!\n\n‚≠êÔ∏è From DuyenTran"
  },
  {
    "objectID": "projects/project4/index.html",
    "href": "projects/project4/index.html",
    "title": "Fraud Detection Report",
    "section": "",
    "text": "In this project, we developed a comprehensive fraud detection model using a methodical approach outlined in five primary stages: data preparation, feature engineering, feature selection, model evaluation and implementation. The culmination of this project in the deployment of an XGBoost-based fraud detection model represents a significant step forward in safeguarding our operations against fraudulent activities. The model‚Äôs performance, evidenced by substantial potential savings and a strategic 5% cutoff point, highlights its effectiveness in identifying high-risk transactions with precision.\nModel Methodology:\n\nBaseline Logistic Model\nDecision Tree\nRandom Forest\nLBGM\nCatboost\nNeural Network\nXGB\n\n\nDownload PDF file."
  },
  {
    "objectID": "projects/intuit/final.html",
    "href": "projects/intuit/final.html",
    "title": "Intuit Upgrade Notebook",
    "section": "",
    "text": "This analytical examines the efficacy of an upsell campaign targeting small businesses to upgrade to the latest QuickBooks software, based on a dataset of 75,000 entities from a pool of 801,821.\nUtilizing logistic regression and neural networks, the analysis aims to:\nBy leveraging logistic regression, we assess the probabilistic impact of variables on customer decisions, while neural networks allow for a deeper dive into complex, non-linear relationships among data points. This dual-model approach provides:"
  },
  {
    "objectID": "projects/intuit/final.html#data-analysis",
    "href": "projects/intuit/final.html#data-analysis",
    "title": "Intuit Upgrade Notebook",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pyrsm as rsm\n\n# increase plot resolution\n# mpl.rcParams[\"figure.dpi\"] = 150\nrsm.__version__\n\nintuit75k = pd.read_parquet(\"data/intuit75k.parquet\")\nrsm.md(\"data/intuit75k_description.md\")\n\n\n\nIntuit: Quickbooks upgrade\nThe purpose of this exercise is to gain experience modeling the response to an upsell campaign. The intuit75k.parquet file contains data on 75,000 (small) businesses that were selected randomly from the 801,821 that were sent the wave-1 mailing. The mailing contained an offer to upgrade to the latest version of the Quickbooks software.\nVariable res1 denotes which of these businesses responded to the mailing by purchasing Quickbooks version 3.0 from Intuit Direct. Note that Intuit Direct sells products directly to its customers rather than through a retailer. Use the available data to predict which businesses that did not respond to the wave-1 mailing, are most likely to respond to the wave-2 mailing. Note that variables were added, deleted, and recoded so please ignore the variable descriptions in Exhibit 3 in the case in the course reader. Instead, use the variable descriptions below:\n\n\nVariable description\n\nid: Small business customer ID\nzip5: 5-Digit ZIP Code (00000=unknown, 99999=international ZIPs).\nzip_bins: Zip-code bins (20 approx. equal sized bins from lowest to highest zip code number)\nsex: Gender Identity ‚ÄúFemale‚Äù, ‚ÄúMale‚Äù, or ‚ÄúUnknown‚Äù\nbizflag: Business Flag. Address contains a Business name (1 = yes, 0 = no or unknown).\nnumords: Number of orders from Intuit Direct in the previous 36 months\ndollars: Total $ ordered from Intuit Direct in the previous 36 months\nlast: Time (in months) since last order from Intuit Direct in previous 36 months\nsincepurch: Time (in months) since original (not upgrade) purchase of Quickbooks\nversion1: Is 1 if customer‚Äôs current Quickbooks is version 1, 0 if version 2\nowntaxprod: Is 1 if customer purchased tax software, 0 otherwise\nupgraded: Is 1 if customer upgraded from Quickbooks vs.¬†1 to vs.¬†2\nres1: Response to wave 1 mailing (‚ÄúYes‚Äù if responded else ‚ÄúNo‚Äù)\ntraining: 70/30 split, 1 for training sample, 0 for validation sample\n\n\n\n\n\nData Preprocessing\nIn the data preprocessing section, we will be reviewing the variables that are present in our dataset and identifying any potential scaling or data pre-processing actions that needs to be made. We will also be reviewing our dataset to ensure tha there are no null values, and that the data types are aligned.\n\n\nCode\nintuit75k.head()\n\n\n\n\n\n\n\n\n\n\nid\nzip5\nzip_bins\nsex\nbizflag\nnumords\ndollars\nlast\nsincepurch\nversion1\nowntaxprod\nupgraded\nres1\ntraining\nres1_yes\n\n\n\n\n0\n1\n94553\n18\nMale\n0\n2\n109.5\n5\n12\n0\n0\n0\nNo\n1\n0\n\n\n1\n2\n53190\n10\nUnknown\n0\n1\n69.5\n4\n3\n0\n0\n0\nNo\n0\n0\n\n\n2\n3\n37091\n8\nMale\n0\n4\n93.0\n14\n29\n0\n0\n1\nNo\n0\n0\n\n\n3\n4\n02125\n1\nMale\n0\n1\n22.0\n17\n1\n0\n0\n0\nNo\n1\n0\n\n\n4\n5\n60201\n11\nMale\n0\n1\n24.5\n2\n3\n0\n0\n0\nNo\n0\n0\n\n\n\n\n\n\n\n\n\n\nCode\nintuit75k.hist(figsize=(20, 20))\n\n\narray([[&lt;Axes: title={'center': 'id'}&gt;,\n        &lt;Axes: title={'center': 'zip_bins'}&gt;,\n        &lt;Axes: title={'center': 'bizflag'}&gt;],\n       [&lt;Axes: title={'center': 'numords'}&gt;,\n        &lt;Axes: title={'center': 'dollars'}&gt;,\n        &lt;Axes: title={'center': 'last'}&gt;],\n       [&lt;Axes: title={'center': 'sincepurch'}&gt;,\n        &lt;Axes: title={'center': 'version1'}&gt;,\n        &lt;Axes: title={'center': 'owntaxprod'}&gt;],\n       [&lt;Axes: title={'center': 'upgraded'}&gt;,\n        &lt;Axes: title={'center': 'training'}&gt;,\n        &lt;Axes: title={'center': 'res1_yes'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nPrior to creating any models, we want to review the distribution and scale of the variables in the dataset. If there is scale in the distribution of the variables as exemplified by the histograms produced above, this will effect the ability of our model to accurately and effectively predict the response to our second wave mailing outreach. Based on the distribution of the histograms above, the variable with the greatest skew is the dollars. It skews heavily towards the left, as do the numrods and sincepurch variables. Due to this, we will want to apply some type of transformation to these variables to normalize their distribution.\n\n\nCode\ntrain_data = intuit75k[intuit75k['training'] == 1]\n\n\nHere we are creating our training set of data. This corresponds to any record that has a 1 in the training column within the Intuit75k database. As per the intuit instructions, the data was already pre-split into 70:30 training:test sets. If the data was not pre-split, we would want to consider employing the StratifyShuffleSplit method to create our training and testing data. This would ensure that our training and test sets have the same number of yes‚Äôs, which would provide a more accurate and representative dataset for the training and testing of our model.\n\n\nCode\nintuit75k['res1'].value_counts()\n\n\nres1\nNo     71399\nYes     3601\nName: count, dtype: int64\n\n\n\n\nCorrelation Between Features\nIn addition to scaling the data before we start building our models, we want to identify any potential correlation between the features in our dataset. If any features are highly correlated, this could lead to multi-collinearity within our models and lead to inflation of the variable coefficients or inflated importance of features on our model. If we are able to identify any correlation prior to building our model, we are able to adjust which features are correlated and see how this might increase their importance if a highly-correlated feature is omitted from the model.\n\n\nCode\ncorr_columns = ['zip_bins','bizflag','numords','dollars','last','sincepurch']\ncorr_matrix = train_data[corr_columns].corr()\ncorr_matrix\n\n\n\n\n\n\n\n\n\n\nzip_bins\nbizflag\nnumords\ndollars\nlast\nsincepurch\n\n\n\n\nzip_bins\n1.000000\n-0.000679\n0.005397\n0.006667\n-0.000726\n-0.001555\n\n\nbizflag\n-0.000679\n1.000000\n-0.000113\n0.002398\n0.000647\n-0.003839\n\n\nnumords\n0.005397\n-0.000113\n1.000000\n0.587267\n-0.130395\n0.000284\n\n\ndollars\n0.006667\n0.002398\n0.587267\n1.000000\n-0.073897\n0.003649\n\n\nlast\n-0.000726\n0.000647\n-0.130395\n-0.073897\n1.000000\n-0.001506\n\n\nsincepurch\n-0.001555\n-0.003839\n0.000284\n0.003649\n-0.001506\n1.000000\n\n\n\n\n\n\n\n\nBased on the correlation matrix created above, we see that there is not a high level of correlation between the features of this dataset. A high level of correlation is traditionally considered within the range of 0.7 and 0.9. Correlation coefficients whose magnitude is between 0.5 and 0.7 indicates that variables are moderately correlated. Dollars and numrods have a moderate correlation value of 0.59. It would make sense that these variables have a moderate correlation. The numrod variable represents number of orders from Intuit Direct in the previous 36 months, whereas Dollars represents the total dollar value ordered from intuit direct in the previous 36 months. Traditionally, orders and dollar value tend to have a positive - direct relationship, as we see orders increase, we expect to see dollars increase as well.\n\n\nLogistic Regression Model\nNow that we have transformed the variables to be on the same scale and have identified correlated variables, we can begin building our first logistic regression model. The goal of this model is to predict the probability of a business responding to a mailed offer.\n\n\nCode\n#response rate of the first wave\nresponse_rate = train_data[\"res1_yes\"].mean()\nresponse_rate\n\n\n0.047580952380952383\n\n\nWe can find the average response rate to the first wave mailing offer by taking the mean of the res1_yes column across the training dataset. This will be important when we try to project our model‚Äôs findings onto the larger client base as a whole. This variable represents whether a business responded to the offer or not. The average response rate in the first wave mailing is 4.76%.\n\nLogistic Regression Model 1\n\n\nCode\nlr = rsm.logistic(\n    data = {'wave1': train_data},\n    rvar = \"res1_yes\",\n    evar = [\n        \"zip_bins\",\n        \"bizflag\",\n        \"numords\",\n        \"dollars\",\n        \"last\",\n        \"sincepurch\",\n        \"sex\",\n        \"version1\",\n        \"owntaxprod\",\n        \"upgraded\"\n    ]\n)\n\n\n\n\nCode\nlr.summary(main = True, vif = True)\n\n\nLogistic regression (GLM)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, bizflag, numords, dollars, last, sincepurch, sex, version1, owntaxprod, upgraded\nNull hyp.: There is no effect of x on res1_yes\nAlt. hyp.: There is an effect of x on res1_yes\n\n                 OR     OR%  coefficient  std.error  z.value p.value     \nIntercept     0.052  -94.8%        -2.96      0.087  -34.016  &lt; .001  ***\nsex[Male]     0.987   -1.3%        -0.01      0.052   -0.249   0.804     \nsex[Unknown]  0.991   -0.9%        -0.01      0.074   -0.118   0.906     \nzip_bins      0.946   -5.4%        -0.06      0.004  -14.931  &lt; .001  ***\nbizflag       1.047    4.7%         0.05      0.048    0.964   0.335     \nnumords       1.251   25.1%         0.22      0.019   12.010  &lt; .001  ***\ndollars       1.001    0.1%         0.00      0.000    3.638  &lt; .001  ***\nlast          0.958   -4.2%        -0.04      0.002  -17.959  &lt; .001  ***\nsincepurch    1.002    0.2%         0.00      0.004    0.518   0.605     \nversion1      2.082  108.2%         0.73      0.085    8.583  &lt; .001  ***\nowntaxprod    1.359   35.9%         0.31      0.101    3.048   0.002   **\nupgraded      2.540  154.0%         0.93      0.084   11.076  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nVariance inflation factors:\n\n              vif    Rsq\nsincepurch  3.736  0.732\nversion1    2.972  0.664\nupgraded    2.918  0.657\nnumords     1.558  0.358\ndollars     1.527  0.345\nowntaxprod  1.027  0.026\nlast        1.017  0.017\nsex         1.000  0.000\nbizflag     1.000  0.000\nzip_bins    1.000  0.000\n\n\n\nLogistic Regression Summary Interpretation\nBased on the first version of our logistic regression model, there are a couple of important things to note in regards to feature selection and overall performance of the model. First, we want to take a look at our Pseudo R-squared (McFadden) and Pseudo R-squared (McFadden Adjusted) values. Based on our output, we see that these values are 0.073 and 0.07, respectively. The Pseudo R-squared values are generally lower than the traditional R-squared in linear regression models and don‚Äôt have a fixed range. However, the values for our McFadden and McFadden are still on the lower side. This isn‚Äôt automatically a deal-breaker for this model or automatically a reason to throw our model away, but we would want weigh these R-squared values in conjunction with other factors (such as AUC, and Chi-squared) to evaluate the effectiveness and predictive performance of our model. \nIn addition to the r-squared values, we also want to evaluate the AUC value. AUC indicates a model‚Äôs ability to discriminate between a positive and negative class. The AUC value is on a fixed scale, with a base threshold of 0.5. Any value over 0.5 is considered to be a better value, and indicate a better model. Our first logisitic regression model had an AUC of 0.708 which suggests that the model has a good ability to distinguish between the two classes (i.e.¬†responded to the mailing offer or not). We can also use the AIC and BIC values to evaluate our model as well. The AIC and BIC respectively represent the Akaike Information Criterion and the Bayesian Information Criterion. These values also indicate the models‚Äôs fit to the data. Generally, the goal is to have a lower AIC and BIC value, and models with lower AIC and BIC values are preferred. Our model has an AIC value of 18688.123, and the BIC value of 18794.545. These values are a bit difficult to represent or evaluate at first, but will become more important when we begin to omit variables and see how these values change. If both the AIC and BIC values become lower when removing a variable, we would consider using the model that produces the lower values.\nFinally, the most important value when evaluating a models performance is the Chi-squared value. The Chi-squared value of 1425.521 with 11 degrees of freedom and a p.value &lt; 0.001 suggests that the model as a whole is statistically significant, and at least some of the predictors or features in this model are significantly related to the outcome. Therefore, our first logistic regression model with all of the variables included seems to be a statistically significant model with a decent prediction ability. We would want to consider removing any statistically insignificant variables to see how the model‚Äôs performance changes, in hopes of building a more powerful prediction model.\n\n\nCode\nlr.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\n\n\n\nPermutation Importance Plot Interpretation\nBased on the permutation importance graph above, we can see that the variables of importance are upgraded, last, zip_bins, numrods, version1, owntaxprod, dollars. These variables however, do not contribute to a very large decrease in AUC, but this graph allows us to view the ratio of importance of the variables. Additionally, we can see that the bizflag, sincepurch, and sex variables are not important. Based on their lack of importance via AUC decrease, and their statistical insignificance, as identified by their high p-values in the logistic regression summary, we will consider testing their removal from the model and seeing if the model‚Äôs performance changes.\n\n\nCode\nlr.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\n\nPrediction Plot Interpretation\nBased on the prediction plots above, it looks like gender has very little influence of a prediction value. Similarily bizflag, and since_purch also looks insignificant, and doesn‚Äôt seem to contribute much to the prediction value. Therefore, we will want to remove them from this logistic regression model to see if the new logistic regression improves the AUC and r^2 value.\n\n\nTesting the removal of features from the model\nAs mentioned above, bizflag, sincepurch, and sex all have low importance and are statistically insignificant to our model, as indicated by their high p-values. We want to test removing them from our model to see if the performance of our model changes. The change in model performance will be interpreted based on how the AUC, BIC, Pseudo R-Squared, and Chi-squared values change in comparison to our baseline model with all of the features included as explanatory variables.\n\nRemoving bizflag, sincepurch, and sex together\n\n\nCode\nlr.summary(main = False, test=['sincepurch', 'sex','bizflag'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 1.257 df (4), p.value 0.869\n\n\n\n\nModel Interpretation\nWe first wanted to see the model‚Äôs performance when we removed sincepurch, sex, and bizflag together from the model. As identified above, we noticed that these variables had the lowest importance to the model and were statistically insignificant to the baseline logistic regression model. The test function enables us to generate a logistic regression output as if we were generating the models without these variables being included. We are then able to compare the model‚Äôs performance with and without them included. As we see, the first output shows our baseline model‚Äôs output. We see the same metrics that were identified previously.\n The second output shows us the Pseudo R-squared values in the baseline logit model vs.¬†our new model with the variables being removed. As we see via the output, the Pseudo r-squared value does not change in model 1 vs.¬†model 2, this indicates that the overall explanatory power of the models is unchanged despite the removal of the sincepurch,sex, and bizflag variables. However, the chi-squared value and the p.value change significantly, which is critical. The chi-squared value of 1.257 with 4 degrees of freedom and a p-value of 0.869 from comparing Model 1 and Model 2 suggests that the variables removed do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes.\n essentially, the comparison suggests that the removal of bizflag, sincepurch, and sex between Model 1 and Model 2 does not significantly affect the model‚Äôs explanatory power or predictive ability. This is indicated by the unchanged Pseudo R-squared values and the non-significant chi-squared test for the comparison. Therefore, if removing variables does not significantly decrease the model‚Äôs performance, it might be beneficial to keep the model simpler by excluding them. A simpler model is easier to interpret and may generalize better to new data. We will also want to test the removal of the variables one by one to see if that improves the model‚Äôs explanatory power.\n\n\nRemoving sincepurch, and sex\n\n\nCode\nlr.summary(main = False, test = ['sincepurch', 'sex'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.331 df (3), p.value 0.954\n\n\n\n\nModel interpretation\nIn the previous section, we evaluated the performance of our logistic regression model when we removed the three insignificant variables from our model as interpreted from their individual p-values. In this removal round, we wanted to evalute the performance of our logistic regression model if we considered removing sincepurch and sex. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.331 with 3 degrees of freedom and a p-value of 0.954 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\nRemoving sex\n\n\nCode\nlr.summary(main = False, test = ['sex'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.062 df (2), p.value 0.97\n\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of sex. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.062 with 2 degrees of freedom and a p-value of 0.97 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\nRemoving sincepurch\n\n\nCode\nlr.summary(main = False, test=['sincepurch'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.268 df (1), p.value 0.605\n\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of sincepurch. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.268 with 1 degrees of freedom and a p-value of 0.605 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\nRemoving bizflag\n\n\nCode\nlr.summary(main = False, test=['bizflag'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.924 df (1), p.value 0.337\n\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of bizflag. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.924 with 1 degree of freedom and a p-value of 0.337 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\nRemoving dollars\n\n\nCode\nlr.summary(main = False, test=['dollars'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + last + sincepurch + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.070 vs 0.071\nChi-squared: 12.732 df (1), p.value &lt; .001\n\n\n\n\nModel interpretation\nAs mentioned previous, Both models have the same Pseudo R-squared values (0.071), suggesting that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.331 with 3 degrees of freedom and a p-value of 0.954 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n The comparison between Model 1 and Model 2 with Pseudo R-squared, Model 1 vs 2: 0.070 vs 0.071 and Chi-squared: 12.732 df (1), p.value &lt; .001 shows a minor improvement in the Pseudo R-squared from Model 1 to Model 2, indicating a slightly better fit in Model 2. The chi-squared value being significant (p &lt; .001) for the comparison suggests that the change between models (likely the inclusion of dollars in Model 2) significantly improves the model‚Äôs fit to the data.\n\n\nRemoving owntaxprod\n\n\nCode\nlr.summary(main = False, test=['owntaxprod'])\n\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 8.694 df (1), p.value 0.003\n\n\n\n\nModel interpretation\nModel 1 and Model 2 are logistic regression models predicting the binary outcome res_yes_no using different sets of explanatory variables. The primary difference between the models is the inclusion of owntaxprod in Model 2, which is not present in Model 1. Pseudo R-squared for Model 1 vs.¬†Model 2: 0.071 vs.¬†0.071. This metric provides a measure of the explanatory power of the models, with values closer to 1 indicating a better fit. The identical Pseudo R-squared values for both models suggest that they have the same explanatory power, meaning the addition of owntaxprod in Model 2 does not improve the model‚Äôs ability to explain the variability in the outcome res_yes_no.\nChi-squared: 8.694, df (1), p.value 0.003. This test compares the two models to determine if the addition of the owntaxprod variable in Model 2 significantly improves the model fit compared to Model 1. The chi-squared statistic is used to assess this, with the degrees of freedom (df) typically representing the difference in the number of parameters estimated between the two models. In this case, the df is 1, corresponding to the one additional variable in Model 2. The p-value of 0.003 indicates that the difference in fit between Model 1 and Model 2 is statistically significant at conventional significance levels (e.g., Œ± = 0.05). Therefore, despite the Pseudo R-squared values being identical, the statistical test suggests that owntaxprod provides a significant contribution to predicting the outcome when it is added to the model. \nGiven these results, we would probably want to include owntaxprod in our model. However, some other factors we would want to consider are:\n\n\nModel complexity: Adding more variables can make the model more complex and potentially harder to interpret.\n\n\nPractical significance: Evaluate whether the inclusion of owntaxprod has practical significance in addition to its statistical significance.\n\n\nCross-validation: Perform cross-validation to assess the generalizability of the model and avoid overfitting.\n\n\nFurther evaluation: Consider other metrics such as AUC, accuracy, precision, recall, and F1 score for a more comprehensive evaluation of model performance, especially in the context of the specific application or decision-making process the model is intended to support.\n\n\n\nCheck cross-validation\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"zip_bins\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: zip_bins, res1\nNull hyp : There is no association between zip_bins and res1\nAlt. hyp : There is an association between zip_bins and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nzip_bins                       \n1         21.7%   78.3%  100.0%\n2          4.2%   95.8%  100.0%\n3         3.37%  96.63%  100.0%\n4         4.06%  95.94%  100.0%\n5         3.35%  96.65%  100.0%\n...         ...     ...     ...\n17        3.88%  96.12%  100.0%\n18        5.21%  94.79%  100.0%\n19        4.14%  95.86%  100.0%\n20        3.71%  96.29%  100.0%\nTotal     4.76%  95.24%  100.0%\n\n[21 rows x 3 columns]\n\nChi-squared: 1802.53 df(19), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"sex\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: sex, res1\nNull hyp : There is no association between sex and res1\nAlt. hyp : There is an association between sex and res1\n\nRow percentages:\n\nres1       Yes      No   Total\nsex                           \nFemale    4.8%   95.2%  100.0%\nMale     4.76%  95.24%  100.0%\nUnknown   4.7%   95.3%  100.0%\nTotal    4.76%  95.24%  100.0%\n\nChi-squared: 0.1 df(2), p.value 0.95\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"bizflag\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: bizflag, res1\nNull hyp : There is no association between bizflag and res1\nAlt. hyp : There is an association between bizflag and res1\n\nRow percentages:\n\nres1       Yes      No   Total\nbizflag                       \n0        4.71%  95.29%  100.0%\n1        4.92%  95.08%  100.0%\nTotal    4.76%  95.24%  100.0%\n\nChi-squared: 1.01 df(1), p.value 0.32\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"version1\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: version1, res1\nNull hyp : There is no association between version1 and res1\nAlt. hyp : There is an association between version1 and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nversion1                       \n0         4.33%  95.67%  100.0%\n1         6.35%  93.65%  100.0%\nTotal     4.76%  95.24%  100.0%\n\nChi-squared: 79.91 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"owntaxprod\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: owntaxprod, res1\nNull hyp : There is no association between owntaxprod and res1\nAlt. hyp : There is an association between owntaxprod and res1\n\nRow percentages:\n\nres1          Yes      No   Total\nowntaxprod                       \n0           4.66%  95.34%  100.0%\n1           8.18%  91.82%  100.0%\nTotal       4.76%  95.24%  100.0%\n\nChi-squared: 40.07 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\npd.crosstab(\n    train_data[\"version1\"],\n    train_data[\"upgraded\"],\n    margins = True,\n    margins_name = \"Total\"\n).map(lambda x: \"{:,}\".format(x))\n\n\n\n\n\n\n\n\n\nupgraded\n0\n1\nTotal\n\n\nversion1\n\n\n\n\n\n\n\n0\n30,433\n10,877\n41,310\n\n\n1\n11,190\n0\n11,190\n\n\nTotal\n41,623\n10,877\n52,500\n\n\n\n\n\n\n\n\n\n\nCode\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"upgraded\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\n\nCross-tabs\nData     : intuit75k\nVariables: upgraded, res1\nNull hyp : There is no association between upgraded and res1\nAlt. hyp : There is an association between upgraded and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nupgraded                       \n0         3.99%  96.01%  100.0%\n1          7.7%   92.3%  100.0%\nTotal     4.76%  95.24%  100.0%\n\nChi-squared: 262.79 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\nCode\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"numords\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n2.568\n0.025\n103.967\n0.0\n***\n\n\n1\nres1[T.No]\n-0.523\n0.025\n-20.643\n0.0\n***\n\n\n\n\n\n\n\n\n\n\nCode\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"dollars\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n117.000\n1.615\n72.448\n0.0\n***\n\n\n1\nres1[T.No]\n-25.533\n1.655\n-15.430\n0.0\n***\n\n\n\n\n\n\n\n\n\n\nCode\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"last\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n12.022\n0.190\n63.192\n0.0\n***\n\n\n1\nres1[T.No]\n4.026\n0.195\n20.653\n0.0\n***\n\n\n\n\n\n\n\n\n\n\nCode\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"sincepurch\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n19.153\n0.200\n95.793\n0.0\n***\n\n\n1\nres1[T.No]\n-3.712\n0.205\n-18.121\n0.0\n***\n\n\n\n\n\n\n\n\n\n\n\nCheck the zip_bins\n\n\nCode\nfig = rsm.prop_plot(train_data, \"zip_bins\", \"res1\", \"Yes\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nintuit75k['zip_bins'] = intuit75k['zip_bins'].astype('category')\n\nlr = rsm.logistic(\n    data = {'intuit75k': train_data},\n    rvar = \"res1_yes\",\n    evar = [\n        \"zip_bins\",\n        \"bizflag\",\n        \"numords\",\n        \"dollars\",\n        \"last\",\n        \"sincepurch\",\n        \"sex\",\n        \"version1\",\n        \"owntaxprod\",\n        \"upgraded\"\n    ]\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\nOR\nOR%\ncoefficient\nstd.error\nz.value\np.value\n\n\n\n\n\n0\nIntercept\n0.052\n-94.814\n-2.959\n0.087\n-34.016\n0.000\n***\n\n\n1\nsex[T.Male]\n0.987\n-1.286\n-0.013\n0.052\n-0.249\n0.804\n\n\n\n2\nsex[T.Unknown]\n0.991\n-0.870\n-0.009\n0.074\n-0.118\n0.906\n\n\n\n3\nzip_bins\n0.946\n-5.376\n-0.055\n0.004\n-14.931\n0.000\n***\n\n\n4\nbizflag\n1.047\n4.743\n0.046\n0.048\n0.964\n0.335\n\n\n\n5\nnumords\n1.251\n25.150\n0.224\n0.019\n12.010\n0.000\n***\n\n\n6\ndollars\n1.001\n0.097\n0.001\n0.000\n3.638\n0.000\n***\n\n\n7\nlast\n0.958\n-4.172\n-0.043\n0.002\n-17.959\n0.000\n***\n\n\n8\nsincepurch\n1.002\n0.203\n0.002\n0.004\n0.518\n0.605\n\n\n\n9\nversion1\n2.082\n108.170\n0.733\n0.085\n8.583\n0.000\n***\n\n\n10\nowntaxprod\n1.359\n35.945\n0.307\n0.101\n3.048\n0.002\n**\n\n\n11\nupgraded\n2.540\n154.028\n0.932\n0.084\n11.076\n0.000\n***\n\n\n\n\n\n\n\n\n\n\nCode\nlr.plot(\"pred\", incl = \"zip_bins\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    intuit75k.query(\"(training == 1) & (zip_bins == 1)\")\n    .groupby(\"zip5\")['res1_yes']\n    .agg([\"mean\", \"sum\", \"count\"])\n    .sort_values(by = \"sum\", ascending = False)[0:10]\n    .assign(mean = lambda x: x[\"mean\"].apply(lambda y: f\"{100 * y:.2f}%\"))\n)\n\n\n\n\n\n\n\n\n\n\nmean\nsum\ncount\n\n\nzip5\n\n\n\n\n\n\n\n00801\n41.12%\n486\n1182\n\n\n00804\n35.43%\n45\n127\n\n\n00000\n3.00%\n3\n100\n\n\n01923\n37.50%\n3\n8\n\n\n01890\n17.65%\n3\n17\n\n\n02050\n28.57%\n2\n7\n\n\n01950\n50.00%\n2\n4\n\n\n01752\n13.33%\n2\n15\n\n\n01754\n40.00%\n2\n5\n\n\n01863\n22.22%\n2\n9\n\n\n\n\n\n\n\n\n\n\nImprove the model by adding dummies for zip 00801 and 00804\n\n\nCode\nintuit75k = intuit75k.assign(\n    zip801 = rsm.ifelse(intuit75k[\"zip5\"] == \"00801\", 1, 0),\n    zip804 = rsm.ifelse(intuit75k[\"zip5\"] == \"00804\", 1, 0),\n)\n\n\n\n\nCode\nlr = rsm.model.logistic(\n    data = {\"intuit75k\": intuit75k.query(\"training == 1\")},\n    rvar = \"res1\",\n    lev = \"Yes\",\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ]\n)\nlr.coef[30:].round(3)\n\n\n\n\n\n\n\n\n\n\nindex\nOR\nOR%\ncoefficient\nstd.error\nz.value\np.value\n\n\n\n\n\n30\nzip801\n25.333\n2433.301\n3.232\n0.164\n19.746\n0.0\n***\n\n\n31\nzip804\n18.398\n1739.833\n2.912\n0.249\n11.709\n0.0\n***\n\n\n\n\n\n\n\n\n\n\nCode\nlr.summary(main = False, fit = False, test = \"zip_bins\")\n\n\n\nModel 1: res1 ~ sex + numords + bizflag + dollars + last + owntaxprod + sincepurch + version1 + upgraded + zip801 + zip804\nModel 2: res1 ~ zip_bins + sex + numords + bizflag + dollars + last + owntaxprod + sincepurch + version1 + upgraded + zip801 + zip804\nPseudo R-squared, Model 1 vs 2: 0.146 vs 0.148\nChi-squared: 30.796 df (19), p.value 0.042\n\n\n\n\nCode\nintuit75k[\"pred_logit\"] = lr.predict(intuit75k)[\"prediction\"]\n\n\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_logit\")\n\n\n\n\n\n\n\n\n\n\n\nAccuracy of Logistic Regression Model\n\n\nCode\ndef accuracy(data, actual, predicted):\n    TP = FP = TN = FN = 0\n    for i, j in zip(actual, predicted):\n        if i == 1 and j == 1:\n            TP += 1\n        elif i == 0 and j == 1:\n            FP += 1\n        elif i == 0 and j == 0:\n            TN += 1\n        elif i == 1 and j == 0:\n            FN += 1\n        else:\n            pass\n    return (TN + TP) / (TN + TP + FN + FP)\n\n\n\n\nBuilding a Representative Neural Network Model\nBased on the output of the logistic regression models, we believe that employing a neural network model, that can account for underlying variable interactions, might be a better predictor than the logistic regression model. We first started by creating a simple neural network(NN) model that has 1 hidden layer with 1 node.\n\nNN with 1 Node and 1 Hidden Layer (all variables)\n\n\nCode\nclf0 = rsm.model.mlp(\n    data = {'intuit75k': intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (1,),\n    mod_type = 'classification'\n)\nclf0.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : intuit75k\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.768\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\n\nCode\nintuit75k[\"pred_nn0\"] = clf0.predict(intuit75k)[\"prediction\"]\n\n\n\n\nCode\nclf0.plot('vimp')\n\n\n\n\n\n\n\n\n\n\n\nCode\nclf0.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn0\")\n\n\n\n\n\n\n\n\n\n\n\n\nNN with 1 Node and 1 Hidden Layer (significant variables)\n\n\nCode\nclf1 = rsm.model.mlp(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (1,),\n    mod_type = 'classification'\n)\nclf1.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.768\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\n\nCode\nintuit75k[\"pred_nn1\"] = clf1.predict(intuit75k)[\"prediction\"]\n\n\n\n\nCode\nclf1.plot('vimp')\n\n\n\n\n\n\n\n\n\nThe permutation importance plot reveals that the geographic segmentation zip_bins, the timing of the last event (last), and the upgrade status (upgraded) are the most influential factors in predicting the yes/no response variable in your Multi-layer Perceptron model. The importance of zip_bins suggests that location-based characteristics are crucial in the model‚Äôs decision-making process. The prominence of last indicates that recent interactions or activities have a significant impact on the predictions, and the positive trend with upgraded implies that customers who have upgraded are more likely to yield a positive response.\n\n\nCode\nclf1.plot(\"pred\")\n\n\n\n\n\n\n\n\n\nThe partial dependence plots show a nuanced view of feature influences. A steeper slope for the zip_bins plot underscores its importance, aligning with the permutation importance results. Other variables like numords, dollars, version1, owntaxprod, and upgraded appear to have a more muted effect on the model‚Äôs output. Overall, the model‚Äôs predictive behavior is mostly swayed by where the customer is located, with other factors playing secondary roles.\nWe wanted to first create a base line neural network model. However, as we can see by the prediction plots, this nn model does not provide much in terms of the predictability based on various variables. We will want to build a neural network with more nodes and more hidden layers, and make our model a bit more flexible.\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn1\")\n\n\n\n\n\n\n\n\n\n\n\nEstimate a model with hidden layers sizes: (2, )\n\n\nCode\nclf2 = rsm.model.mlp(\n    data= {'wave1':intuit75k.query('training == 1')},\n    rvar=\"res1_yes\",\n    evar=[\n        'zip_bins',\n        'bizflag',\n        'sex',\n        'sincepurch',\n        'numords',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes=(2,),\n    mod_type=\"classification\",\n)\nclf2.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, bizflag, sex, sincepurch, numords, dollars, last, owntaxprod, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (2,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.777\n\nRaw data             :\nzip_bins  bizflag  sex  sincepurch  numords  dollars  last  owntaxprod  version1  upgraded  zip801  zip804\n      18        0 Male          12        2    109.5     5           0         0         0       0       0\n       1        0 Male           1        1     22.0    17           0         0         0       0       0\n       3        0 Male          17        1     20.0    17           0         0         1       0       0\n      11        1 Male          17        1     24.5     4           0         1         0       0       0\n       5        0 Male           9        3     73.5    10           0         0         0       0       0\n\nEstimation data      :\n bizflag  sincepurch   numords   dollars      last  owntaxprod  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n       0   -0.360809 -0.057079  0.207902 -1.137183           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n       0   -1.458194 -0.863720 -0.873735  0.119783           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       0    0.138003 -0.863720 -0.898458  0.119783           0         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       1    0.138003 -0.863720 -0.842831 -1.241930           0         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n       0   -0.660096  0.749561 -0.237115 -0.613447           0         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\n\nCode\nclf2.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\nIn the second neural network model, which now includes two hidden layers with one node each, the permutation importance plot indicates a shift in feature influence. Zip_bins retains its position as the most impactful feature, but upgraded has risen in importance, surpassing last, which was more significant in the first model. This suggests that the model is now giving more weight to the upgrade status of a customer when predicting the outcome. Features like numords and version1 also contribute to the model‚Äôs predictions, but to a lesser extent, while dollars and owntaxprod remain less influential.\n\n\nCode\nclf2.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nintuit75k['pred_nn2'] = clf2.predict(intuit75k)['prediction']\n\n\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn2\")\n\n\n\n\n\n\n\n\n\nThe partial dependence plots offer a more detailed look into the relationships between each feature and the model‚Äôs predictions. The zip_bins plot continues to show that lower bin values are associated with a higher probability of a positive response, reinforcing its top importance. The upgraded feature shows an upward trend, which aligns with its increased importance, suggesting that customers who have upgraded are more likely to have a positive response. Other features, such as numords, last, version1, and owntaxprod, show relatively flat trends, indicating a less significant effect on the prediction. These plots further confirm the model‚Äôs reliance on geographical segmentation and upgrade status, while time since last activity has become relatively less important compared to the first model.\nAs indicated by the prediction plots above, we see that by adding more hidden layers, our neural network become more flexible. However, the flexibility of the model can still be increased, and we will want to consider building an even more flexible model.\n\n\nNeural Network with 1 hidden layer and 3 nodes\n\n\nCode\nclf3 = rsm.model.mlp(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (3,),\n    mod_type = 'classification'\n)\nclf3.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (3,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.779\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\n\nCode\nclf3.plot('vimp')\n\n\n\n\n\n\n\n\n\nIn the third model, which is a neural network with one hidden layer and three nodes, the permutation importance plot shows a slight rearrangement in feature importance. Zip_bins continues to be the leading factor, suggesting that geographic segmentation remains a key predictor in the model. The feature upgraded retains substantial importance, affirming its strong predictive power regarding the response variable. Interestingly, version1 has increased in importance compared to the previous models, hinting that the specific version of a product or service is becoming more relevant in the model‚Äôs predictions. Last and numords follow in importance, indicating they are useful but less decisive in influencing the model‚Äôs outcome.\n\n\nCode\nclf3.plot('pred')\n\n\n\n\n\n\n\n\n\nThe partial dependence plots reveal the following insights:\n\nZip_bins shows a sharp decline, indicating a strong relationship between lower bin values and the likelihood of a positive response.\nThe effect of numords is relatively flat, suggesting a limited influence on the predicted outcome.\nThe last feature shows a downward trend, implying that more recent interactions might correspond to a lower probability of a positive outcome, which is consistent with its position in the permutation importance plot.\nThe upgraded plot has an upward trend, reinforcing its role as an influential predictor, as higher values correlate with a greater likelihood of a positive outcome.\n\nThe consistency of zip_bins and upgraded across all models as top predictors underlines their significance in determining the response variable. The rise of version1 in importance could indicate that the model is beginning to capture more nuanced patterns as the complexity of the network increases. However, features like dollars and owntaxprod remain less impactful, suggesting that they might not be as crucial for the model‚Äôs decision-making process or that their effects are possibly being captured indirectly through interactions with other features.\n\n\nCode\nintuit75k['pred_nn3a'] = clf3.predict(intuit75k)['prediction']\n\n\n\n\nNeural Network with 2 nodes and 10 hidden layers\n\n\nCode\nclf10 = rsm.model.mlp(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (10,10),\n    mod_type = 'classification'\n)\nclf10.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (10, 10)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.838\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\n\nCode\nclf10.plot('vimp')\n\n\n\n\n\n\n\n\n\n\n\nCode\nclf10.plot('pred')\n\n\n\n\n\n\n\n\n\n\n\nCode\nintuit75k['pred_nn10'] = clf10.predict(intuit75k)['prediction']\n\n\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn10\")\n\n\n\n\n\n\n\n\n\nFor the latest neural network model with two nodes in each of ten hidden layers, the summary and permutation importance plot provide insights into its learning process. This more complex model architecture did not converge, which suggests that the optimization algorithm (likely L-BFGS) was unable to find a solution that minimally satisfies the problem constraints within the iteration limit. This might be due to an insufficient number of iterations, a need for better scaling of input data, or an overly complex model for the given data.\nThe permutation importance plot for this non-converged model indicates that zip_bins remains the most critical feature, followed by upgraded and last, which is consistent with previous models. This underscores the model‚Äôs reliance on geographic data, upgrade status, and recent activity as key determinants of the predicted outcome. Other features such as numords, version1, and dollars continue to be relevant, but their impact on the model‚Äôs predictions is less pronounced.\nWithout convergence, however, the reliability of these importance scores might be compromised. It‚Äôs also important to note that without a properly converged model, any interpretation of the feature importances and relationships should be taken with caution. Adjusting the model‚Äôs parameters or preprocessing the input data could help achieve convergence and provide more reliable insights into the importance and effects of different features on the model‚Äôs predictions."
  },
  {
    "objectID": "projects/intuit/final.html#intuit-quickbooks-upgrade",
    "href": "projects/intuit/final.html#intuit-quickbooks-upgrade",
    "title": "Intuit Upgrade Notebook",
    "section": "Intuit: Quickbooks upgrade",
    "text": "Intuit: Quickbooks upgrade\nThe purpose of this exercise is to gain experience modeling the response to an upsell campaign. The intuit75k.parquet file contains data on 75,000 (small) businesses that were selected randomly from the 801,821 that were sent the wave-1 mailing. The mailing contained an offer to upgrade to the latest version of the Quickbooks software.\nVariable res1 denotes which of these businesses responded to the mailing by purchasing Quickbooks version 3.0 from Intuit Direct. Note that Intuit Direct sells products directly to its customers rather than through a retailer. Use the available data to predict which businesses that did not respond to the wave-1 mailing, are most likely to respond to the wave-2 mailing. Note that variables were added, deleted, and recoded so please ignore the variable descriptions in Exhibit 3 in the case in the course reader. Instead, use the variable descriptions below:"
  },
  {
    "objectID": "projects/intuit/final.html#variable-description",
    "href": "projects/intuit/final.html#variable-description",
    "title": "Intuit Upgrade Notebook",
    "section": "Variable description",
    "text": "Variable description\n\nid: Small business customer ID\nzip5: 5-Digit ZIP Code (00000=unknown, 99999=international ZIPs).\nzip_bins: Zip-code bins (20 approx. equal sized bins from lowest to highest zip code number)\nsex: Gender Identity ‚ÄúFemale‚Äù, ‚ÄúMale‚Äù, or ‚ÄúUnknown‚Äù\nbizflag: Business Flag. Address contains a Business name (1 = yes, 0 = no or unknown).\nnumords: Number of orders from Intuit Direct in the previous 36 months\ndollars: Total $ ordered from Intuit Direct in the previous 36 months\nlast: Time (in months) since last order from Intuit Direct in previous 36 months\nsincepurch: Time (in months) since original (not upgrade) purchase of Quickbooks\nversion1: Is 1 if customer‚Äôs current Quickbooks is version 1, 0 if version 2\nowntaxprod: Is 1 if customer purchased tax software, 0 otherwise\nupgraded: Is 1 if customer upgraded from Quickbooks vs.¬†1 to vs.¬†2\nres1: Response to wave 1 mailing (‚ÄúYes‚Äù if responded else ‚ÄúNo‚Äù)\ntraining: 70/30 split, 1 for training sample, 0 for validation sample"
  },
  {
    "objectID": "projects/intuit/final.html#evaluating-neural-network-performance-from-gains-chart",
    "href": "projects/intuit/final.html#evaluating-neural-network-performance-from-gains-chart",
    "title": "Intuit Upgrade Notebook",
    "section": "Evaluating Neural Network Performance from Gains Chart",
    "text": "Evaluating Neural Network Performance from Gains Chart\n\n\nCode\nintuit75k.head()\n\n\n\n\n\n\n\n\n\n\nid\nzip5\nzip_bins\nsex\nbizflag\nnumords\ndollars\nlast\nsincepurch\nversion1\n...\ntraining\nres1_yes\nzip801\nzip804\npred_logit\npred_nn0\npred_nn1\npred_nn2\npred_nn3a\npred_nn10\n\n\n\n\n0\n1\n94553\n18\nMale\n0\n2\n109.5\n5\n12\n0\n...\n1\n0\n0\n0\n0.044161\n0.039253\n0.039253\n0.046232\n0.040749\n0.040564\n\n\n1\n2\n53190\n10\nUnknown\n0\n1\n69.5\n4\n3\n0\n...\n0\n0\n0\n0\n0.022178\n0.020067\n0.020067\n0.029727\n0.027667\n0.018918\n\n\n2\n3\n37091\n8\nMale\n0\n4\n93.0\n14\n29\n0\n...\n0\n0\n0\n0\n0.091385\n0.094208\n0.094208\n0.075088\n0.100424\n0.151937\n\n\n3\n4\n02125\n1\nMale\n0\n1\n22.0\n17\n1\n0\n...\n1\n0\n0\n0\n0.011381\n0.011572\n0.011572\n0.012489\n0.006542\n0.022096\n\n\n4\n5\n60201\n11\nMale\n0\n1\n24.5\n2\n3\n0\n...\n0\n0\n0\n0\n0.025147\n0.024115\n0.024115\n0.021093\n0.025415\n0.009841\n\n\n\n\n5 rows √ó 23 columns\n\n\n\n\n\n\nCode\nfig = rsm.gains_plot(intuit75k[intuit75k.training == 1],\n    \"res1\", \"Yes\",\n    [\"pred_logit\", \"pred_nn0\",\"pred_nn1\", \"pred_nn2\", \"pred_nn3a\", \"pred_nn10\"])\n\n\n\n\n\n\n\n\n\nThe gains chart provided compares the performance of three predictive models: a logistic regression model (pred_logit) and two neural network models (pred_nn1 and pred_nn2). The chart plots the percentage of positive outcomes (buyers) captured against the percentage of the population targeted based on the model‚Äôs predictions.\nAll models perform above the diagonal line, which represents random chance; this indicates that each model has predictive power beyond mere guessing. The chart shows that the logistic regression and the first neural network model (pred_nn1) have similar performance, capturing nearly the same percentage of buyers across the population targeted. The second neural network model (pred_nn2) appears to perform slightly better, capturing a marginally higher percentage of buyers for most of the population percentages targeted. This suggests that pred_nn2 may have a more nuanced understanding of the data, possibly due to a more complex model structure, and is able to more accurately identify potential buyers.\nHowever, the differences between the models are not drastic, which implies that the complexity added in the neural network models may not be translating into significantly better performance over the logistic regression in this case. The closeness of their performances also suggests that the relationships within the data might be captured almost as effectively by the simpler logistic regression model as by the more complex neural network models.\n\n\nCode\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", [\"pred_logit\"])\n\n\n\n\n\n\n\n\n\nBoth the training and test curves lie above the diagonal, indicating that the logistic regression model has learned to identify buyers more effectively than random chance. The closeness of the two lines suggests that the model generalizes well; it performs similarly on both the training data (seen data) and the test data (unseen data). This is indicative of a good model fit without overfitting, as overfitting would typically be revealed by a high performance on the training data but a significantly lower performance on the test data.\nThe chart shows that as you target more of the population based on the model‚Äôs predictions, you capture a higher percentage of buyers. Both lines show a relatively steady and linear increase, suggesting that the model ranks potential buyers effectively across the entire dataset. There‚Äôs no sharp increase at the beginning, which would suggest that there‚Äôs a subset of the population with a significantly higher likelihood of buying. Instead, the model‚Äôs predictions are spread out across the population, capturing buyers at a fairly uniform rate as more of the population is targeted.\n\n\nCode\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn0\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn1\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn2\")\n\n\n\n\n\n\n\n\n\nThis gains chart displays the performance of the second neural network model (pred_nn2) on both the training and test datasets. The chart demonstrates that the model is effective at identifying buyers, as indicated by the curves lying well above the diagonal line representing random selection.\nThe performance on the training data is very close to the performance on the test data, which suggests that the model generalizes well and is not overfitting. Overfitting would be indicated by a high performance on the training data but significantly worse performance on the test data. The fact that both curves are almost superimposed on each other indicates that the patterns the model has learned from the training data are applicable to the unseen test data.\nAdditionally, the curves are steeper at the beginning, which means that targeting a smaller percentage of the population based on the model‚Äôs predictions can capture a larger percentage of buyers. This is a desirable trait in a model used for prioritizing which segments of a population to target for marketing or intervention efforts. The model seems to rank potential buyers effectively, identifying those most likely to purchase early on as the population is targeted incrementally. This can be particularly valuable in applications where it is cost-effective to target only a subset of the entire population."
  },
  {
    "objectID": "projects/replicate/index.html",
    "href": "projects/replicate/index.html",
    "title": "A Replication of Karlan and List",
    "section": "",
    "text": "Code\n# import sys;\n# print(sys.executable)"
  },
  {
    "objectID": "projects/replicate/index.html#data",
    "href": "projects/replicate/index.html#data",
    "title": "A Replication of Karlan and List",
    "section": "Data",
    "text": "Data\n\nDescription\nThe data set contains 50,083 observations and 51 variables. The key variables are as follows:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows √ó 51 columns\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB"
  },
  {
    "objectID": "projects/replicate/index.html#balance-test",
    "href": "projects/replicate/index.html#balance-test",
    "title": "A Replication of Karlan and List",
    "section": "Balance Test",
    "text": "Balance Test\n\nNumber of Months since last donation\nTest months since last donation to see if the treatment and control group are statistically different at the 95% confidence level\n\nT-test\n\\[H_0: \\mu_{months\\;treatment} = \\mu_{months\\;control} \\] \\[H_a: \\mu_{months\\;treatment} \\neq \\mu_{months\\;control}  \\]\nThe t-value formula is given by:\n\\[\nt_{\\text{value}} = \\frac{\\bar{x}_{\\text{1}} - \\bar{x}_{\\text{2}}}{\\sqrt{\\frac{s_{\\text{1}}^2}{n_{\\text{1}}} + \\frac{s_{\\text{2}}^2}{n_{\\text{2}}}}}\n\\]\nI know there are serverals library that can automatically calculate the t values for us, but as structure, we will define the t_value using the formula in the class slides\n\n\nCode\ndef t_value(treatment, control):\n    # calculate x_bar\n    x_treatment = treatment.mean()\n    x_control = control.mean()\n    #calculate std\n    s_treatment = treatment.std()\n    s_control = control.std()\n    \n    n_treatment = len(treatment)\n    n_control = len(control)\n    \n    t_value = (x_treatment - x_control)/np.sqrt((s_treatment**2/n_treatment) + (s_control**2/n_control))\n    return t_value\n\n\n\n\nCode\nt_value_months = t_value(months_treatment, months_control)\nprint(f\"T_values is {round(t_value_months, 4)}\")\n\n\nT_values is 0.1195\n\n\nThe formula of degree of freedom is:\n\\[\ndf = \\frac{(n_1 - 1)(n_2 - 1)}{(n_2 - 1)(\\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}})^2 + (n_1 - 1)(1 - \\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}})^2}\n\\]\nDefine a fuction to find degree of freedom\n\n\nCode\ndef dof(treatment, control):\n    # calculate x_bar\n    x_treatment = treatment.mean()\n    x_control = control.mean()\n    #calculate std\n    s_treatment = treatment.std()\n    s_control = control.std()\n    \n    n_treatment = len(treatment)\n    n_control = len(control)\n    \n    # Find the degree of freedom\n\n    #Assign the complex formula in the denorminator to b\n    b = (s_treatment**2/n_treatment)/(s_treatment**2/n_treatment+s_control**2/n_control)\n    \n    numerator = (n_treatment-1)*(n_control-1)\n    \n    denominator = (n_control-1)*(b**2) + (n_treatment-1)*((1-b)**2)\n    \n    degree_of_freedom = numerator/denominator\n    return degree_of_freedom\n\n\n\n\nCode\ndof_months = dof(months_treatment, months_control)\nprint(f\"Degree of freedom is {round(dof_months, 2)}\")\n\n\nDegree of freedom is 33394.14\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_months, dof_months))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.905\n\n\nThe independent t-test on the ‚Äòmrm2‚Äô variable (months since last donation) between the treatment and control groups yields a t-statistic of approximately 0.1195 and a p-value of 0.905.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean number of months since the last donation between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'mrm2',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n12.998\n0.094\n138.979\n0.000\n***\n\n\n1\ntreatment\n0.014\n0.115\n0.119\n0.905\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.014 with a standard error of about 0.115.\nThe t-statistic for the treatment coefficient is 0.119, and the p-value is 0.905.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the number of months since last donation (mrm2) between the treatment and control groups at the 95% confidence level.\n\n\n\nHighest previous contribution\nTest the highest previous contribution to see if the treatment and control group are statistically different at 95% confidence level\n\\[H_0: \\mu_{contribution\\;treatment} = \\mu_{contribution\\;control} \\] \\[H_a: \\mu_{contribution\\;treatment} \\neq \\mu_{contribution\\;control}  \\]\n\n\nCode\nhpa_treatment = data.loc[data['treatment'] == 1,'hpa']\nhpa_control = data.loc[data['treatment'] == 0,'hpa']\n\n# Apply the t_values function that we defined above\nt_value_hpa = t_value(hpa_treatment, hpa_control)\nprint(f\"T_value is {round(t_value_hpa, 4)}\")\n\n\nT_value is 0.9704\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_hpa = dof(hpa_treatment, hpa_control)\nprint(f\"Degree of freedom is {round(dof_hpa, 2)}\")\n\n\nDegree of freedom is 35913.89\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_hpa, dof_hpa))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.332\n\n\nThe independent t-test on the ‚Äòhpa‚Äô variable (highest previous contribution) between the treatment and control groups yields a t-statistic of approximately 0.9704 and a p-value of 0.332.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean number of the highest previous contribution between the treatment and control groups.\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'hpa',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n58.960\n0.551\n107.005\n0.000\n***\n\n\n1\ntreatment\n0.637\n0.675\n0.944\n0.345\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.637 with a standard error of about 0.675.\nThe t-statistic for the treatment coefficient is 0.944, and the p-value is 0.345.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the number of highest previous contribution of treatment and control groups at the 95% confidence level.\n\n\n\nPercent already donated in 2005\nTest the percent already donated in 2005 to see if the treatment and control group are statistically different at 95% confidence level\n\\[H_0: \\mu_{percent\\;treatment} = \\mu_{percent\\;control} \\] \\[H_a: \\mu_{percent\\;treatment} \\neq \\mu_{percent\\;control}  \\]\n\nT-test\n\n\nCode\ndormant_treatment = data.loc[data['treatment'] == 1,'dormant']\ndormant_control = data.loc[data['treatment'] == 0,'dormant']\n\n# Apply the t_values function that we defined above\nt_value_dormant = t_value(dormant_treatment, dormant_control)\nprint(f\"T_value is {round(t_value_dormant, 3)}\")\n\n\nT_value is 0.174\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_dormant = dof(dormant_treatment, dormant_control)\nprint(f\"Degree of freedom is {round(dof_dormant, 2)}\")\n\n\nDegree of freedom is 33362.05\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_dormant, dof_dormant))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.862\n\n\nThe independent t-test on the ‚Äòdormant‚Äô variable (Percent already donated in 2005) between the treatment and control groups yields a t-statistic of approximately 0.174 and a p-value of 0.862.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the mean percent already donated in 2005 between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'dormant',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.523\n0.004\n135.247\n0.000\n***\n\n\n1\ntreatment\n0.001\n0.005\n0.174\n0.862\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.001 with a standard error of about 0.005.\nThe t-statistic for the treatment coefficient is 0.171, and the p-value is 0.862.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the percent already donated in 2005 of treatment and control groups at the 95% confidence level.\n\n\n\nProportion of white people within the zipcode\n\\[H_0: \\mu_{pwhite\\;treatment} = \\mu_{pwhite\\;control} \\] \\[H_a: \\mu_{pwhite\\;treatment} \\neq \\mu_{pwhite\\;control}  \\]\n\nT-test\n\n\nCode\npwhite_treatment = data.loc[data['treatment'] == 1,'pwhite']\npwhite_control = data.loc[data['treatment'] == 0,'pwhite']\n\n# Apply the t_values function that we defined above\nt_value_pwhite = t_value(pwhite_treatment, pwhite_control)\nprint(f\"T_value is {round(t_value_pwhite, 3)}\")\n\n\nT_value is -0.57\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_pwhite = dof(pwhite_treatment, pwhite_control)\nprint(f\"Degree of freedom is {round(dof_pwhite, 2)}\")\n\n\nDegree of freedom is 33185.24\n\n\n\n\nCode\n# Find p-value\np_value = (t.cdf(t_value_pwhite, dof_pwhite))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.569\n\n\nThe independent t-test on the ‚Äòpwhite‚Äô variable (proportion of white people within zipcode) between the treatment and control groups yields a t-statistic of approximately -0.57 and a p-value of 0.569.\nGiven the high p-value (much greater than the alpha level of 0.05), we fail to reject the null hypothesis, which suggests that there is no statistically significant difference in the proportion of white people within zipcode between the treatment and control groups.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'pwhite',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.820\n0.001\n616.281\n0.000\n***\n\n\n1\ntreatment\n-0.001\n0.002\n-0.560\n0.575\n\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately - 0.001 with a standard error of about 0.002.\nThe t-statistic for the treatment coefficient is -0.560, and the p-value is 0.575.\n\nThese results are consistent with the independent t-test findings. The p-value in both analyses is much larger than the alpha level of 0.05, indicating no statistical significance. The t-statistic from the regression is the same as the t-statistic from the t-test, and the p-value confirms that there is no significant difference in the proportion of white people in the zipcode of treatment and control groups at the 95% confidence level.\nSidenote: I can‚Äôt replicate the results same as table 1 in the paper. Mean for pwhite is:\n\n\nCode\navg_white_treatment = pwhite_treatment.mean()\navg_white_control = pwhite_control.mean()\nprint(f\"The avg pwhite of treatment group is {round(avg_white_treatment,2)}\")\n\nprint(f\"The avg pwhite of treatment group is {round(avg_white_control,2)}\")\n\n\nThe avg pwhite of treatment group is 0.8199999928474426\nThe avg pwhite of treatment group is 0.8199999928474426\n\n\nWhereas the results in table 1 are 0.831 and 0.830, respectively"
  },
  {
    "objectID": "projects/replicate/index.html#charitable-contribution-made",
    "href": "projects/replicate/index.html#charitable-contribution-made",
    "title": "A Replication of Karlan and List",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\n\nProportion of donation\n\n\nCode\ngave_treatment = data[data['treatment'] == 1]['gave'].mean()\ngave_control = data[data['treatment'] == 0]['gave'].mean()\n\nproportions = [gave_treatment, gave_control]\ngroup_labels = ['Treatment', 'Control']\n\n# Create the bar plot\nplt.bar(group_labels, proportions, color=['blue', 'orange'])\n\n# Add labels and title\nplt.ylabel('Proportion who donated')\nplt.title('Proportion of Donations by Group')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAs visual, we can interpret that the treatment group had a higher response rate for making donations than the control group, which implies that the treatment might have effective in encouraging donations. However, we have to do further statistical analysis to assess the reliability of this observed difference\n\n\nT-test\nFirst of all, let‚Äôs check the similarity between our data and Panel A in table 2A\n\n\nCode\ndonated_treatment = data[data['treatment'] == 1]['gave']\ndonated_control = data[data['treatment'] == 0]['gave']\n\nx_bar_treatment = donated_treatment.mean()\nx_bar_control = donated_control.mean()\nprint(f'The response rate of treatment group panel A is {round(x_bar_treatment, 3)}')\nprint(f'The response rate of treatment group panel A is {round(x_bar_control, 3)}')\n\n\nThe response rate of treatment group panel A is 0.022\nThe response rate of treatment group panel A is 0.018\n\n\n\n\nCode\n# Apply the t_values function that we defined above\nt_value_donated = t_value(donated_treatment, donated_control)\nprint(f\"T_value is {round(t_value_donated, 3)}\")\n\n\nT_value is 3.209\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_donated = dof(donated_treatment, donated_control)\nprint(f\"Degree of freedom is {round(dof_donated, 2)}\")\n\n\nDegree of freedom is 36576.84\n\n\n\n\nCode\n# Find p-value\np_value = (1 - t.cdf(t_value_donated, dof_donated))*2\nprint(f\"P_value is {round(p_value, 3)}\")\n\n\nP_value is 0.001\n\n\nThe independent t-test on the ‚Äògave‚Äô variable between the treatment and control groups yields a t-statistic of approximately 3.209 and a p-value of 0.001.\nThis p-value is less than the alpha level of 0.05, indicating that there is a statistically significant difference between the treatment and control groups in terms of the proportion of participants who made a donation.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'gave',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\ntreatment\n0.004\n0.001\n3.101\n0.002\n**\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\n\nThe coefficient for treatment is approximately 0.004 with a standard error of about 0.001.\nThe t-statistic for the treatment coefficient is 3.1, and the p-value is 0.002.\n\nThis p-value is also less than the alpha level of 0.05, suggesting that the treatment has a statistically significant effect on the likelihood of making a donation. The positive coefficient indicates that being in the treatment group is associated with higher odds of giving a donation compared to the control group.\nBoth the t-test and logistic regression demonstrate that there is a significant difference in the donation behavior between the treatment and control groups, with the treatment group showing a higher propensity to give\nIn the context of the experiment, the statistical significance of the treatment group‚Äôs coefficient implies that the treatment has a positive effect on the likelihood that someone will donate. This insight into human behavior suggests that the strategy employed to encourage donations in the treatment group was effective in increasing donation rates.\nIn other words, if the experiment involved sending out letters asking for donations, and the treatment group received letters with a special message or offer not given to the control group, we could interpret these results to mean that the special message or offer motivated more people to donate. The important takeaway here is that small changes in how we ask for donations can have a significant impact on people‚Äôs willingness to contribute to a cause. This aligns with the findings presented in Table 2A, Panel A, which we aimed to confirm with our analysis.\n\n\nProbit Regression\n\n\nCode\nX_linear = sm.add_constant(data['treatment'])  # Add a constant to the independent variable\ny_linear = data['gave'] \n# Fit the Probit regression model using 'treatment' as the predictor and 'gave' as the binary outcome\nprobit_model = sm.Probit(y_linear, X_linear).fit()\n\n# Get the summary of the Probit regression results\nprobit_summary = probit_model.summary()\n# Extract only the regression results table\nprobit_results_table = probit_summary.tables[1]\n\n# To display or print out the table\nprint(probit_results_table)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThe Probit regression results are as follows:\nThe coefficient for treatment is 0.0868, with a standard error of 0.028. The z-statistic for the treatment coefficient is 3.113, and the p-value is 0.002.\nThe coeficient and standard errors do not match between the Table 3 on the paper and the Probit Regression, it matches to the Linear regression results instead\nHowever, p value of both models are the same, which is smaller than 0.05, sugessts that treatment had a positive impact on the probability of giving, which supports the same notion: the treatment apprears to have effectively encouraged more people to donate"
  },
  {
    "objectID": "projects/replicate/index.html#differences-between-match-rates",
    "href": "projects/replicate/index.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nThe Z score formula of proportion is \\[\nz = \\frac{p_a - p_b}{\\sqrt{\\frac{p_a(1 - p_a)}{n_a} + \\frac{p_b(1 - p_b)}{n_b}}}\n\\]\nSame as the t_value function above, let‚Äôs define z_score function\n\n\nCode\ndef z_score(data1, data2, full_data):\n    p1 = data1.mean()\n    p2 = data2.mean()\n\n    numerator = p1-p2\n    denominator = np.sqrt((p1*(1-p1))/len(data1) + (p2*(1-p2)/len(data2)))\n\n    z_score = numerator / denominator\n    return z_score\n\n\nDouble check the similarity response rate to Table 2 Panel A\n\n\nCode\ngave_ratio1 = data[data['ratio'] == 1]['gave']\ngave_control = data[data['ratio'] == \"Control\"]['gave']\ngave_ratio2 = data[data['ratio'] == 2]['gave']\ngave_ratio3 = data[data['ratio'] == 3]['gave']\n\nprint(f'Response rate of 1:1 matching is {round(gave_ratio1.mean(), 3)}')\nprint(f'Response rate of 2:1 matching is {round(gave_ratio2.mean(), 3)}')\nprint(f'Response rate of 3:1 matching is {round(gave_ratio3.mean(), 3)}')\n\n\nResponse rate of 1:1 matching is 0.021\nResponse rate of 2:1 matching is 0.023\nResponse rate of 3:1 matching is 0.023\n\n\n\nControl vs 1:1 match\n\\[H_0: p_{Control} = p_{1:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{1:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\n# Apply the function that we defined above\nz_score_ratio1 = z_score(gave_ratio1, gave_control, data)\nprint(f'Z score is {round(z_score_ratio1, 3)}')\n\n\nZ score is 1.705\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio1)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.088\n\n\nZ-Value: A z-value of 1.705 indicates that the effect of being in the 1:1 match ratio group is 1.705 standard deviations away from the mean effect of being in the control group. This suggests a positive direction of influence towards increasing the likelihood of donation.\nP-Value: The p-value of 0.088, though close, is above the conventional threshold of 0.05 (95% confidence level). This suggests that while there is some evidence to suggest an effect, it does not meet the usual criteria for statistical significance. Hence, we would not reject the null hypothesis at the 5% significance level, which states there is no difference in donation likelihood between the 1:1 match and control groups\n\n\n\nControl vs 2:1 match\n\\[H_0: p_{Control} = p_{2:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{2:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\nz_score_ratio2 = z_score(gave_ratio2, gave_control, data)\nprint(f'Z score is {round(z_score_ratio2, 4)}')\n\n\nZ score is 2.7397\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio2)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.006\n\n\nZ-Value: A z-value of 2.739 indicates that the effect of being in the 2:1 match ratio group is 2.739 standard deviations away from the mean effect of being in the control group. This represents a stronger and more distinct effect compared to the control, suggesting a significant positive influence on donation likelihood.\nP-Value: The p-value of 0.006 is well below the conventional threshold of 0.05, indicating strong statistical significance. This suggests that we can reject the null hypothesis, or there is significant difference in the likelihood of making a donation between the 2:1 match ratio and control groups.\n\n\n\nControl vs 3:1 match\n\\[H_0: p_{Control} = p_{3:1\\;Ratio}\\] \\[H_a: p_{Control} \\neq p_{3:1\\;Ratio}\\]\n\nZ-test\n\n\nCode\nz_score_ratio3 = z_score(gave_ratio3, gave_control, data)\nprint(f'Z score is {round(z_score_ratio3, 3)}')\n\n\nZ score is 2.793\n\n\n\n\nCode\n# find p value\np_value = norm.sf(z_score_ratio3)*2\nprint(f\"p_value is {round(p_value, 3)}\")\n\n\np_value is 0.005\n\n\nZ-Value: A z-value of 2.793 indicates that the effect of being in the 3:1 match ratio group is nearly 2.8 standard deviations away from the mean effect of being in the control group. This shows a strong positive impact of the 3:1 matching offer on the likelihood of making a donation.\nP-Value: The p-value of 0.0052 clearly falls below the typical significance threshold of 0.05, affirming that this result is statistically significant. This strongly suggests rejecting the null hypothesis, or there is significant difference in donation likelihood between the 3:1 match ratio and the control groups.\nAbout the figures suggest\n\nSupport the statement: If the match thresholds for different ratios were relatively low and thus easilu attainable, and the data still shows a significant different between higher ratios and the control, it supports the notion that increasing the match ratio is an effectve strategy. It suggests that the actual ratio, rather than the ease of achieving a match, motivates donors\nContradiction of the statement: The significant effects effects observed at higher match ratios contradict any implication the match ratios do not influence donor behavior. It shows that higher ratios can indeed motivate more donations\n\n\n\n\nRegression on ratio1, ratio2, ratio3\nFirst of all, create a new ratio1 variable\n\n\nCode\ndata['ratio1'] = rsm.ifelse(data.ratio == 1, 1, 0)\n\n\n\nLinear regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'gave',\n    evar = ['ratio1', 'ratio2', 'ratio3']\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\nratio1\n0.003\n0.002\n1.661\n0.097\n.\n\n\n2\nratio2\n0.005\n0.002\n2.744\n0.006\n**\n\n\n3\nratio3\n0.005\n0.002\n2.802\n0.005\n**\n\n\n\n\n\n\n\n\nThe linear regression results are as follows:\nratio1 represent 1:1 matching:\n\nCoefficient: 0.003: suggesting that ratio increases the probability of making a donation by 0.3% points\np_value = 0.097\n\nAt the 95% confidence level, the coefficient for 1:1 match ratio is not statistically significant because p_value is greater than 0.05, which means we are less confident that 1:1 match ratio has a real effect on the donation probability in the population\nratio2 represent 2:1 matching:\n\nCoefficient: 0.005, indicates a 0.5% point increase in the probability of making a donation\nP-value: 0.006\n\nThe coefficient for the 2:1 match ratio is statistically significant at the 95% confidence level, as the p-value is well below 0.05, which means we can quite confident that the 2:1 match ratio has a positive effect on the probability of donating\nratio3 represent 3:1 matching:\n\nCoefficient: 0.005, also indicates a 0.5% point increase in the probability of making a donation\nP-value: 0.005\n\nSimilarly, the coefficient for the 3:1 match ratio is statistically significant at the 95% confidence level.\nStatistical Precision\nThe ‚Äòstd.error‚Äô column shows the standard error of each coefficient, which is a measure of the precision of the coefficient estimate. Smaller standard errors indicate more precise estimates. In this table, the standard errors for the treatment levels are the same (0.002), suggesting similar levels of precision across these estimates.\nThe results indicate that only the 2:1 and 3:1 match ratios significantly increase the likelihood of donations compared to the control group at the 95% confidence level. The effects of these higher match ratios are robust, suggesting that they are effective strategies for increasing donation rates.\nThe 1:1 match ratio, while showing a positive effect, does not reach the threshold for statistical significance at this confidence level. This implies that while there might be a slight increase in donation likelihood with a 1:1 match, the evidence is not strong enough to conclusively state this at the 95% confidence level.\n\n\nResponse rate difference\n\n\nCode\nrr_1_1 = data[data['ratio1'] == 1]['gave'].mean()\nrr_2_1 = data[data['ratio2'] == 1]['gave'].mean()\nrr_3_1 = data[data['ratio3'] == 1]['gave'].mean()\n\nrr_diff_11_vs_21 = rr_2_1 - rr_1_1\nrr_diff_21_vs_31 = rr_3_1 - rr_2_1\n\nprint(f'The response rate difference between the 1:1 and 2:1 match ratios {round(rr_diff_11_vs_21, 3)}')\nprint(f'The response rate difference between the 2:1 and 3:1 match ratios {round(rr_diff_21_vs_31, 3)}')\n\n\nThe response rate difference between the 1:1 and 2:1 match ratios 0.002\nThe response rate difference between the 2:1 and 3:1 match ratios 0.0\n\n\nRecall the linear regression result above:\n\n\nCode\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.018\n0.001\n16.225\n0.000\n***\n\n\n1\nratio1\n0.003\n0.002\n1.661\n0.097\n.\n\n\n2\nratio2\n0.005\n0.002\n2.744\n0.006\n**\n\n\n3\nratio3\n0.005\n0.002\n2.802\n0.005\n**\n\n\n\n\n\n\n\n\n\n\nCode\ncoef_ratio1 = 0.003\ncoef_ratio2 = 0.005\ncoef_ratio3 = 0.005\n\ncoef_11_vs_21 = coef_ratio2 - coef_ratio1\ncoef_21_vs_31 = coef_ratio3 - coef_ratio2\n\nprint(f'The response rate difference between the 1:1 and 2:1 match ratios from regression is {round(coef_11_vs_21, 3)}')\nprint(f'The response rate difference between the 2:1 and 3:1 match ratios from regression is {round(coef_21_vs_31, 3)}')\n\n\nThe response rate difference between the 1:1 and 2:1 match ratios from regression is 0.002\nThe response rate difference between the 2:1 and 3:1 match ratios from regression is 0.0\n\n\n1:1 vs 2:1 : The very small difference (0.002) suggests only a marginal improvement in the response rate from the 1:1 and 2:1 match ratio. This indicates that while the 2:1 match ratio might be slightly more effective than 1:1, the difference is minimal\n2:1 vs 3:1 The zero difference suggest that there is no additional benefit in increasing the match ratio from 2:1 to 3:1 regarding the likelihood of donations. This suggests diminishing returns when the match ratio increase beyond 2:1\nThese findings imply that while increasing the match ratio from 1:1 to 2:1 might offer a slight increase in donation likelihood, increasing it further to 3:1 does not yield additional benefits. This could be crucial for organizations in deciding how aggressively to pursue higher matching ratios in their fundraising strategies. The minimal differences suggest that other factors may be more critical in influencing donation behavior than merely adjusting the match ratio."
  },
  {
    "objectID": "projects/replicate/index.html#size-of-charitable-contribution",
    "href": "projects/replicate/index.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\n\\[H_0: \\mu_{amount\\;treatment} = \\mu_{amount\\;control} \\] \\[H_a: \\mu_{amount\\;treatment} \\neq \\mu_{amount\\;control}  \\]\n\n\nCode\namount_treatment = data.loc[data['treatment'] == 1,'amount']\namount_control = data.loc[data['treatment'] == 0,'amount']\n\n# Apply the t_values function that we defined above\nt_value_amount = t_value(amount_treatment, amount_control)\nprint(f\"T_value is {t_value_amount}\")\n\n\nT_value is 1.9182617883567805\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_amount = dof(amount_treatment, amount_control)\nprint(f\"Degree of freedom is {dof_amount}\")\n\n\nDegree of freedom is 36216.06015374612\n\n\n\n\nCode\n# Find p-value\np_value = (1-t.cdf(t_value_amount, dof_amount))*2\nprint(f\"P_value is {p_value}\")\n\n\nP_value is 0.055085678607487365\n\n\nThe t-test results show a t-statistic of approximately 1.918 with a p-value of 0.055. This p-value is slightly above the conventional threshold of 0.05, indicating that the difference in average donation amounts between the treatment and control groups is not statistically significant at the 5% level. However, the p-value is close to the threshold, suggesting a potential trend where the treatment group might have higher donation amounts than the control group, though this difference isn‚Äôt strong enough to be considered statistically significant.\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data,\n    rvar = 'amount',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n0.813\n0.067\n12.063\n0.000\n***\n\n\n1\ntreatment\n0.154\n0.083\n1.861\n0.063\n.\n\n\n\n\n\n\n\n\nLinear Regression Results:\nIntercept: 0.8133\nThis suggests that the average donation amount for the control group is approximately 0.813 units.\nCoefficient for Treatment: 0.154\nThis indicates that being in the treatment group is associated with an increase in the donation amount by approximately 0.154 units compared to the control group.\nP-Value for Treatment: 0.063\nThe p-value is slightly above the conventional threshold of 0.05, indicating that the effect of treatment on donation amount is not statistically significant at the 5% level. However, it‚Äôs close, suggesting a potential positive impact of the treatment on donation amounts.\nOverall Interpretation: Both the t-test and the linear regression suggest that the treatment might have a slight positive effect on donation amounts, but this effect does not reach statistical significance. This could imply that while the treatment potentially influences donations positively, the effect is not large or consistent enough across the sample to conclude definitively about its effectiveness. It‚Äôs also possible that other variables not considered here could be influencing the donations, and including those in the model could potentially change these results.\n\n\nLimit only those who made a donation:\n\n\nCode\ndata_donated = data[data['amount'] &gt; 0]\n\n\n\nT-Test\n\n\nCode\ndonated_treatment = data_donated.loc[data_donated['treatment'] == 1,'amount']\ndonated_control = data_donated.loc[data_donated['treatment'] == 0,'amount']\n\n# Apply the t_values function that we defined above\nt_value_donated = t_value(donated_treatment, donated_control)\nprint(f\"T_value is {t_value_donated}\")\n\n\nT_value is -0.5846089783693985\n\n\n\n\nCode\n#Apply the degree of freedom funcion that we defined above\ndof_donated = dof(donated_treatment, donated_control)\nprint(f\"Degree of freedom is {dof_donated}\")\n\n\nDegree of freedom is 557.4599283248195\n\n\n\n\nCode\n# Find p-value\np_value = t.cdf(t_value_donated, dof_donated)*2\nprint(f\"P_value is {p_value}\")\n\n\nP_value is 0.5590471873269819\n\n\nThe t-statistic of -0.5846 suggests a lower average donation amount in the treatment group compared to the control group among donors, though the difference is minor.\nThe p-value of 0.559 indicates that this difference is not statistically significant. This means we do not have sufficient evidence to conclude that the treatment affects donation amounts among those who donate.\n\n\nLinear Regression\n\n\nCode\nlr = rsm.model.regress(\n    data = data_donated,\n    rvar = 'amount',\n    evar = 'treatment'\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n45.540\n2.423\n18.792\n0.000\n***\n\n\n1\ntreatment\n-1.668\n2.872\n-0.581\n0.561\n\n\n\n\n\n\n\n\n\nTreatment Coefficient: The coefficient for the treatment group is approximately -1.67 with a standard error of about 2.87. This suggests that, on average, the treatment group donated about 1.67 units less than the control group among those who donated.\nStatistical Significance: P-value for the Treatment: The p-value for the treatment effect is 0.561, which is not statistically significant. This implies that there is no strong evidence to suggest that the treatment effect differs from zero in the population of donors.\nThe negatice coefficient for the treatment group suggests that the treatment potentially lead to a decrease in the amount donated compared to the control group among those who donated. However, the lack of statistical significance (p-value &gt; 0.05) means we can not confidently assert that this treatment effect is different from zero in the broader donor population.\nCausal Interpretation:\nWhether the treatment coefficient can be interpreted causally depends on how the treatment was assigned. If the treatment was randomly assigned to participants, then the coefficient could potentially have a causal interpretation. Random assignment would help control for both observed and unobserved confounding variables, allowing us to attribute differences in outcomes directly to the treatment effect.\nWhat Did We Learn?\nThe analysis indicates that among those who chose to donate, the treatment did not significantly increase donation amounts. In fact, the point estimate suggests a slight decrease in donations, but this result is not statistically significant. This finding helps understand the treatment‚Äôs impact specifically on the subset of the population that decides to donate, complementing the broader analysis which includes non-donors.\n\n\nPlots\n\n\nCode\n# Prepare the plots for treatment and control groups who made donations\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\n# Plot for Treatment Group\naxes[0].hist(donated_treatment, bins=30, color='blue', alpha=0.7)\naxes[0].axvline(donated_treatment.mean(), color='red', linestyle='dashed', linewidth=1)\naxes[0].set_title('Treatment Group Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(donated_treatment.mean(), max(axes[0].get_ylim()) * 0.5, f'Average: {donated_treatment.mean():.2f}', color='red')\n\n# Plot for Control Group\naxes[1].hist(donated_control, bins=30, color='green', alpha=0.7)\naxes[1].axvline(donated_control.mean(), color='red', linestyle='dashed', linewidth=1)\naxes[1].set_title('Control Group Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].set_ylabel('Frequency')\naxes[1].text(donated_control.mean(), max(axes[1].get_ylim()) * 0.5, f'Average: {donated_control.mean():.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTreatment Group Donation Amounts:\nThe majority of donations are concentrated in the lower range of amounts, with the frequency decreasing as the amount increases.\nThere‚Äôs a significant frequency at the lowest amount, indicating that many donations are of a small value.\nThe average donation amount in the treatment group is indicated by a vertical dotted red line and is annotated as $43.87.\nControl Group Donation Amounts:\nSimilar to the treatment group, most donations are of a lower amount, with frequency tapering off for higher donation amounts.\nThe distribution of donations appears slightly more spread out than the treatment group, with some higher amounts being more frequent compared to the treatment group.\nThe average donation amount for the control group is slightly higher than the treatment group, marked by a vertical dotted red line, at $45.54.\nInterpretation:\n\nThe control group has a slightly higher average donation amount compared to the treatment group.\nBoth histograms appear right-skewed, which is common in financial data since a large number of small donations are often accompanied by a smaller number of much larger donations"
  },
  {
    "objectID": "projects/replicate/index.html#simulation-experiment",
    "href": "projects/replicate/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nLaw of Large Numbers\n\n\nCode\n# Define probabilities for Bernoulli distributions\np_control = 0.018  # Probability for control group\np_treatment = 0.022  # Probability for treatment group\n\n# Simulate 100,000 draws for the control group\ncontrol_samples = np.random.binomial(1, p_control, 100000)\n\n# Simulate 10,000 draws for the treatment group\ntreatment_samples = np.random.binomial(1, p_treatment, 10000)\n\n# Calculate differences for each of the first 10,000 elements in the control sample (to match treatment sample size)\ndifferences = treatment_samples - control_samples[:10000]\n\n# Calculate cumulative averages of differences\ncumulative_averages = np.cumsum(differences) / (np.arange(10000) + 1)\n\n# Plot the cumulative averages of the differences\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_averages, label='Cumulative Average of Differences', color='blue')\nplt.axhline(y=(p_treatment - p_control), color='red', linestyle='dashed', label='True Difference (0.022 - 0.018)')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average of Difference')\nplt.title('Cumulative Average of Differences Between Treatment and Control')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nExplaination of the plot The plot above illustrates the cumulative average of differences in donation behavior between treatment group (with a charitable donation match) and the control group (without a match), based on simulated data.\n\nBlue line: This represents the cumulative average of the difference between the donation outcomes of indiividual in the treatmetn and control groups across 10,000 simulated trial. The differences are calculated for each trial as the outcome of the treatment minus the outcome of the control.\nRed dashed line: This line marks the true difference in means between the probabilities of making a donation in the treatment and control groups, which is p_treatment - p_control = 0.022 - 0.018 = 0.004.\n\nObservations from the plot\nThe blue line, or the cumulative average of differences, fluctuates initially but starts to stabilize and approach the red dashed line as the number of trials increases. This behavior exemplifies the Law of Large Numbers, which states that as the number of trials increases, the sample average will converge to the expected value (in this case, the true difference in means).\nThe plot confirms that with a sufficient number of trials, the cumulative average of the differences in donation behavior between the treatment and control groups approaches the true difference in means. This simulation reinforces our understanding of statistical concepts like the Law of Large Numbers and provides a visual affirmation that with enough data, our estimates can reliably approximate true population parameters. This also supports the validity of using such statistical methods to evaluate the effects of interventions like charitable donation matches\n\n\nCentral Limit Theorem\n\n\nCode\n# Define sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5), sharey=True)\n\n# Simulate the process and plot the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate drawing samples and calculating the means 1000 times\n    sample_means = np.array([np.mean(np.random.binomial(1, p_treatment, sample_size) - \n                                     np.random.binomial(1, p_control, sample_size)) \n                             for _ in range(1000)])\n    \n    # Plot the histogram\n    axes[i].hist(sample_means, bins=30, orientation='horizontal', color='blue', alpha=0.6, edgecolor='black')\n    \n    # Calculate the mean and standard deviation for the normal distribution curve\n    mean_of_sample_means = np.mean(sample_means)\n    std_dev_of_sample_means = np.std(sample_means)\n\n    # Generate values for the normal distribution curve\n    curve_x = np.linspace(mean_of_sample_means - 3 * std_dev_of_sample_means, \n                          mean_of_sample_means + 3 * std_dev_of_sample_means, 100)\n    curve_y = (1 / (std_dev_of_sample_means * np.sqrt(2 * np.pi)) *\n               np.exp(-(curve_x - mean_of_sample_means) ** 2 / (2 * std_dev_of_sample_means ** 2)))\n    \n    # Scale the curve y to match the histogram scale\n    curve_y_scaled = curve_y * max(np.histogram(sample_means, bins=30)[0]) / max(curve_y)\n    \n    # Draw the normal distribution curve as a red line\n    axes[i].plot(curve_y_scaled, curve_x, '-')\n\n    # Add a red dashed line at the true difference\n    axes[i].axhline(y=0.004, color='red', linestyle='dashed', linewidth=2)\n\n    # Set titles and labels\n    axes[i].set_title(f'Sample Size {sample_size}')\n    axes[i].set_xlabel('Frequency' if i == len(sample_sizes) - 1 else '')  # Only add xlabel to the last subplot\n    axes[i].set_ylabel('Average Difference' if i == 0 else '')  # Only add ylabel to the first subplot\n\n# Adjust layout for better fit\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see:\nSample Size 50: There is considerable variability around the true difference, indicating that smaller sample sizes can produce estimates that fluctuate widely.\nSample Size 200: The histogram becomes tighter around the true difference, with less variability than the sample size of 50.\nSample Size 500: Further reduction in variability is observed, and the distribution of averages is centered closely around the true difference.\nSample Size 1000: This histogram shows the least variability, and the average differences are clustering tightly around the true difference, with the center of the distribution aligning closely with the red line.\nLaw of Large Numbers: As the sample size increases, the cumulative average difference becomes more consistent and stable around the true difference. This is a demonstration of the Law of Large Numbers‚Äîthe sample averages converge to the expected value as the sample size grows.\nCentral Limit Theorem: As the sample size increases, the distribution of sample means (average differences in this case) becomes more symmetrical and bell-shaped, which is evidence of the Central Limit Theorem in action. The sample means for larger sample sizes tend to form a normal distribution centered around the true population mean.\nZero in the Distribution: In all histograms, the zero mark is not in the middle of the distribution; it‚Äôs in the left tail. This is because the treatment group has a higher probability of donating than the control group (0.022 vs.¬†0.018), so the true difference is expected to be positive (0.004), not zero."
  },
  {
    "objectID": "projects/intuit/data/intuit75k_description.html",
    "href": "projects/intuit/data/intuit75k_description.html",
    "title": "Website",
    "section": "",
    "text": "The purpose of this exercise is to gain experience modeling the response to an upsell campaign. The intuit75k.parquet file contains data on 75,000 (small) businesses that were selected randomly from the 801,821 that were sent the wave-1 mailing. The mailing contained an offer to upgrade to the latest version of the Quickbooks software.\nVariable res1 denotes which of these businesses responded to the mailing by purchasing Quickbooks version 3.0 from Intuit Direct. Note that Intuit Direct sells products directly to its customers rather than through a retailer. Use the available data to predict which businesses that did not respond to the wave-1 mailing, are most likely to respond to the wave-2 mailing. Note that variables were added, deleted, and recoded so please ignore the variable descriptions in Exhibit 3 in the case in the course reader. Instead, use the variable descriptions below:"
  },
  {
    "objectID": "projects/intuit/data/intuit75k_description.html#intuit-quickbooks-upgrade",
    "href": "projects/intuit/data/intuit75k_description.html#intuit-quickbooks-upgrade",
    "title": "Website",
    "section": "",
    "text": "The purpose of this exercise is to gain experience modeling the response to an upsell campaign. The intuit75k.parquet file contains data on 75,000 (small) businesses that were selected randomly from the 801,821 that were sent the wave-1 mailing. The mailing contained an offer to upgrade to the latest version of the Quickbooks software.\nVariable res1 denotes which of these businesses responded to the mailing by purchasing Quickbooks version 3.0 from Intuit Direct. Note that Intuit Direct sells products directly to its customers rather than through a retailer. Use the available data to predict which businesses that did not respond to the wave-1 mailing, are most likely to respond to the wave-2 mailing. Note that variables were added, deleted, and recoded so please ignore the variable descriptions in Exhibit 3 in the case in the course reader. Instead, use the variable descriptions below:"
  },
  {
    "objectID": "projects/intuit/data/intuit75k_description.html#variable-description",
    "href": "projects/intuit/data/intuit75k_description.html#variable-description",
    "title": "Website",
    "section": "Variable description",
    "text": "Variable description\n\nid: Small business customer ID\nzip5: 5-Digit ZIP Code (00000=unknown, 99999=international ZIPs).\nzip_bins: Zip-code bins (20 approx. equal sized bins from lowest to highest zip code number)\nsex: Gender Identity ‚ÄúFemale‚Äù, ‚ÄúMale‚Äù, or ‚ÄúUnknown‚Äù\nbizflag: Business Flag. Address contains a Business name (1 = yes, 0 = no or unknown).\nnumords: Number of orders from Intuit Direct in the previous 36 months\ndollars: Total $ ordered from Intuit Direct in the previous 36 months\nlast: Time (in months) since last order from Intuit Direct in previous 36 months\nsincepurch: Time (in months) since original (not upgrade) purchase of Quickbooks\nversion1: Is 1 if customer‚Äôs current Quickbooks is version 1, 0 if version 2\nowntaxprod: Is 1 if customer purchased tax software, 0 otherwise\nupgraded: Is 1 if customer upgraded from Quickbooks vs.¬†1 to vs.¬†2\nres1: Response to wave 1 mailing (‚ÄúYes‚Äù if responded else ‚ÄúNo‚Äù)\ntraining: 70/30 split, 1 for training sample, 0 for validation sample"
  },
  {
    "objectID": "projects/conjoint/index.html",
    "href": "projects/conjoint/index.html",
    "title": "Conjoint",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/conjoint/index.html#estimating-yogurt-preferences",
    "href": "projects/conjoint/index.html#estimating-yogurt-preferences",
    "title": "Conjoint",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\nData Overview\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nanonymized consumer identifiers.\n\n\ny1, y2, y3, y4\na vector indicating the chosen product.\n\n\nf1, f2, f3, f4\na vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising\n\n\np1, p2, p3, p4\nthe products‚Äô prices\n\n\n\n\n\n\n\n\nCode\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head()\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\n\n\nCode\nyogurt_data.describe(include='all')\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\ncount\n2430.0000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n\n\nmean\n1215.5000\n0.341975\n0.401235\n0.029218\n0.227572\n0.055556\n0.039506\n0.037449\n0.037449\n0.106248\n0.081532\n0.053622\n0.079507\n\n\nstd\n701.6249\n0.474469\n0.490249\n0.168452\n0.419351\n0.229109\n0.194836\n0.189897\n0.189897\n0.020587\n0.011047\n0.008054\n0.007714\n\n\nmin\n1.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-0.012000\n0.000000\n0.025000\n0.004000\n\n\n25%\n608.2500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.103000\n0.081000\n0.050000\n0.079000\n\n\n50%\n1215.5000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108000\n0.086000\n0.054000\n0.079000\n\n\n75%\n1822.7500\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.115000\n0.086000\n0.061000\n0.086000\n\n\nmax\n2430.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.193000\n0.111000\n0.086000\n0.104000\n\n\n\n\n\n\n\n\nStatistics Summary:\n\nThere are 2,430 records.\nBinary fields (y1, y2, y3, y4, f1, f2, f3, f4) indicate varying levels of frequency with which different yogurts were chosen or conditions were met.\nPrice or index fields (p1, p2, p3, p4) show distributions with differing means, minima, and maxima, suggesting variability in yogurt pricing or attributes across the samples.\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we‚Äôll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts‚Äô prices:\n\\[\nx_j' = \\left[ \\mathbb{1}(\\text{Yogurt 1}), \\mathbb{1}(\\text{Yogurt 2}), \\mathbb{1}(\\text{Yogurt 3}), X_f, X_p \\right]\n\\]\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a ‚Äúwide‚Äù shape with \\(n\\) rows and multiple columns for each covariate, to a ‚Äúlong‚Äù shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we‚Äôll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be ‚Äúpivoted‚Äù or ‚Äúmelted‚Äù from wide to long.\n\nReshape and prep the data\n\n\nCode\n# Melt the data into a long format\nlong_data = pd.melt(yogurt_data, id_vars=['id'], \n                    value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],\n                    var_name='product_feature', value_name='value')\n\n# Extract product and feature types from the 'product_feature' column\nlong_data['product'] = long_data['product_feature'].str.extract('(\\d)').astype(int)\nlong_data['feature'] = long_data['product_feature'].str.extract('([a-z]+)')\n\n# Pivot the table to get one row per consumer per product\nreshaped_yogurt = long_data.pivot_table(index=['id', 'product'], columns='feature', values='value', aggfunc='first').reset_index()\n\n# Add the binary indicators for the first three yogurts\nfor j in range(1, 4):\n    reshaped_yogurt[f'Yogurt{j}'] = (reshaped_yogurt['product'] == j).astype(int)\n\n# Ensure the resulting DataFrame is correctly structured\nreshaped_yogurt\n\n\n\n\n\n\n\n\n\nfeature\nid\nproduct\nf\np\ny\nYogurt1\nYogurt2\nYogurt3\n\n\n\n\n0\n1\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n1\n1\n2\n0.0\n0.081\n0.0\n0\n1\n0\n\n\n2\n1\n3\n0.0\n0.061\n0.0\n0\n0\n1\n\n\n3\n1\n4\n0.0\n0.079\n1.0\n0\n0\n0\n\n\n4\n2\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9715\n2429\n4\n0.0\n0.086\n1.0\n0\n0\n0\n\n\n9716\n2430\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n9717\n2430\n2\n0.0\n0.086\n0.0\n0\n1\n0\n\n\n9718\n2430\n3\n0.0\n0.043\n0.0\n0\n0\n1\n\n\n9719\n2430\n4\n0.0\n0.079\n1.0\n0\n0\n0\n\n\n\n\n9720 rows √ó 8 columns\n\n\n\n\n\n\n\n\nEstimation\n\nCode up the log-likelihood function.\n\n\nCode\ndef log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return np.sum(log_likelihood)\n\n\nUse optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)).\n\n\nCode\ndef negative_log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return -np.sum(log_likelihood)\n\n# Prepare the input matrix X and the choice vector\nn_products = 4  # There are 4 products\nfeatures = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'f', 'p']\nX = reshaped_yogurt[features].values\nchoices = reshaped_yogurt['y'].values\n\n# Define initial guesses for the parameters\ninitial_beta = np.zeros(len(features))\n\n# Rerun the optimization with numpy properly imported and initial_beta defined\nresult = minimize(negative_log_likelihood, initial_beta, args=(X, choices))\nCoef = result.x\n\n\nCoef_table = pd.DataFrame({\n    'Variables': features,\n    'Coeficient': Coef\n})\n\nCoef_table\n\n\n\n\n\n\n\n\n\n\nVariables\nCoeficient\n\n\n\n\n0\nYogurt1\n1.387751\n\n\n1\nYogurt2\n0.643505\n\n\n2\nYogurt3\n-3.086113\n\n\n3\nf\n0.487415\n\n\n4\np\n-37.057828\n\n\n\n\n\n\n\n\n\n\nCode\n# unique_choices = reshaped_yogurt['product'].nunique()\n\n\n# X = reshaped_yogurt[features]\n# X = sm.add_constant(reshaped_yogurt[features])\n# choices = reshaped_yogurt['y']\n\n\n\n# mnl_model = sm.MNLogit(choices, X).fit()\n# mnl_summary = mnl_model.summary()\n# mnl_results_table = mnl_summary.tables[1]\n\n# # To display or print out the table\n# print(mnl_results_table)\n\n\n\n\n\nDiscussion\nThe estimated parameters for the three yogurt product intercepts are:\n\\(\\beta_1\\) = 1.39\n\\(\\beta_2\\) = 0.64\n\\(\\beta_3\\) = - 3.09\nThese coefficients represent the intrinsic utilities (or preferences) of the three yogurt products when all other variables (such as price and whether the product was featured) are held constant. Here‚Äôs how to interpret these intercepts in the context of consumer preferences:\n\\(\\beta_1\\) (Yogurt 1): The positive and highest value among the three suggests that Yogurt 1 is the most preferred when no other attributes (like price or features) are considered. It has the highest intrinsic utility.\n\\(\\beta_2\\) (Yogurt 2): This is also positive but lower than \\(\\beta_1\\) , indicating that Yogurt 2 is less preferred than Yogurt 1 but still has a positive intrinsic appeal compared to a baseline (which could be another product not included in these three, like Yogurt 4 in this analysis).\n\\(\\beta_3\\) (Yogurt 3): The negative value here suggests that Yogurt 3 is least preferred among the three, having a lower intrinsic utility relative to the others.\nGiven these interpretations, Yogurt 1 appears to be the most preferred option among the first three, followed by Yogurt 2, with Yogurt 3 being the least preferred under the assumption that other factors are equal. This intrinsic preference could be driven by factors not explicitly modeled but captured by the intercepts, such as brand affinity, flavor preferences, or other unobserved attributes associated with each product.\nUse the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\n\nCode\n# Extracted beta values for Yogurt 1 and Yogurt 3 and the price coefficient\nbeta_1 = 1.39\nbeta_3 = -3.09\nbeta_p = -37.06  # The negative price coefficient\n\n# Calculate utility difference\nutility_difference = beta_1 - beta_3\n\n# Convert utility difference to dollar benefit using the price coefficient\ndollar_benefit = utility_difference / abs(beta_p)\n\nprint(\"Per-unit monetary measure of brand value is \", round(dollar_benefit, 4))\n\n\nPer-unit monetary measure of brand value is  0.1209\n\n\nThe per-unit monetary measure of brand value between the most-preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is approximately $0.12 per unit. This means that, in terms of brand value, consumers might be willing to pay an extra 12 cents per unit for Yogurt 1 compared to Yogurt 3, based solely on their preference (utility difference) as captured by the model. This is a useful way to quantify the monetary value of consumer preferences in this context\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nCalculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities). Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?\n\n\nCode\n#reload the function cuz the disconnecting between tasks\ndef negative_log_likelihood(beta, X, choices):\n    # Utility calculation\n    utility = X.dot(beta)\n    # Exponentiated utilities\n    exp_util = np.exp(utility)\n    # Sum of exponentiated utilities across choices\n    sum_exp_util = np.sum(exp_util.reshape(-1, 4), axis=1)\n    # Compute choice probabilities\n    probabilities = exp_util / np.repeat(sum_exp_util, 4)\n    # Log of probabilities of chosen alternatives\n    log_likelihood = np.log(probabilities) * choices\n\n    return -np.sum(log_likelihood)\n\n# Prepare the input matrix X and the choice vector\nn_products = 4  # There are 4 products\nfeatures = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'f', 'p']\nX = reshaped_yogurt[features].values\nchoices = reshaped_yogurt['y'].values\n\n# Define initial guesses for the parameters\ninitial_beta = np.zeros(len(features))\n\n# Rerun the optimization with numpy properly imported and initial_beta defined\nresult = minimize(negative_log_likelihood, initial_beta, args=(X, choices))\n\n\n\n\nCode\ndef calculate_probabilities(beta, X):\n    utility = X.dot(beta)\n    exp_util = np.exp(utility)\n    sum_exp_util = np.sum(exp_util.reshape(-1, n_products), axis=1)\n    probabilities = exp_util / np.repeat(sum_exp_util, n_products)\n    return probabilities.reshape(-1, n_products)\n\nestimated_beta = result.x\n# Calculate the initial choice probabilities for all products and all consumers\ninitial_probabilities = calculate_probabilities(estimated_beta, X)\n\n# Calculate the current market shares by taking the mean of probabilities across all consumers for each product\ncurrent_market_shares = np.mean(initial_probabilities, axis=0)\n\n# Display the new market shares\ncurrent_market_shares_df = pd.DataFrame({\n    'Product': ['Yogurt 1', 'Yogurt 2', 'Yogurt 3', 'Yogurt 4'],\n    'Current Market Share': current_market_shares\n})\n\ncurrent_market_shares_df\n\n\n\n\n\n\n\n\n\n\nProduct\nCurrent Market Share\n\n\n\n\n0\nYogurt 1\n0.341975\n\n\n1\nYogurt 2\n0.401235\n\n\n2\nYogurt 3\n0.029218\n\n\n3\nYogurt 4\n0.227572\n\n\n\n\n\n\n\n\nThe current market shares for the four yogurt products are approximately:\nYogurt 1: 34.2%\nYogurt 2: 40.1%\nYogurt 3: 2.9%\nYogurt 4: 22.8%\nNext, let‚Äôs increase the price of Yogurt 1 by $0.10 and then use the fitted model to predict the new choice probabilities. We‚Äôll see how the market shares change, particularly for Yogurt 1, as a result of this price increase.\n\n\nCode\n# Increase the price of Yogurt 1 by $0.10\n# First, create a new X matrix with the updated price for Yogurt 1\nX_new_prices = X.copy()\nprice_increase = 0.10\nX_new_prices[:, 4][X_new_prices[:, 0] == 1] += price_increase  # Only increase the price in the entries for Yogurt 1\n\n# Calculate the new choice probabilities with the increased price of Yogurt 1\nnew_probabilities = calculate_probabilities(estimated_beta, X_new_prices)\n\n# Calculate the new market shares by taking the mean of new probabilities across all consumers for each product\nnew_market_shares = np.mean(new_probabilities, axis=0)\n\n# Display the new market shares\nnew_market_shares_df = pd.DataFrame({\n    'Product': ['Yogurt 1', 'Yogurt 2', 'Yogurt 3', 'Yogurt 4'],\n    'New Market Share': new_market_shares\n})\n\nnew_market_shares_df\n\n\n\n\n\n\n\n\n\n\nProduct\nNew Market Share\n\n\n\n\n0\nYogurt 1\n0.021118\n\n\n1\nYogurt 2\n0.591145\n\n\n2\nYogurt 3\n0.044040\n\n\n3\nYogurt 4\n0.343697\n\n\n\n\n\n\n\n\nThe new market shares for the four yogurt products after increasing the price of Yogurt 1 by $0.10 are approximately:\nYogurt 1: 2.1%\nYogurt 2: 59.1%\nYogurt 3: 4.4%\nYogurt 4: 34.4%\nYogurt 1‚Äôs market share dramatically decreases from 34.2% to 2.1% due to the price increase.\nYogurt 2‚Äôs market share significantly increases, absorbing most of the share lost by Yogurt 1.\nYogurt 3 and Yogurt 4 also see some increase in their market shares. This demonstrates the sensitivity of market share to price changes in competitive markets, especially under the assumption of a Multinomial Logit model where the relative utilities directly affect the choice probabilities. Yogurt 1‚Äôs substantial price increase leads consumers to switch to the more affordable alternatives, illustrating the impact of price elasticity on consumer choice behavior"
  },
  {
    "objectID": "projects/conjoint/index.html#estimating-minivan-preferences",
    "href": "projects/conjoint/index.html#estimating-minivan-preferences",
    "title": "Conjoint",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\nLoad data and Describe the data\n\n\nCode\nminivan = pd.read_csv(\"rintro-chapter13conjoint.csv\")\nminivan\n\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8995\n200\n14\n2\nno\n7\n3ft\ngas\n35\n1\n\n\n8996\n200\n14\n3\nno\n7\n3ft\nhyb\n35\n0\n\n\n8997\n200\n15\n1\nno\n7\n2ft\ngas\n35\n0\n\n\n8998\n200\n15\n2\nno\n8\n3ft\nelec\n40\n0\n\n\n8999\n200\n15\n3\nno\n6\n3ft\ngas\n35\n1\n\n\n\n\n9000 rows √ó 9 columns\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nresp.id\nIdentifier for the respondent.\n\n\nques\nQuestion or choice task number.\n\n\nalt\nAlternative within each choice task.\n\n\ncarpool, seat, cargo, eng\nAttributes of each alternative, such as carpool availability, seating capacity, cargo space, and engine type.\n\n\nprice\nPrice associated with each alternative.\n\n\nchoice\nWhether the alternative was chosen (1) or not (0).\n\n\n\n\n\n\n\n\nCode\n# Number of respondents\nnum_respondents = minivan['resp.id'].nunique()\n\n# Number of choice tasks completed by each respondent\ntasks_per_respondent = minivan.groupby('resp.id')['ques'].nunique()\n\n# Number of alternatives per choice task\n# Assuming the structure is consistent across the dataset\nalternatives_per_task = minivan.groupby(['resp.id', 'ques'])['alt'].nunique().max()\n\nprint(\"Number of Respondents:\", num_respondents)\nprint(tasks_per_respondent.describe())\nprint(\"Number of Alternatives per Choice Task:\",alternatives_per_task)\n\n\nNumber of Respondents: 200\ncount    200.0\nmean      15.0\nstd        0.0\nmin       15.0\n25%       15.0\n50%       15.0\n75%       15.0\nmax       15.0\nName: ques, dtype: float64\nNumber of Alternatives per Choice Task: 3\n\n\nHere‚Äôs a summary of the conjoint survey data:\n\nNumber of Respondents: There are 200 respondents who participated in the survey.\nNumber of Choice Tasks per Respondent: Each respondent completed 15 choice tasks. This number is consistent across all respondents.\nNumber of Alternatives per Choice Task: Each choice task presented 3 alternatives.\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\nEstimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine) and show the table of Coefficients and Standard Errors\n\n\nCode\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Create dummy variables for the categorical attributes, excluding base levels\nminivan['seat_7'] = (minivan['seat'] == 7).astype(int)\nminivan['seat_8'] = (minivan['seat'] == 8).astype(int)\nminivan['cargo_3ft'] = (minivan['cargo'] == '3ft').astype(int)\nminivan['eng_hybrid'] = (minivan['eng'] == 'hyb').astype(int)\nminivan['eng_electric'] = (minivan['eng'] == 'elec').astype(int)\nminivan = minivan.apply(pd.to_numeric, errors='coerce')\n\nX = minivan[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hybrid', 'eng_electric']]\nX = sm.add_constant(X)  # Add intercept\ny = minivan['choice']  # Make sure 'choice' is coded appropriately for multinomial\n\nmnl_model = sm.MNLogit(y, X).fit()\nmnl_summary = mnl_model.summary()\nmnl_results_table = mnl_summary.tables[1]\n\n# To display or print out the table\nprint(mnl_results_table)\n\n\nOptimization terminated successfully.\n         Current function value: 0.558663\n         Iterations 6\n================================================================================\n    choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            5.5322      0.224     24.677      0.000       5.093       5.972\nprice           -0.1591      0.006    -25.616      0.000      -0.171      -0.147\nseat_7          -0.5248      0.060     -8.800      0.000      -0.642      -0.408\nseat_8          -0.2931      0.059     -5.009      0.000      -0.408      -0.178\ncargo_3ft        0.4385      0.049      9.004      0.000       0.343       0.534\neng_hybrid      -0.7605      0.057    -13.361      0.000      -0.872      -0.649\neng_electric    -1.4347      0.062    -23.217      0.000      -1.556      -1.314\n================================================================================\n\n\n\n\nResults\nInterpretation:\nPrice: The negative coefficient (-0.1591) suggests that as the price increases by one thousand dollars, the log odds of choosing a particular car decrease, indicating a typical negative relationship between price and purchase probability.\nSeat 7: Having 7 seats, compared to the baseline of 6 seats, is associated with lower odds of the car being chosen.\nSeat 8: Similarly, having 8 seats is also less preferable compared to 6 seats but less so than 7 seats.\nCargo 3ft: More cargo space (3ft) increases the odds of choosing the car compared to the base level of 2ft. This feature is preferred over the baseline of 2ft cargo space, as indicated by the positive coefficient. Consumers prefer more cargo space, all else being equal.\nEngine Hybrid and Electric: Both hybrid and electric engines are less preferred compared to a traditional gas engine, with electric being the least preferred among the options.\nUse the price coefficient as a dollar-per-util conversion factor. We could find the dollar value of 3ft of cargo space as compared to 2ft of cargo space:\n\n\nCode\n # Coefficients from the model results\ncargo_coeff = 0.4385\nprice_coeff = -0.1591\n\n# Calculate the dollar value of having 3ft of cargo space compared to 2ft\ndollar_value_cargo = (cargo_coeff / price_coeff) * (-1)\nprint(\"The Dolla Value of having 3ft cargo space compared to 2ft:\", round(dollar_value_cargo, 3))\n\n\nThe Dolla Value of having 3ft cargo space compared to 2ft: 2.756\n\n\nThe dollar value of having 3 feet of cargo space compared to 2 feet, based on the model, is approximately $2,756. This amount represents the additional value that respondents place on having an extra foot of cargo space in their vehicle choice.\nAssume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n\nCode\n# Coefficients from the MNL model\ncoef_const = 5.5322\ncoef_price = -0.1591\ncoef_seat_7 = -0.5248\ncoef_seat_8 = -0.2931\ncoef_cargo_3ft = 0.4385\ncoef_eng_elec = -1.4347\ncoef_eng_hyb = -0.7605\n\n# Define the attributes of each minivan\nminivans = [\n    {\"seats\": 7, \"cargo\": 2, \"engine\": \"Hyb\", \"price\": 30},\n    {\"seats\": 6, \"cargo\": 2, \"engine\": \"Gas\", \"price\": 30},\n    {\"seats\": 8, \"cargo\": 2, \"engine\": \"Gas\", \"price\": 30},\n    {\"seats\": 7, \"cargo\": 3, \"engine\": \"Gas\", \"price\": 40},\n    {\"seats\": 6, \"cargo\": 2, \"engine\": \"Elec\", \"price\": 40},\n    {\"seats\": 7, \"cargo\": 2, \"engine\": \"Hyb\", \"price\": 35}\n]\n\n# Function to calculate utility\ndef calculate_utility(minivan):\n    utility = coef_const\n    utility += coef_price * minivan[\"price\"]\n    if minivan[\"seats\"] == 7:\n        utility += coef_seat_7\n    elif minivan[\"seats\"] == 8:\n        utility += coef_seat_8\n    if minivan[\"cargo\"] == 3:\n        utility += coef_cargo_3ft\n    if minivan[\"engine\"] == \"Elec\":\n        utility += coef_eng_elec\n    elif minivan[\"engine\"] == \"Hyb\":\n        utility += coef_eng_hyb\n    return utility\n\n# Calculate utilities for each minivan\nutilities = [calculate_utility(minivan) for minivan in minivans]\n\n# Calculate the market shares using the softmax function\nexp_utilities = np.exp(utilities)\nmarket_shares = exp_utilities / np.sum(exp_utilities)\n\n# Create a DataFrame for the results\nminivan_names = ['A', 'B', 'C', 'D', 'E', 'F']\nmarket_shares_df = pd.DataFrame({\n    'Minivan': minivan_names,\n    'Market Share': market_shares\n})\n\nmarket_shares_df\n\n\n\n\n\n\n\n\n\n\nMinivan\nMarket Share\n\n\n\n\n0\nA\n0.116071\n\n\n1\nB\n0.419684\n\n\n2\nC\n0.313062\n\n\n3\nD\n0.078430\n\n\n4\nE\n0.020365\n\n\n5\nF\n0.052389\n\n\n\n\n\n\n\n\nHigh Market Share\nMinivan B (41.97%): 6 seats, 2ft cargo, gas engine, $30. This minivan has the highest market share, suggesting that consumers highly value the combination of a lower price ($30) with standard features (6 seats, 2ft cargo, gas engine). This option appears to be the most cost-effective and appeals to the majority of consumers.\nMinivan C (31.31%): 8 seats, 2ft cargo, gas engine, $30. This minivan also has a significant market share, indicating that some consumers are willing to opt for a vehicle with more seating capacity at the same price. The additional seats (8 seats) add utility, making it an attractive option despite not being the top choice.\nModerate Market Share Minivan A (11.61%): 7 seats, 2ft cargo, hybrid engine, $30. The hybrid engine in this minivan reduces its market share compared to the purely gas-powered options at the same price point. While hybrid engines are generally valued for their efficiency, the preference in this case seems to be towards conventional gas engines at a lower price.\nLow Market Share Minivan F (5.24%): 7 seats, 2ft cargo, hybrid engine, $35. The increase in price to $35, coupled with similar features as Minivan A, significantly lowers its market share. This suggests that consumers are sensitive to price increases and are less inclined to pay an extra $5 for a similar hybrid vehicle with the same number of seats and cargo space.\nMinivan D (7.84%): 7 seats, 3ft cargo, gas engine, $40. Despite offering more cargo space (3ft), the higher price of $40 detracts from its attractiveness. This indicates that the additional cargo space does not compensate for the higher price for most consumers.\nMinivan E (2.04%): 6 seats, 2ft cargo, electric engine, $40. This minivan has the lowest market share, suggesting that consumers place a relatively low value on electric engines in this context, especially when paired with a high price. The cost does not justify the perceived benefits of the electric engine, resulting in minimal consumer interest.\nKey Takeaways:\nPrice Sensitivity: Consumers show a strong preference for lower-priced options. Minivans priced at $30 dominate the market shares, indicating high price sensitivity.\nEngine Type Preferences: Gas engines are favored over hybrid and electric engines, reflecting either cost concerns or possibly a lack of perceived additional value from alternative engine types at higher prices.\nFeature Trade-offs: Additional features like more seats or cargo space are valued but have a diminishing return when paired with higher prices. Consumers appear to balance their preferences for additional features with their willingness to pay more."
  },
  {
    "objectID": "projects/fraud/index.html",
    "href": "projects/fraud/index.html",
    "title": "Fraud Detection Report",
    "section": "",
    "text": "In this project, we developed a comprehensive fraud detection model using a methodical approach outlined in five primary stages: data preparation, feature engineering, feature selection, model evaluation and implementation. The culmination of this project in the deployment of an XGBoost-based fraud detection model represents a significant step forward in safeguarding our operations against fraudulent activities. The model‚Äôs performance, evidenced by substantial potential savings and a strategic 5% cutoff point, highlights its effectiveness in identifying high-risk transactions with precision.\nModel Methodology:\n\nBaseline Logistic Model\nDecision Tree\nRandom Forest\nLBGM\nCatboost\nNeural Network\nXGB\n\n\nDownload PDF file."
  },
  {
    "objectID": "projects/variable_importance/index.html",
    "href": "projects/variable_importance/index.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Key Driver Analysis (KDA) is a statistical technique used to determine the factors (or ‚Äúdrivers‚Äù) that most significantly impact a particular outcome or dependent variable. It is commonly used in fields like marketing, customer satisfaction, product development, and human resources to understand what influences key outcomes such as customer satisfaction, employee engagement, or product success.\nThis post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card. This involves calculating pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest."
  },
  {
    "objectID": "projects/poisson_reg/index.html",
    "href": "projects/poisson_reg/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\ndata = pd.read_csv('blueprinty.csv')\ndata = data.drop('Unnamed: 0', axis=1)\ndata\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows √ó 4 columns\n\n\n\n\nCompare histograms and means of number of patents by customer status:\n\n\nCode\nmean_patents = data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nThis indicate that, on average, customers have a slightly higher number of patents than non-customers. This might suggest that customer are more engaged or invest more patentable innovations, though other factors could also influence these resutls.\n\n\nCode\n# Set up the figure with two subplots for better comparison\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n# Plot histogram for non-customers\nsns.histplot(data[data['iscustomer'] == 0]['patents'], ax=axes[0], color='blue', bins=30, kde=True)\naxes[0].set_title('Patent Distribution for Non-Customers')\naxes[0].set_xlabel('Number of Patents')\naxes[0].set_ylabel('Frequency')\n\n# Plot histogram for customers\nsns.histplot(data[data['iscustomer'] == 1]['patents'], ax=axes[1], color='orange', bins=30, kde=True)\naxes[1].set_title('Patent Distribution for Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Frequency')\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBoth groups show a similar general shape in their distribution, but customers tend to have a slightly higher frequency at higher patent counts, which corroborates the earlier finding that customers on average have more patents.\nThe distribution for both groups is skewed towards lower numbers of patents, with most individuals holding fewer patents.\nThe skewness is slightly more pronounced for customers, which could indicate that while fewer customers might have patents, those who do are likely to have more of them.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\n# Visualize the age distribution across regions by customer status\nplt.figure(figsize=(10, 5))\n\n# Boxplot to show age distribution\nsns.boxplot(x='region', y='age', hue='iscustomer', data=data, palette=['blue', 'orange'])\n\n# Adjust plot labels and title\nplt.title('Age Distribution by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Age')\nplt.legend(title='Is Customer', labels=['Not Customer', 'Customer'])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group by 'iscustomer' and 'region', and get counts and mean age for each group\nregion_customer_summary = data.groupby(['iscustomer', 'region']).agg(\n    count=('region', 'count'),\n    mean_age=('age', 'mean')\n).reset_index()\n\n# Pivot the table to reshape it\npivot_table = region_customer_summary.pivot_table(\n    index='region',\n    columns='iscustomer',\n    values=['count', 'mean_age'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten the columns MultiIndex properly by converting all elements to string before joining\npivot_table.columns = [' '.join(map(str, col)).strip() for col in pivot_table.columns.values]\n\n# Reset index if needed\npivot_table.reset_index(inplace=True)\npivot_table\n\n\n\n\n\n\n\n\n\n\nindex\nregion\ncount 0\ncount 1\nmean_age 0\nmean_age 1\n\n\n\n\n0\n0\nMidwest\n207\n17\n27.596618\n22.852941\n\n\n1\n1\nNortheast\n488\n113\n26.519467\n24.579646\n\n\n2\n2\nNorthwest\n171\n16\n26.532164\n20.812500\n\n\n3\n3\nSouth\n171\n20\n27.464912\n24.950000\n\n\n4\n4\nSouthwest\n266\n31\n25.907895\n24.500000\n\n\n\n\n\n\n\n\nCount and Customer Status:\nNon customer are more numerous across all regions compared to customers\nNorthleast has the highest count of non-customers, while Midwest has the lowest count of customers\nMean Age\nNon-customers tend to have a higher mean age in all regions compared to customers\nThe Northwest region has the youndest average for customer\nThis information suggests that there might be regional preferences or differences in how products and services are adopted between customer groups. Younger individual tend to be customers more in Northwest, indicating possible more innovate or youth-targeted offerings in that region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe Likelihood Function is: \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{\\lambda}\\lambda^{Y_i}}{Y_i!}\n\\]\nThis can also be expressed as: \\[\nL(\\lambda) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}.\n\\]\nThe log-likelihood function is: \\[\n\\log L(\\lambda) = -n\\lambda + \\log(\\lambda) \\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nDefine likelihood and log-likekihood function for the Poisson model\n\n\nCode\ndef poisson_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    likelihood = np.exp(-n * lam) * (lam ** sum_Y) / np.prod([np.math.factorial(y) for y in Y])\n    return likelihood\n\ndef poisson_log_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lam + np.log(lam) * sum_Y - np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return log_likelihood\n\n\nUse function to plot lambda\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\nY = data['patents']\n\n# Define the range for lambda values\nlambda_range = np.linspace(0.01, 10, 1000)  # Start from 0.01 to avoid log(0)\n\n# Calculate the log-likelihood for each lambda in the range\nlog_likelihood_values = [poisson_log_likelihood(lam, Y) for lam in lambda_range]\n\n# Plot the results\nplt.figure(figsize=(8, 3))\nplt.plot(lambda_range, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('Lambda (Œª)')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Observed Patent Counts Across Lambda Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) , denoted as \\(\\lambda_{MLE}\\), we take the derivative of the log-likelihood with respect to \\(\\lambda\\) and set it equal to zero. The derivative is:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this derivative equal to zero for maximization gives:\n\\[\n0 = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSimplifying and solving for ( ) yields:\n\\[\n\\lambda_{MLE} = \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i\n\\]\nSo the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\), which is intuitive since for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\nCode\nlambda_mle = Y.mean()\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\nFind the MLE by optimizing your likelihood function with scipy.optimize in Python.\n\n\nCode\nfrom scipy.optimize import minimize\n\n# We define the negative log-likelihood function for the Poisson distribution\ndef negative_poisson_log_likelihood(lam, Y):\n    if lam &lt;= 0:  # Avoid log(0) for lam=0\n        return np.inf\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    # The negative sign is important because we want to maximize the log-likelihood,\n    # which is equivalent to minimizing its negative.\n    neg_log_likelihood = n * lam - np.log(lam) * sum_Y + np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return neg_log_likelihood\n\n# Initial guess for lambda can be the sample mean\ninitial_guess = [Y.mean()]\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=negative_poisson_log_likelihood,\n    x0=initial_guess,\n    args=(Y,),\n    method='L-BFGS-B', # This optimization method allows for bounding the solution\n    bounds=[(1e-5, None)] # Lambda must be greater than 0\n)\n\n# Extract the MLE for lambda from the result\nlambda_mle = result.x[0] if result.success else None\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe Log-Likelihood function is:\n\\[\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( -e^{X_i' \\beta} + Y_i X_i' \\beta - \\log(Y_i!) \\right)\n\\]\n\n\nCode\n# Define Log-Likelihood function\ndef possion_regression_function(beta, Y, X):\n   # Calculate lambda for each observation\n    linear_predictor = X.dot(beta)\n\n    lambda_i = np.exp(linear_predictor)\n    \n    # Calculate the log-likelihood\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - np.array([np.log(np.math.factorial(y)) for y in Y]))\n    \n    return log_likelihood\n\n\nUse the fuction to find the MLE vector and the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nfrom scipy.stats import poisson\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age'] ** 2\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Select the numeric predictors\nnumeric_features = ['age', 'age_squared']\n\n# Fit and transform the features\ndata[numeric_features] = scaler.fit_transform(data[numeric_features])\n\nregions = pd.get_dummies(data['region'], drop_first=True)\n\nX = pd.concat([pd.Series(1, index=data.index, name='Intercept'), data[['age', 'age_squared', 'iscustomer']], regions], axis=1)\n\ndef convert_boolean_columns_to_int(df):\n    for column in df.columns:\n        if df[column].dtype == bool:\n            # Convert Boolean column to int\n            df[column] = df[column].astype(int)\n    return df\n\nX = convert_boolean_columns_to_int(X)\n\nX_column_names = X.columns\n\nY = data['patents']\n\nX_glm = X.copy()\nY_glm = Y.copy()\n\nX = X.values\nY = Y.values\n\n\ndef possion_neg_regression_function(beta, Y, X):\n    # Calculate lambda for each observation\n    linear_predictor = np.dot(X, beta)\n    lambda_i = np.exp(linear_predictor)\n    lambda_i = np.clip(lambda_i, 1e-10, np.inf)  \n\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i)\n    return neg_log_likelihood\n\n# initial_beta = np.zeros(X.shape[1])\n# initial_beta = np.random.normal(loc=0, scale=1, size=X.shape[1])\ninitial_beta = np.zeros(X.shape[1])\ninitial_beta[0] = np.log(np.mean(Y))\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=possion_neg_regression_function,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n)\n\n\n\n\nCode\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_i = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_i)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix = inv(hessian_matrix)\n\nstandard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Display the coefficients and their standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': np.round(result.x, 4),\n    'Standard Error': np.round(standard_errors, 3)\n}, index=X_column_names)\nprint(coefficients_table)\n\n\n             Coefficient  Standard Error\nIntercept         1.2154           0.036\nage               1.0464           0.100\nage_squared      -1.1408           0.102\niscustomer        0.1181           0.039\nNortheast         0.0986           0.042\nNorthwest        -0.0201           0.054\nSouth             0.0572           0.053\nSouthwest         0.0514           0.047\n\n\nCheck results using sm.GLM() function.\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson())\nresults = model.fit()\n\n# Get the summary of the results\nglm_summary = results.summary()\n# Extract only the regression results table\nglm_results_table = glm_summary.tables[1]\n\n# To display or print out the table\nprint(glm_results_table)\n\n\n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       1.2154      0.036     33.368      0.000       1.144       1.287\nage             1.0465      0.100     10.414      0.000       0.850       1.243\nage_squared    -1.1408      0.102    -11.131      0.000      -1.342      -0.940\niscustomer      0.1181      0.039      3.035      0.002       0.042       0.194\nNortheast       0.0986      0.042      2.347      0.019       0.016       0.181\nNorthwest      -0.0201      0.054     -0.374      0.709      -0.126       0.085\nSouth           0.0572      0.053      1.085      0.278      -0.046       0.160\nSouthwest       0.0513      0.047      1.088      0.277      -0.041       0.144\n===============================================================================\n\n\nInterpret the results:\n\nIntercept: This value represents the baseline log-odds of patent success when all other predictor variables are held at zero.\nage: For every one-year increase in age, the log-odds of patent success increase by 0.1445. This suggests a positive relationship between the age of the patent application (or applicant) and the likelihood of success.With p &lt; 0.001, this variable indicates a strong positive relationship with the outcome as age increases, up to a point (due to the quadratic term).\nage squared:The negative coefficient for age squared indicates a diminishing return effect; as age increases, its positive impact on patent success starts to decrease. This typically suggests a peak point beyond which additional years in age reduce the likelihood of success. Also with p &lt; 0.001, it confirms the non-linear relationship where increasing age has diminishing returns on the log odds of the outcome.\niscustomer: Being a customer is associated with an increase in the log-odds of patent success by 0.1181 compared to non-customers. This effect is statistically significant and suggests that customers of Blueprinty might have a higher likelihood of patent success. This variable is statistically significant (p = 0.002), showing that being a customer positively influences the outcome.\nRegional Effects:\n\n\nNortheast: Indicates a positive effect on patent success in the Northeast compared to the baseline region (the one dropped during dummy coding).\nNorthwest: Suggests a slight negative effect on patent success compared to the baseline, but this may not be statistically significant.\nSouth and Southwest: Both regions show a positive effect on patent success, though the effects are small and the confidence around these estimates might overlap with zero, suggesting limited statistical significance.\n\nConclusions:\n\nAge: There‚Äôs a clear positive relationship between age and patent success, which peaks and then starts to decline as indicated by the quadratic term (age squared). This could reflect that mid-career individuals or entities are most successful in patent applications, possibly due to optimal combinations of experience and active engagement in their fields.\nCustomer Status: Being a customer of Blueprinty has a positive impact on patent success. This suggests that the services or products provided by Blueprinty are effective in enhancing the success rate of patents for their customers.\nRegional Variations: There are some regional differences in patent success rates, with the Northeast and Southern regions showing a positive impact relative to the baseline region."
  },
  {
    "objectID": "projects/poisson_reg/index.html#blueprinty-case-study",
    "href": "projects/poisson_reg/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\ndata = pd.read_csv('blueprinty.csv')\ndata = data.drop('Unnamed: 0', axis=1)\ndata\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows √ó 4 columns\n\n\n\n\nCompare histograms and means of number of patents by customer status:\n\n\nCode\nmean_patents = data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nThis indicate that, on average, customers have a slightly higher number of patents than non-customers. This might suggest that customer are more engaged or invest more patentable innovations, though other factors could also influence these resutls.\n\n\nCode\n# Set up the figure with two subplots for better comparison\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n# Plot histogram for non-customers\nsns.histplot(data[data['iscustomer'] == 0]['patents'], ax=axes[0], color='blue', bins=30, kde=True)\naxes[0].set_title('Patent Distribution for Non-Customers')\naxes[0].set_xlabel('Number of Patents')\naxes[0].set_ylabel('Frequency')\n\n# Plot histogram for customers\nsns.histplot(data[data['iscustomer'] == 1]['patents'], ax=axes[1], color='orange', bins=30, kde=True)\naxes[1].set_title('Patent Distribution for Customers')\naxes[1].set_xlabel('Number of Patents')\naxes[1].set_ylabel('Frequency')\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBoth groups show a similar general shape in their distribution, but customers tend to have a slightly higher frequency at higher patent counts, which corroborates the earlier finding that customers on average have more patents.\nThe distribution for both groups is skewed towards lower numbers of patents, with most individuals holding fewer patents.\nThe skewness is slightly more pronounced for customers, which could indicate that while fewer customers might have patents, those who do are likely to have more of them.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\n# Visualize the age distribution across regions by customer status\nplt.figure(figsize=(10, 5))\n\n# Boxplot to show age distribution\nsns.boxplot(x='region', y='age', hue='iscustomer', data=data, palette=['blue', 'orange'])\n\n# Adjust plot labels and title\nplt.title('Age Distribution by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Age')\nplt.legend(title='Is Customer', labels=['Not Customer', 'Customer'])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group by 'iscustomer' and 'region', and get counts and mean age for each group\nregion_customer_summary = data.groupby(['iscustomer', 'region']).agg(\n    count=('region', 'count'),\n    mean_age=('age', 'mean')\n).reset_index()\n\n# Pivot the table to reshape it\npivot_table = region_customer_summary.pivot_table(\n    index='region',\n    columns='iscustomer',\n    values=['count', 'mean_age'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten the columns MultiIndex properly by converting all elements to string before joining\npivot_table.columns = [' '.join(map(str, col)).strip() for col in pivot_table.columns.values]\n\n# Reset index if needed\npivot_table.reset_index(inplace=True)\npivot_table\n\n\n\n\n\n\n\n\n\n\nindex\nregion\ncount 0\ncount 1\nmean_age 0\nmean_age 1\n\n\n\n\n0\n0\nMidwest\n207\n17\n27.596618\n22.852941\n\n\n1\n1\nNortheast\n488\n113\n26.519467\n24.579646\n\n\n2\n2\nNorthwest\n171\n16\n26.532164\n20.812500\n\n\n3\n3\nSouth\n171\n20\n27.464912\n24.950000\n\n\n4\n4\nSouthwest\n266\n31\n25.907895\n24.500000\n\n\n\n\n\n\n\n\nCount and Customer Status:\nNon customer are more numerous across all regions compared to customers\nNorthleast has the highest count of non-customers, while Midwest has the lowest count of customers\nMean Age\nNon-customers tend to have a higher mean age in all regions compared to customers\nThe Northwest region has the youndest average for customer\nThis information suggests that there might be regional preferences or differences in how products and services are adopted between customer groups. Younger individual tend to be customers more in Northwest, indicating possible more innovate or youth-targeted offerings in that region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe Likelihood Function is: \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{\\lambda}\\lambda^{Y_i}}{Y_i!}\n\\]\nThis can also be expressed as: \\[\nL(\\lambda) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}.\n\\]\nThe log-likelihood function is: \\[\n\\log L(\\lambda) = -n\\lambda + \\log(\\lambda) \\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nDefine likelihood and log-likekihood function for the Poisson model\n\n\nCode\ndef poisson_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    likelihood = np.exp(-n * lam) * (lam ** sum_Y) / np.prod([np.math.factorial(y) for y in Y])\n    return likelihood\n\ndef poisson_log_likelihood(lam, Y):\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lam + np.log(lam) * sum_Y - np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return log_likelihood\n\n\nUse function to plot lambda\n\n\nCode\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\nY = data['patents']\n\n# Define the range for lambda values\nlambda_range = np.linspace(0.01, 10, 1000)  # Start from 0.01 to avoid log(0)\n\n# Calculate the log-likelihood for each lambda in the range\nlog_likelihood_values = [poisson_log_likelihood(lam, Y) for lam in lambda_range]\n\n# Plot the results\nplt.figure(figsize=(8, 3))\nplt.plot(lambda_range, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('Lambda (Œª)')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Observed Patent Counts Across Lambda Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\) , denoted as \\(\\lambda_{MLE}\\), we take the derivative of the log-likelihood with respect to \\(\\lambda\\) and set it equal to zero. The derivative is:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this derivative equal to zero for maximization gives:\n\\[\n0 = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSimplifying and solving for ( ) yields:\n\\[\n\\lambda_{MLE} = \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i\n\\]\nSo the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\), which is intuitive since for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\nCode\nlambda_mle = Y.mean()\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\nFind the MLE by optimizing your likelihood function with scipy.optimize in Python.\n\n\nCode\nfrom scipy.optimize import minimize\n\n# We define the negative log-likelihood function for the Poisson distribution\ndef negative_poisson_log_likelihood(lam, Y):\n    if lam &lt;= 0:  # Avoid log(0) for lam=0\n        return np.inf\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    # The negative sign is important because we want to maximize the log-likelihood,\n    # which is equivalent to minimizing its negative.\n    neg_log_likelihood = n * lam - np.log(lam) * sum_Y + np.sum([np.log(np.math.factorial(y)) for y in Y])\n    return neg_log_likelihood\n\n# Initial guess for lambda can be the sample mean\ninitial_guess = [Y.mean()]\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=negative_poisson_log_likelihood,\n    x0=initial_guess,\n    args=(Y,),\n    method='L-BFGS-B', # This optimization method allows for bounding the solution\n    bounds=[(1e-5, None)] # Lambda must be greater than 0\n)\n\n# Extract the MLE for lambda from the result\nlambda_mle = result.x[0] if result.success else None\nprint('Maximum Likelihood Estimator is',{lambda_mle})\n\n\nMaximum Likelihood Estimator is {3.6846666666666668}\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe Log-Likelihood function is:\n\\[\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( -e^{X_i' \\beta} + Y_i X_i' \\beta - \\log(Y_i!) \\right)\n\\]\n\n\nCode\n# Define Log-Likelihood function\ndef possion_regression_function(beta, Y, X):\n   # Calculate lambda for each observation\n    linear_predictor = X.dot(beta)\n\n    lambda_i = np.exp(linear_predictor)\n    \n    # Calculate the log-likelihood\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - np.array([np.log(np.math.factorial(y)) for y in Y]))\n    \n    return log_likelihood\n\n\nUse the fuction to find the MLE vector and the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nfrom scipy.stats import poisson\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare the covariate matrix X\ndata['age_squared'] = data['age'] ** 2\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Select the numeric predictors\nnumeric_features = ['age', 'age_squared']\n\n# Fit and transform the features\ndata[numeric_features] = scaler.fit_transform(data[numeric_features])\n\nregions = pd.get_dummies(data['region'], drop_first=True)\n\nX = pd.concat([pd.Series(1, index=data.index, name='Intercept'), data[['age', 'age_squared', 'iscustomer']], regions], axis=1)\n\ndef convert_boolean_columns_to_int(df):\n    for column in df.columns:\n        if df[column].dtype == bool:\n            # Convert Boolean column to int\n            df[column] = df[column].astype(int)\n    return df\n\nX = convert_boolean_columns_to_int(X)\n\nX_column_names = X.columns\n\nY = data['patents']\n\nX_glm = X.copy()\nY_glm = Y.copy()\n\nX = X.values\nY = Y.values\n\n\ndef possion_neg_regression_function(beta, Y, X):\n    # Calculate lambda for each observation\n    linear_predictor = np.dot(X, beta)\n    lambda_i = np.exp(linear_predictor)\n    lambda_i = np.clip(lambda_i, 1e-10, np.inf)  \n\n    neg_log_likelihood = -np.sum(Y * np.log(lambda_i) - lambda_i)\n    return neg_log_likelihood\n\n# initial_beta = np.zeros(X.shape[1])\n# initial_beta = np.random.normal(loc=0, scale=1, size=X.shape[1])\ninitial_beta = np.zeros(X.shape[1])\ninitial_beta[0] = np.log(np.mean(Y))\n\n# Minimize the negative log-likelihood function\nresult = minimize(\n    fun=possion_neg_regression_function,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n)\n\n\n\n\nCode\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_i = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_i)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix = inv(hessian_matrix)\n\nstandard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Display the coefficients and their standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': np.round(result.x, 4),\n    'Standard Error': np.round(standard_errors, 3)\n}, index=X_column_names)\nprint(coefficients_table)\n\n\n             Coefficient  Standard Error\nIntercept         1.2154           0.036\nage               1.0464           0.100\nage_squared      -1.1408           0.102\niscustomer        0.1181           0.039\nNortheast         0.0986           0.042\nNorthwest        -0.0201           0.054\nSouth             0.0572           0.053\nSouthwest         0.0514           0.047\n\n\nCheck results using sm.GLM() function.\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson())\nresults = model.fit()\n\n# Get the summary of the results\nglm_summary = results.summary()\n# Extract only the regression results table\nglm_results_table = glm_summary.tables[1]\n\n# To display or print out the table\nprint(glm_results_table)\n\n\n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       1.2154      0.036     33.368      0.000       1.144       1.287\nage             1.0465      0.100     10.414      0.000       0.850       1.243\nage_squared    -1.1408      0.102    -11.131      0.000      -1.342      -0.940\niscustomer      0.1181      0.039      3.035      0.002       0.042       0.194\nNortheast       0.0986      0.042      2.347      0.019       0.016       0.181\nNorthwest      -0.0201      0.054     -0.374      0.709      -0.126       0.085\nSouth           0.0572      0.053      1.085      0.278      -0.046       0.160\nSouthwest       0.0513      0.047      1.088      0.277      -0.041       0.144\n===============================================================================\n\n\nInterpret the results:\n\nIntercept: This value represents the baseline log-odds of patent success when all other predictor variables are held at zero.\nage: For every one-year increase in age, the log-odds of patent success increase by 0.1445. This suggests a positive relationship between the age of the patent application (or applicant) and the likelihood of success.With p &lt; 0.001, this variable indicates a strong positive relationship with the outcome as age increases, up to a point (due to the quadratic term).\nage squared:The negative coefficient for age squared indicates a diminishing return effect; as age increases, its positive impact on patent success starts to decrease. This typically suggests a peak point beyond which additional years in age reduce the likelihood of success. Also with p &lt; 0.001, it confirms the non-linear relationship where increasing age has diminishing returns on the log odds of the outcome.\niscustomer: Being a customer is associated with an increase in the log-odds of patent success by 0.1181 compared to non-customers. This effect is statistically significant and suggests that customers of Blueprinty might have a higher likelihood of patent success. This variable is statistically significant (p = 0.002), showing that being a customer positively influences the outcome.\nRegional Effects:\n\n\nNortheast: Indicates a positive effect on patent success in the Northeast compared to the baseline region (the one dropped during dummy coding).\nNorthwest: Suggests a slight negative effect on patent success compared to the baseline, but this may not be statistically significant.\nSouth and Southwest: Both regions show a positive effect on patent success, though the effects are small and the confidence around these estimates might overlap with zero, suggesting limited statistical significance.\n\nConclusions:\n\nAge: There‚Äôs a clear positive relationship between age and patent success, which peaks and then starts to decline as indicated by the quadratic term (age squared). This could reflect that mid-career individuals or entities are most successful in patent applications, possibly due to optimal combinations of experience and active engagement in their fields.\nCustomer Status: Being a customer of Blueprinty has a positive impact on patent success. This suggests that the services or products provided by Blueprinty are effective in enhancing the success rate of patents for their customers.\nRegional Variations: There are some regional differences in patent success rates, with the Northeast and Southern regions showing a positive impact relative to the baseline region."
  },
  {
    "objectID": "projects/poisson_reg/index.html#airbnb-case-study",
    "href": "projects/poisson_reg/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nBasics exploratory data analysis\n\n\nCode\nairbnb = pd.read_csv('airbnb.csv')\nairbnb = airbnb.drop('Unnamed: 0', axis=1)\nairbnb.head()\n\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nSummary statistics for numerical variables\n\n\nCode\nnumerical_summary = airbnb.describe()\nnumerical_summary\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nCount of missing values for each column\n\n\nCode\nmissing_values = airbnb.isnull().sum()\nmissing_values\n\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nHere‚Äôs what we‚Äôve learned from the dataset:\nMissing Values:\nbathrooms: 160 missing values.\nbedrooms: 76 missing values.\nreview_scores_cleanliness, review_scores_location,review_scores_value: Over 10,000 missing values each. This represents about 25% of the data, which is significant.\nhost_since: 35 missing values.\nLet‚Äôs start by handling the missing values based on the strategy described. We will impute the missing values for bathrooms and bedrooms with the median and decide on the review scores next\nImpute missing values for ‚Äòbathrooms‚Äô and ‚Äòbedrooms‚Äô with their respective medians\n\n\nCode\nairbnb['bathrooms'].fillna(airbnb['bathrooms'].median(), inplace=True)\nairbnb['bedrooms'].fillna(airbnb['bedrooms'].median(), inplace=True)\n# Checking updated missing values status\nupdated_missing_values = airbnb.isnull().sum()\nupdated_missing_values\n\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                        0\nbedrooms                         0\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nReview Scores: Given the high proportion of missing data (around 25%), filling them with the median or mean could introduce bias, especially if the missingness is not random. An alternative approach is to create a binary indicator variable for each of these scores, which will indicate whether the score was originally missing. This way, we retain all listings in our analysis and potentially capture some information about why scores might be missing (e.g., newer listings might not have scores yet).\nImpute missing review scores with the median\n\n\nCode\n# Create binary indicators for missing review scores\nairbnb['cleanliness_missing'] = airbnb['review_scores_cleanliness'].isnull().astype(int)\nairbnb['location_missing'] = airbnb['review_scores_location'].isnull().astype(int)\nairbnb['value_missing'] = airbnb['review_scores_value'].isnull().astype(int)\n\n# Impute missing review scores with the median\nairbnb['review_scores_cleanliness'].fillna(airbnb['review_scores_cleanliness'].median(), inplace=True)\nairbnb['review_scores_location'].fillna(airbnb['review_scores_location'].median(), inplace=True)\nairbnb['review_scores_value'].fillna(airbnb['review_scores_value'].median(), inplace=True)\n\n# Check if all missing values are addressed\nfinal_missing_values_check = airbnb.isnull().sum()\nfinal_missing_values_check\n\n\nid                            0\ndays                          0\nlast_scraped                  0\nhost_since                   35\nroom_type                     0\nbathrooms                     0\nbedrooms                      0\nprice                         0\nnumber_of_reviews             0\nreview_scores_cleanliness     0\nreview_scores_location        0\nreview_scores_value           0\ninstant_bookable              0\ncleanliness_missing           0\nlocation_missing              0\nvalue_missing                 0\ndtype: int64\n\n\nAll the missing values in the review scores have been addressed, and we now have binary indicators for whether the original scores were missing. The host_since column still has 35 missing values, but since it is not directly used in our model, we won‚Äôt focus on imputing it right now.\n\n\nCode\n# Setting up the visualization style\nsns.set(style=\"whitegrid\")\n\n# Plotting the distribution of number of reviews\nplt.figure(figsize=(8, 4))\nsns.histplot(airbnb['number_of_reviews'], bins=50, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet‚Äôs take a closer eye on 80% population of data\n\n\nCode\n# Calculate the 80th percentile\npercentile_80 = airbnb['number_of_reviews'].quantile(0.8)\n\n# Plotting the distribution of number of reviews limited to the 80th percentile\nplt.figure(figsize=(8, 4))\nsns.histplot(airbnb['number_of_reviews'], bins=50, kde=True, binrange=(0, percentile_80))\nplt.title('Distribution of Number of Reviews (80% of Data)')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.xlim(0, percentile_80)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe distribution of the number of reviews is highly skewed to the right, with most listings having a relatively low number of reviews and a few listings having a very high number\n\n\nCode\n# Visualization of number of reviews by room type\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='room_type', y='number_of_reviews', data=airbnb)\nplt.title('Number of Reviews by Room Type')\nplt.xlabel('Room Type')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Scatter plot for number of reviews vs. price\nplt.figure(figsize=(8, 4))\nsns.scatterplot(x='price', y='number_of_reviews', data=airbnb)\nplt.title('Number of Reviews vs. Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.xscale('log')  # Using logarithmic scale due to wide range of prices\nplt.show()\n\n# Scatter plots for number of reviews vs. review scores\nfig, axes = plt.subplots(1, 3, figsize=(8, 4))\nscore_vars = ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\ntitles = ['Cleanliness', 'Location', 'Value']\n\nfor ax, var, title in zip(axes, score_vars, titles):\n    sns.scatterplot(ax=ax, x=airbnb[var], y=airbnb['number_of_reviews'])\n    ax.set_title(f'Number of Reviews vs. {title}')\n    ax.set_xlabel(title)\n    ax.set_ylabel('Number of Reviews')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot interpretation:\nRoom Type:\nThe number of reviews varies by room type, with entire homes/apartments generally receiving more reviews than private or shared rooms. This might reflect a preference or higher turnover in these types of listings.\nPrice:\nThe relationship between price and number of reviews is not linear, suggesting that very high or very low priced listings might have fewer reviews. The logarithmic scale on price helps in visualizing this across a wide range of values.\nReview Scores:\nThere isn‚Äôt a clear trend in the scatter plots between review scores and the number of reviews, indicating that while scores may affect guest satisfaction, they do not necessarily correlate directly with the frequency of bookings (as measured by reviews). There might be a slight increase in reviews with higher scores for cleanliness and value.\n\n\nBuild Poisson Regression Model\n\n\nCode\nfrom patsy import dmatrices\n# Convert variables to the same time interval\n\nairbnb['last_scraped'] = pd.to_datetime(airbnb['last_scraped'])\nairbnb['host_since'] = pd.to_datetime(airbnb['host_since'])\n\n# Calculate the duration in years that each listing has been active\nairbnb['duration_years'] = (airbnb['last_scraped'] - airbnb['host_since']).dt.days / 365.25\n\n# Compute reviews per year\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['duration_years']\n\n# Handle cases where duration_years is zero to avoid division by zero\nairbnb['reviews_per_year'].fillna(0, inplace=True)\n\n\n# Encoding categorical variables using patsy (for statsmodels compatibility)\nformula = \"\"\"reviews_per_year ~ room_type + bathrooms + bedrooms + price +\n             review_scores_cleanliness + review_scores_location +\n             review_scores_value + cleanliness_missing + location_missing + value_missing\"\"\"\n\n# Prepare the design matrices for regression\ny, X = dmatrices(formula, airbnb, return_type='dataframe')\n\n# Fit a Poisson regression model using the same training data\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\npoisson_summary = poisson_results.summary()\n\n# Extract only the regression results table\npoisson_results_table = poisson_summary.tables[1]\n\n# To display or print out the table\nprint(poisson_results_table)\n\n\n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     2.1560      0.026     84.372      0.000       2.106       2.206\nroom_type[T.Private room]     0.0338      0.004      7.737      0.000       0.025       0.042\nroom_type[T.Shared room]      0.0871      0.012      7.525      0.000       0.064       0.110\nbathrooms                     0.0105      0.006      1.903      0.057      -0.000       0.021\nbedrooms                      0.0811      0.003     25.960      0.000       0.075       0.087\nprice                        -0.0002   1.86e-05    -12.370      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1249      0.002     53.488      0.000       0.120       0.129\nreview_scores_location       -0.0711      0.003    -28.236      0.000      -0.076      -0.066\nreview_scores_value          -0.0577      0.003    -20.285      0.000      -0.063      -0.052\ncleanliness_missing          -2.2750      0.122    -18.625      0.000      -2.514      -2.036\nlocation_missing             -1.1305      0.148     -7.651      0.000      -1.420      -0.841\nvalue_missing                -1.4111      0.145     -9.734      0.000      -1.695      -1.127\n=============================================================================================\n\n\n\n\nBuild Binomial Regression Model\n\n\nCode\n# # Encoding categorical variables using patsy (for statsmodels compatibility)\n# formula = \"\"\"number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n#              review_scores_cleanliness + review_scores_location +\n#              review_scores_value + cleanliness_missing + location_missing + value_missing\"\"\"\n\n# Fit the negative binomial regression model\nnb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial())\nnb_results = nb_model.fit()\n\n# Obtain the summary of the model\nnb_summary = nb_results.summary()\n\n# Extract only the regression results table\nnb_results_table = nb_summary.tables[1]\n\n# Display or print out the table\nprint(nb_results_table)\n\n\n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     2.3226      0.081     28.540      0.000       2.163       2.482\nroom_type[T.Private room]     0.0337      0.013      2.573      0.010       0.008       0.059\nroom_type[T.Shared room]      0.0954      0.037      2.588      0.010       0.023       0.168\nbathrooms                     0.0106      0.017      0.611      0.541      -0.023       0.045\nbedrooms                      0.0762      0.010      7.796      0.000       0.057       0.095\nprice                        -0.0002   4.12e-05     -4.826      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1825      0.007     26.014      0.000       0.169       0.196\nreview_scores_location       -0.0949      0.008    -11.657      0.000      -0.111      -0.079\nreview_scores_value          -0.1087      0.009    -11.981      0.000      -0.126      -0.091\ncleanliness_missing          -2.5131      0.165    -15.223      0.000      -2.837      -2.190\nlocation_missing             -0.6405      0.260     -2.466      0.014      -1.150      -0.131\nvalue_missing                -1.6547      0.258     -6.421      0.000      -2.160      -1.150\n=============================================================================================\n\n\nCoeficient Interpret for both model:\n\nIntercep: This is log of expected count of reviews when all other variables are zero. Since this scenario isn‚Äôt realistic, it primarily serves as baseline for the model\nroom_type\n\n\nPrivate room: listings that are private rooms have slightly more reviews compared to entire homes/apartments, holding other factors constant. The effect is relatively small\nShare room: Shared rooms are expected to have more reviews than the baseline category. Shared rooms show a stronger positive association with the number of reviews compared to private rooms.\n\n\nbathrooms: More bathrooms are associated with more reviews, indicating that listings with more bathrooms might be more frequently booked or reviewed.\nbedrooms: More bedrooms are associated with an increase in the expected count of reviews, suggesting that larger properties might attract more bookings and thus more reviews\nprice: A higher price is slightly negatively associated with the number of reviews. This small coeficient suggests that price inceases might slightly reduce the likelihood of getting reviewed.\nReview scores:\n\n\ncleanliness: higher clealiness scrores are positively associated with more reviews, indicating that cleanser listings are more likely to receive reviews.\nlocation: Surprisingly, better location scores are associated with fewer reviews. This might reflex a complex interaction with other factors not captured in the model or the properties in desiable locations might not meet all guest expectations or that guests in such locations review less frequently.\nvalue: Better value scores are also negatively associated with the number of reviews, which might suggest that guest have higher expectations that aren‚Äôt met as often they rate the value highly.\n\n\nMissing Indicators:\n\n\ncleanliness missing: Listing missing cleanliness scores have significantly fewer reviews, possibly indicating newer or less popular listings.\nlocation missing: Similarly, listings missing location scores have fewer reviews\nvalue missing: Listing missing value scores also have fewer reviews."
  },
  {
    "objectID": "projects/intuit/index.html",
    "href": "projects/intuit/index.html",
    "title": "Intuit Upgrade Notebook",
    "section": "",
    "text": "This analytical examines the efficacy of an upsell campaign targeting small businesses to upgrade to the latest QuickBooks software, based on a dataset of 75,000 entities from a pool of 801,821.\nUtilizing logistic regression and neural networks, the analysis aims to:\nBy leveraging logistic regression, we assess the probabilistic impact of variables on customer decisions, while neural networks allow for a deeper dive into complex, non-linear relationships among data points. This dual-model approach provides:"
  },
  {
    "objectID": "projects/intuit/index.html#data-analysis",
    "href": "projects/intuit/index.html#data-analysis",
    "title": "Intuit Upgrade Notebook",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pyrsm as rsm\n\n# increase plot resolution\n# mpl.rcParams[\"figure.dpi\"] = 150\nrsm.__version__\n\nintuit75k = pd.read_parquet(\"data/intuit75k.parquet\")\n\n\nData Description: Intuit Quickbooks upgrade\nThe purpose of this exercise is to gain experience modeling the response to an upsell campaign. The intuit75k.parquet file contains data on 75,000 (small) businesses that were selected randomly from the 801,821 that were sent the wave-1 mailing. The mailing contained an offer to upgrade to the latest version of the Quickbooks software.\nVariable res1 denotes which of these businesses responded to the mailing by purchasing Quickbooks version 3.0 from Intuit Direct. Note that Intuit Direct sells products directly to its customers rather than through a retailer. Use the available data to predict which businesses that did not respond to the wave-1 mailing, are most likely to respond to the wave-2 mailing. Note that variables were added, deleted, and recoded so please ignore the variable descriptions in Exhibit 3 in the case in the course reader. Instead, use the variable descriptions below:\n\nVariable description\n\nid: Small business customer ID\nzip5: 5-Digit ZIP Code (00000=unknown, 99999=international ZIPs).\nzip_bins: Zip-code bins (20 approx. equal sized bins from lowest to highest zip code number)\nsex: Gender Identity ‚ÄúFemale‚Äù, ‚ÄúMale‚Äù, or ‚ÄúUnknown‚Äù\nbizflag: Business Flag. Address contains a Business name (1 = yes, 0 = no or unknown).\nnumords: Number of orders from Intuit Direct in the previous 36 months\ndollars: Total $ ordered from Intuit Direct in the previous 36 months\nlast: Time (in months) since last order from Intuit Direct in previous 36 months\nsincepurch: Time (in months) since original (not upgrade) purchase of Quickbooks\nversion1: Is 1 if customer‚Äôs current Quickbooks is version 1, 0 if version 2\nowntaxprod: Is 1 if customer purchased tax software, 0 otherwise\nupgraded: Is 1 if customer upgraded from Quickbooks vs.¬†1 to vs.¬†2\nres1: Response to wave 1 mailing (‚ÄúYes‚Äù if responded else ‚ÄúNo‚Äù)\ntraining: 70/30 split, 1 for training sample, 0 for validation sample\n\n\n\n\nData Preprocessing\nIn the data preprocessing section, we will be reviewing the variables that are present in our dataset and identifying any potential scaling or data pre-processing actions that needs to be made. We will also be reviewing our dataset to ensure tha there are no null values, and that the data types are aligned.\n\nintuit75k.head()\n\n\n\n\n\n\n\n\n\nid\nzip5\nzip_bins\nsex\nbizflag\nnumords\ndollars\nlast\nsincepurch\nversion1\nowntaxprod\nupgraded\nres1\ntraining\nres1_yes\n\n\n\n\n0\n1\n94553\n18\nMale\n0\n2\n109.5\n5\n12\n0\n0\n0\nNo\n1\n0\n\n\n1\n2\n53190\n10\nUnknown\n0\n1\n69.5\n4\n3\n0\n0\n0\nNo\n0\n0\n\n\n2\n3\n37091\n8\nMale\n0\n4\n93.0\n14\n29\n0\n0\n1\nNo\n0\n0\n\n\n3\n4\n02125\n1\nMale\n0\n1\n22.0\n17\n1\n0\n0\n0\nNo\n1\n0\n\n\n4\n5\n60201\n11\nMale\n0\n1\n24.5\n2\n3\n0\n0\n0\nNo\n0\n0\n\n\n\n\n\n\n\n\n\nintuit75k.hist(figsize=(20, 20))\n\narray([[&lt;Axes: title={'center': 'id'}&gt;,\n        &lt;Axes: title={'center': 'zip_bins'}&gt;,\n        &lt;Axes: title={'center': 'bizflag'}&gt;],\n       [&lt;Axes: title={'center': 'numords'}&gt;,\n        &lt;Axes: title={'center': 'dollars'}&gt;,\n        &lt;Axes: title={'center': 'last'}&gt;],\n       [&lt;Axes: title={'center': 'sincepurch'}&gt;,\n        &lt;Axes: title={'center': 'version1'}&gt;,\n        &lt;Axes: title={'center': 'owntaxprod'}&gt;],\n       [&lt;Axes: title={'center': 'upgraded'}&gt;,\n        &lt;Axes: title={'center': 'training'}&gt;,\n        &lt;Axes: title={'center': 'res1_yes'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nPrior to creating any models, we want to review the distribution and scale of the variables in the dataset. If there is scale in the distribution of the variables as exemplified by the histograms produced above, this will effect the ability of our model to accurately and effectively predict the response to our second wave mailing outreach. Based on the distribution of the histograms above, the variable with the greatest skew is the dollars. It skews heavily towards the left, as do the numrods and sincepurch variables. Due to this, we will want to apply some type of transformation to these variables to normalize their distribution.\n\ntrain_data = intuit75k[intuit75k['training'] == 1]\n\nHere we are creating our training set of data. This corresponds to any record that has a 1 in the training column within the Intuit75k database. As per the intuit instructions, the data was already pre-split into 70:30 training:test sets. If the data was not pre-split, we would want to consider employing the StratifyShuffleSplit method to create our training and testing data. This would ensure that our training and test sets have the same number of yes‚Äôs, which would provide a more accurate and representative dataset for the training and testing of our model.\n\nintuit75k['res1'].value_counts()\n\nres1\nNo     71399\nYes     3601\nName: count, dtype: int64\n\n\n\n\nCorrelation Between Features\nIn addition to scaling the data before we start building our models, we want to identify any potential correlation between the features in our dataset. If any features are highly correlated, this could lead to multi-collinearity within our models and lead to inflation of the variable coefficients or inflated importance of features on our model. If we are able to identify any correlation prior to building our model, we are able to adjust which features are correlated and see how this might increase their importance if a highly-correlated feature is omitted from the model.\n\ncorr_columns = ['zip_bins','bizflag','numords','dollars','last','sincepurch']\ncorr_matrix = train_data[corr_columns].corr()\ncorr_matrix\n\n\n\n\n\n\n\n\n\nzip_bins\nbizflag\nnumords\ndollars\nlast\nsincepurch\n\n\n\n\nzip_bins\n1.000000\n-0.000679\n0.005397\n0.006667\n-0.000726\n-0.001555\n\n\nbizflag\n-0.000679\n1.000000\n-0.000113\n0.002398\n0.000647\n-0.003839\n\n\nnumords\n0.005397\n-0.000113\n1.000000\n0.587267\n-0.130395\n0.000284\n\n\ndollars\n0.006667\n0.002398\n0.587267\n1.000000\n-0.073897\n0.003649\n\n\nlast\n-0.000726\n0.000647\n-0.130395\n-0.073897\n1.000000\n-0.001506\n\n\nsincepurch\n-0.001555\n-0.003839\n0.000284\n0.003649\n-0.001506\n1.000000\n\n\n\n\n\n\n\n\nBased on the correlation matrix created above, we see that there is not a high level of correlation between the features of this dataset. A high level of correlation is traditionally considered within the range of 0.7 and 0.9. Correlation coefficients whose magnitude is between 0.5 and 0.7 indicates that variables are moderately correlated. Dollars and numrods have a moderate correlation value of 0.59. It would make sense that these variables have a moderate correlation. The numrod variable represents number of orders from Intuit Direct in the previous 36 months, whereas Dollars represents the total dollar value ordered from intuit direct in the previous 36 months. Traditionally, orders and dollar value tend to have a positive - direct relationship, as we see orders increase, we expect to see dollars increase as well."
  },
  {
    "objectID": "projects/intuit/index.html#logistic-regression-model",
    "href": "projects/intuit/index.html#logistic-regression-model",
    "title": "Intuit Upgrade Notebook",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nNow that we have transformed the variables to be on the same scale and have identified correlated variables, we can begin building our first logistic regression model. The goal of this model is to predict the probability of a business responding to a mailed offer.\n\n#response rate of the first wave\nresponse_rate = train_data[\"res1_yes\"].mean()\nresponse_rate\n\n0.047580952380952383\n\n\nWe can find the average response rate to the first wave mailing offer by taking the mean of the res1_yes column across the training dataset. This will be important when we try to project our model‚Äôs findings onto the larger client base as a whole. This variable represents whether a business responded to the offer or not. The average response rate in the first wave mailing is 4.76%.\n\nLogistic Regression Model 1\n\nlr = rsm.logistic(\n    data = {'wave1': train_data},\n    rvar = \"res1_yes\",\n    evar = [\n        \"zip_bins\",\n        \"bizflag\",\n        \"numords\",\n        \"dollars\",\n        \"last\",\n        \"sincepurch\",\n        \"sex\",\n        \"version1\",\n        \"owntaxprod\",\n        \"upgraded\"\n    ]\n)\n\n\nlr.summary(main = True, vif = True)\n\nLogistic regression (GLM)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, bizflag, numords, dollars, last, sincepurch, sex, version1, owntaxprod, upgraded\nNull hyp.: There is no effect of x on res1_yes\nAlt. hyp.: There is an effect of x on res1_yes\n\n                 OR     OR%  coefficient  std.error  z.value p.value     \nIntercept     0.052  -94.8%        -2.96      0.087  -34.016  &lt; .001  ***\nsex[Male]     0.987   -1.3%        -0.01      0.052   -0.249   0.804     \nsex[Unknown]  0.991   -0.9%        -0.01      0.074   -0.118   0.906     \nzip_bins      0.946   -5.4%        -0.06      0.004  -14.931  &lt; .001  ***\nbizflag       1.047    4.7%         0.05      0.048    0.964   0.335     \nnumords       1.251   25.1%         0.22      0.019   12.010  &lt; .001  ***\ndollars       1.001    0.1%         0.00      0.000    3.638  &lt; .001  ***\nlast          0.958   -4.2%        -0.04      0.002  -17.959  &lt; .001  ***\nsincepurch    1.002    0.2%         0.00      0.004    0.518   0.605     \nversion1      2.082  108.2%         0.73      0.085    8.583  &lt; .001  ***\nowntaxprod    1.359   35.9%         0.31      0.101    3.048   0.002   **\nupgraded      2.540  154.0%         0.93      0.084   11.076  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nVariance inflation factors:\n\n              vif    Rsq\nsincepurch  3.736  0.732\nversion1    2.972  0.664\nupgraded    2.918  0.657\nnumords     1.558  0.358\ndollars     1.527  0.345\nowntaxprod  1.027  0.026\nlast        1.017  0.017\nsex         1.000  0.000\nbizflag     1.000  0.000\nzip_bins    1.000  0.000\n\n\n\nLogistic Regression Summary Interpretation\nBased on the first version of our logistic regression model, there are a couple of important things to note in regards to feature selection and overall performance of the model. First, we want to take a look at our Pseudo R-squared (McFadden) and Pseudo R-squared (McFadden Adjusted) values. Based on our output, we see that these values are 0.073 and 0.07, respectively. The Pseudo R-squared values are generally lower than the traditional R-squared in linear regression models and don‚Äôt have a fixed range. However, the values for our McFadden and McFadden are still on the lower side. This isn‚Äôt automatically a deal-breaker for this model or automatically a reason to throw our model away, but we would want weigh these R-squared values in conjunction with other factors (such as AUC, and Chi-squared) to evaluate the effectiveness and predictive performance of our model. \nIn addition to the r-squared values, we also want to evaluate the AUC value. AUC indicates a model‚Äôs ability to discriminate between a positive and negative class. The AUC value is on a fixed scale, with a base threshold of 0.5. Any value over 0.5 is considered to be a better value, and indicate a better model. Our first logisitic regression model had an AUC of 0.708 which suggests that the model has a good ability to distinguish between the two classes (i.e.¬†responded to the mailing offer or not). We can also use the AIC and BIC values to evaluate our model as well. The AIC and BIC respectively represent the Akaike Information Criterion and the Bayesian Information Criterion. These values also indicate the models‚Äôs fit to the data. Generally, the goal is to have a lower AIC and BIC value, and models with lower AIC and BIC values are preferred. Our model has an AIC value of 18688.123, and the BIC value of 18794.545. These values are a bit difficult to represent or evaluate at first, but will become more important when we begin to omit variables and see how these values change. If both the AIC and BIC values become lower when removing a variable, we would consider using the model that produces the lower values.\nFinally, the most important value when evaluating a models performance is the Chi-squared value. The Chi-squared value of 1425.521 with 11 degrees of freedom and a p.value &lt; 0.001 suggests that the model as a whole is statistically significant, and at least some of the predictors or features in this model are significantly related to the outcome. Therefore, our first logistic regression model with all of the variables included seems to be a statistically significant model with a decent prediction ability. We would want to consider removing any statistically insignificant variables to see how the model‚Äôs performance changes, in hopes of building a more powerful prediction model.\n\nlr.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\n\nPermutation Importance Plot Interpretation\nBased on the permutation importance graph above, we can see that the variables of importance are upgraded, last, zip_bins, numrods, version1, owntaxprod, dollars. These variables however, do not contribute to a very large decrease in AUC, but this graph allows us to view the ratio of importance of the variables. Additionally, we can see that the bizflag, sincepurch, and sex variables are not important. Based on their lack of importance via AUC decrease, and their statistical insignificance, as identified by their high p-values in the logistic regression summary, we will consider testing their removal from the model and seeing if the model‚Äôs performance changes.\n\nlr.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\nPrediction Plot Interpretation\nBased on the prediction plots above, it looks like gender has very little influence of a prediction value. Similarily bizflag, and since_purch also looks insignificant, and doesn‚Äôt seem to contribute much to the prediction value. Therefore, we will want to remove them from this logistic regression model to see if the new logistic regression improves the AUC and r^2 value.\n\n\nTesting the removal of features from the model\nAs mentioned above, bizflag, sincepurch, and sex all have low importance and are statistically insignificant to our model, as indicated by their high p-values. We want to test removing them from our model to see if the performance of our model changes. The change in model performance will be interpreted based on how the AUC, BIC, Pseudo R-Squared, and Chi-squared values change in comparison to our baseline model with all of the features included as explanatory variables.\n\n\nRemoving bizflag, sincepurch, and sex together\n\nlr.summary(main = False, test=['sincepurch', 'sex','bizflag'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 1.257 df (4), p.value 0.869\n\n\n\nModel Interpretation\nWe first wanted to see the model‚Äôs performance when we removed sincepurch, sex, and bizflag together from the model. As identified above, we noticed that these variables had the lowest importance to the model and were statistically insignificant to the baseline logistic regression model. The test function enables us to generate a logistic regression output as if we were generating the models without these variables being included. We are then able to compare the model‚Äôs performance with and without them included. As we see, the first output shows our baseline model‚Äôs output. We see the same metrics that were identified previously.\n The second output shows us the Pseudo R-squared values in the baseline logit model vs.¬†our new model with the variables being removed. As we see via the output, the Pseudo r-squared value does not change in model 1 vs.¬†model 2, this indicates that the overall explanatory power of the models is unchanged despite the removal of the sincepurch,sex, and bizflag variables. However, the chi-squared value and the p.value change significantly, which is critical. The chi-squared value of 1.257 with 4 degrees of freedom and a p-value of 0.869 from comparing Model 1 and Model 2 suggests that the variables removed do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes.\n essentially, the comparison suggests that the removal of bizflag, sincepurch, and sex between Model 1 and Model 2 does not significantly affect the model‚Äôs explanatory power or predictive ability. This is indicated by the unchanged Pseudo R-squared values and the non-significant chi-squared test for the comparison. Therefore, if removing variables does not significantly decrease the model‚Äôs performance, it might be beneficial to keep the model simpler by excluding them. A simpler model is easier to interpret and may generalize better to new data. We will also want to test the removal of the variables one by one to see if that improves the model‚Äôs explanatory power.\n\n\n\nRemoving sincepurch, and sex\n\nlr.summary(main = False, test = ['sincepurch', 'sex'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.331 df (3), p.value 0.954\n\n\n\nModel interpretation\nIn the previous section, we evaluated the performance of our logistic regression model when we removed the three insignificant variables from our model as interpreted from their individual p-values. In this removal round, we wanted to evalute the performance of our logistic regression model if we considered removing sincepurch and sex. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.331 with 3 degrees of freedom and a p-value of 0.954 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\n\nRemoving sex\n\nlr.summary(main = False, test = ['sex'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.062 df (2), p.value 0.97\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of sex. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.062 with 2 degrees of freedom and a p-value of 0.97 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\n\nRemoving sincepurch\n\nlr.summary(main = False, test=['sincepurch'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.268 df (1), p.value 0.605\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of sincepurch. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.268 with 1 degrees of freedom and a p-value of 0.605 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\n\nRemoving bizflag\n\nlr.summary(main = False, test=['bizflag'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 0.924 df (1), p.value 0.337\n\n\n\nModel interpretation\nIn this removal round, we wanted to evaluate the performance of our logistic regression model if we tested the removal of bizflag. Both models have the same Pseudo R-squared values (0.071), which suggests that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.924 with 1 degree of freedom and a p-value of 0.337 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n\n\n\nRemoving dollars\n\nlr.summary(main = False, test=['dollars'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + last + sincepurch + sex + version1 + owntaxprod + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.070 vs 0.071\nChi-squared: 12.732 df (1), p.value &lt; .001\n\n\n\nModel interpretation\nAs mentioned previous, Both models have the same Pseudo R-squared values (0.071), suggesting that the overall explanatory power of the models is unchanged despite the removal or addition of variables. Additionally, The chi-squared value of 0.331 with 3 degrees of freedom and a p-value of 0.954 from comparing Model 1 and Model 2 suggests that the variables removed (or added) do not significantly contribute to the model. A high p-value (p &gt; 0.05) indicates that there is no significant loss (or gain) in explanatory power due to these changes. This indicates that we might want to consider removing them completely from the model as they don‚Äôt increase the explanatory power of the model.\n The comparison between Model 1 and Model 2 with Pseudo R-squared, Model 1 vs 2: 0.070 vs 0.071 and Chi-squared: 12.732 df (1), p.value &lt; .001 shows a minor improvement in the Pseudo R-squared from Model 1 to Model 2, indicating a slightly better fit in Model 2. The chi-squared value being significant (p &lt; .001) for the comparison suggests that the change between models (likely the inclusion of dollars in Model 2) significantly improves the model‚Äôs fit to the data.\n\n\n\nRemoving owntaxprod\n\nlr.summary(main = False, test=['owntaxprod'])\n\n\nPseudo R-squared (McFadden): 0.071\nPseudo R-squared (McFadden adjusted): 0.07\nArea under the RO Curve (AUC): 0.709\nLog-likelihood: -9332.061, AIC: 18688.123, BIC: 18794.545\nChi-squared: 1425.521, df(11), p.value &lt; 0.001 \nNr obs: 52,500\n\nModel 1: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + upgraded\nModel 2: res1_yes ~ zip_bins + bizflag + numords + dollars + last + sincepurch + sex + version1 + owntaxprod + upgraded\nPseudo R-squared, Model 1 vs 2: 0.071 vs 0.071\nChi-squared: 8.694 df (1), p.value 0.003\n\n\n\nModel interpretation\nModel 1 and Model 2 are logistic regression models predicting the binary outcome res_yes_no using different sets of explanatory variables. The primary difference between the models is the inclusion of owntaxprod in Model 2, which is not present in Model 1. Pseudo R-squared for Model 1 vs.¬†Model 2: 0.071 vs.¬†0.071. This metric provides a measure of the explanatory power of the models, with values closer to 1 indicating a better fit. The identical Pseudo R-squared values for both models suggest that they have the same explanatory power, meaning the addition of owntaxprod in Model 2 does not improve the model‚Äôs ability to explain the variability in the outcome res_yes_no.\nChi-squared: 8.694, df (1), p.value 0.003. This test compares the two models to determine if the addition of the owntaxprod variable in Model 2 significantly improves the model fit compared to Model 1. The chi-squared statistic is used to assess this, with the degrees of freedom (df) typically representing the difference in the number of parameters estimated between the two models. In this case, the df is 1, corresponding to the one additional variable in Model 2. The p-value of 0.003 indicates that the difference in fit between Model 1 and Model 2 is statistically significant at conventional significance levels (e.g., Œ± = 0.05). Therefore, despite the Pseudo R-squared values being identical, the statistical test suggests that owntaxprod provides a significant contribution to predicting the outcome when it is added to the model. \nGiven these results, we would probably want to include owntaxprod in our model. However, some other factors we would want to consider are:\n\n\nModel complexity: Adding more variables can make the model more complex and potentially harder to interpret.\n\n\nPractical significance: Evaluate whether the inclusion of owntaxprod has practical significance in addition to its statistical significance.\n\n\nCross-validation: Perform cross-validation to assess the generalizability of the model and avoid overfitting.\n\n\nFurther evaluation: Consider other metrics such as AUC, accuracy, precision, recall, and F1 score for a more comprehensive evaluation of model performance, especially in the context of the specific application or decision-making process the model is intended to support.\n\n\n\nCheck cross-validation\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"zip_bins\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: zip_bins, res1\nNull hyp : There is no association between zip_bins and res1\nAlt. hyp : There is an association between zip_bins and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nzip_bins                       \n1         21.7%   78.3%  100.0%\n2          4.2%   95.8%  100.0%\n3         3.37%  96.63%  100.0%\n4         4.06%  95.94%  100.0%\n5         3.35%  96.65%  100.0%\n...         ...     ...     ...\n17        3.88%  96.12%  100.0%\n18        5.21%  94.79%  100.0%\n19        4.14%  95.86%  100.0%\n20        3.71%  96.29%  100.0%\nTotal     4.76%  95.24%  100.0%\n\n[21 rows x 3 columns]\n\nChi-squared: 1802.53 df(19), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"sex\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: sex, res1\nNull hyp : There is no association between sex and res1\nAlt. hyp : There is an association between sex and res1\n\nRow percentages:\n\nres1       Yes      No   Total\nsex                           \nFemale    4.8%   95.2%  100.0%\nMale     4.76%  95.24%  100.0%\nUnknown   4.7%   95.3%  100.0%\nTotal    4.76%  95.24%  100.0%\n\nChi-squared: 0.1 df(2), p.value 0.95\n0.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"bizflag\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: bizflag, res1\nNull hyp : There is no association between bizflag and res1\nAlt. hyp : There is an association between bizflag and res1\n\nRow percentages:\n\nres1       Yes      No   Total\nbizflag                       \n0        4.71%  95.29%  100.0%\n1        4.92%  95.08%  100.0%\nTotal    4.76%  95.24%  100.0%\n\nChi-squared: 1.01 df(1), p.value 0.32\n0.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"version1\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: version1, res1\nNull hyp : There is no association between version1 and res1\nAlt. hyp : There is an association between version1 and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nversion1                       \n0         4.33%  95.67%  100.0%\n1         6.35%  93.65%  100.0%\nTotal     4.76%  95.24%  100.0%\n\nChi-squared: 79.91 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"owntaxprod\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: owntaxprod, res1\nNull hyp : There is no association between owntaxprod and res1\nAlt. hyp : There is an association between owntaxprod and res1\n\nRow percentages:\n\nres1          Yes      No   Total\nowntaxprod                       \n0           4.66%  95.34%  100.0%\n1           8.18%  91.82%  100.0%\nTotal       4.76%  95.24%  100.0%\n\nChi-squared: 40.07 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\npd.crosstab(\n    train_data[\"version1\"],\n    train_data[\"upgraded\"],\n    margins = True,\n    margins_name = \"Total\"\n).map(lambda x: \"{:,}\".format(x))\n\n\n\n\n\n\n\n\nupgraded\n0\n1\nTotal\n\n\nversion1\n\n\n\n\n\n\n\n0\n30,433\n10,877\n41,310\n\n\n1\n11,190\n0\n11,190\n\n\nTotal\n41,623\n10,877\n52,500\n\n\n\n\n\n\n\n\n\nct = rsm.basics.cross_tabs({'intuit75k' :train_data}, \"upgraded\", \"res1\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : intuit75k\nVariables: upgraded, res1\nNull hyp : There is no association between upgraded and res1\nAlt. hyp : There is an association between upgraded and res1\n\nRow percentages:\n\nres1        Yes      No   Total\nupgraded                       \n0         3.99%  96.01%  100.0%\n1          7.7%   92.3%  100.0%\nTotal     4.76%  95.24%  100.0%\n\nChi-squared: 262.79 df(1), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"numords\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n2.568\n0.025\n103.967\n0.0\n***\n\n\n1\nres1[T.No]\n-0.523\n0.025\n-20.643\n0.0\n***\n\n\n\n\n\n\n\n\n\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"dollars\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n117.000\n1.615\n72.448\n0.0\n***\n\n\n1\nres1[T.No]\n-25.533\n1.655\n-15.430\n0.0\n***\n\n\n\n\n\n\n\n\n\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"last\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n12.022\n0.190\n63.192\n0.0\n***\n\n\n1\nres1[T.No]\n4.026\n0.195\n20.653\n0.0\n***\n\n\n\n\n\n\n\n\n\nreg = rsm.model.regress(\n    data = {\"intuit75k\": train_data},\n    rvar = \"sincepurch\",\n    evar = 'res1'\n)\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n19.153\n0.200\n95.793\n0.0\n***\n\n\n1\nres1[T.No]\n-3.712\n0.205\n-18.121\n0.0\n***\n\n\n\n\n\n\n\n\n\n\nCheck the zip_bins\n\nfig = rsm.prop_plot(train_data, \"zip_bins\", \"res1\", \"Yes\")\n\n\n\n\n\n\n\n\n\nintuit75k['zip_bins'] = intuit75k['zip_bins'].astype('category')\n\nlr = rsm.logistic(\n    data = {'intuit75k': train_data},\n    rvar = \"res1_yes\",\n    evar = [\n        \"zip_bins\",\n        \"bizflag\",\n        \"numords\",\n        \"dollars\",\n        \"last\",\n        \"sincepurch\",\n        \"sex\",\n        \"version1\",\n        \"owntaxprod\",\n        \"upgraded\"\n    ]\n)\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\nOR\nOR%\ncoefficient\nstd.error\nz.value\np.value\n\n\n\n\n\n0\nIntercept\n0.052\n-94.814\n-2.959\n0.087\n-34.016\n0.000\n***\n\n\n1\nsex[T.Male]\n0.987\n-1.286\n-0.013\n0.052\n-0.249\n0.804\n\n\n\n2\nsex[T.Unknown]\n0.991\n-0.870\n-0.009\n0.074\n-0.118\n0.906\n\n\n\n3\nzip_bins\n0.946\n-5.376\n-0.055\n0.004\n-14.931\n0.000\n***\n\n\n4\nbizflag\n1.047\n4.743\n0.046\n0.048\n0.964\n0.335\n\n\n\n5\nnumords\n1.251\n25.150\n0.224\n0.019\n12.010\n0.000\n***\n\n\n6\ndollars\n1.001\n0.097\n0.001\n0.000\n3.638\n0.000\n***\n\n\n7\nlast\n0.958\n-4.172\n-0.043\n0.002\n-17.959\n0.000\n***\n\n\n8\nsincepurch\n1.002\n0.203\n0.002\n0.004\n0.518\n0.605\n\n\n\n9\nversion1\n2.082\n108.170\n0.733\n0.085\n8.583\n0.000\n***\n\n\n10\nowntaxprod\n1.359\n35.945\n0.307\n0.101\n3.048\n0.002\n**\n\n\n11\nupgraded\n2.540\n154.028\n0.932\n0.084\n11.076\n0.000\n***\n\n\n\n\n\n\n\n\n\nlr.plot(\"pred\", incl = \"zip_bins\")\n\n\n\n\n\n\n\n\n\n(\n    intuit75k.query(\"(training == 1) & (zip_bins == 1)\")\n    .groupby(\"zip5\")['res1_yes']\n    .agg([\"mean\", \"sum\", \"count\"])\n    .sort_values(by = \"sum\", ascending = False)[0:10]\n    .assign(mean = lambda x: x[\"mean\"].apply(lambda y: f\"{100 * y:.2f}%\"))\n)\n\n\n\n\n\n\n\n\n\nmean\nsum\ncount\n\n\nzip5\n\n\n\n\n\n\n\n00801\n41.12%\n486\n1182\n\n\n00804\n35.43%\n45\n127\n\n\n00000\n3.00%\n3\n100\n\n\n01923\n37.50%\n3\n8\n\n\n01890\n17.65%\n3\n17\n\n\n02050\n28.57%\n2\n7\n\n\n01950\n50.00%\n2\n4\n\n\n01752\n13.33%\n2\n15\n\n\n01754\n40.00%\n2\n5\n\n\n01863\n22.22%\n2\n9\n\n\n\n\n\n\n\n\n\n\n\nImprove the model by adding dummies for zip 00801 and 00804\n\nintuit75k = intuit75k.assign(\n    zip801 = rsm.ifelse(intuit75k[\"zip5\"] == \"00801\", 1, 0),\n    zip804 = rsm.ifelse(intuit75k[\"zip5\"] == \"00804\", 1, 0),\n)\n\n\nlr = rsm.model.logistic(\n    data = {\"intuit75k\": intuit75k.query(\"training == 1\")},\n    rvar = \"res1\",\n    lev = \"Yes\",\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ]\n)\nlr.coef[30:].round(3)\n\n\n\n\n\n\n\n\n\nindex\nOR\nOR%\ncoefficient\nstd.error\nz.value\np.value\n\n\n\n\n\n30\nzip801\n25.333\n2433.301\n3.232\n0.164\n19.746\n0.0\n***\n\n\n31\nzip804\n18.398\n1739.833\n2.912\n0.249\n11.709\n0.0\n***\n\n\n\n\n\n\n\n\n\nlr.summary(main = False, fit = False, test = \"zip_bins\")\n\n\nModel 1: res1 ~ sex + numords + bizflag + dollars + last + owntaxprod + sincepurch + version1 + upgraded + zip801 + zip804\nModel 2: res1 ~ zip_bins + sex + numords + bizflag + dollars + last + owntaxprod + sincepurch + version1 + upgraded + zip801 + zip804\nPseudo R-squared, Model 1 vs 2: 0.146 vs 0.148\nChi-squared: 30.796 df (19), p.value 0.042\n\n\n\nintuit75k[\"pred_logit\"] = lr.predict(intuit75k)[\"prediction\"]\n\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_logit\")\n\n\n\n\n\n\n\n\n\n\nAccuracy of Logistic Regression Model\n\ndef accuracy(data, actual, predicted):\n    TP = FP = TN = FN = 0\n    for i, j in zip(actual, predicted):\n        if i == 1 and j == 1:\n            TP += 1\n        elif i == 0 and j == 1:\n            FP += 1\n        elif i == 0 and j == 0:\n            TN += 1\n        elif i == 1 and j == 0:\n            FN += 1\n        else:\n            pass\n    return (TN + TP) / (TN + TP + FN + FP)"
  },
  {
    "objectID": "projects/intuit/index.html#neural-network-model",
    "href": "projects/intuit/index.html#neural-network-model",
    "title": "Intuit Upgrade Notebook",
    "section": "Neural Network Model",
    "text": "Neural Network Model\nBased on the output of the logistic regression models, we believe that employing a neural network model, that can account for underlying variable interactions, might be a better predictor than the logistic regression model. We first started by creating a simple neural network(NN) model that has 1 hidden layer with 1 node.\n\nNN with 1 Node and 1 Hidden Layer (all variables)\n\nclf0 = rsm.model.mlp(\n    data = {'intuit75k': intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (1,),\n    mod_type = 'classification'\n)\nclf0.summary()\n\nMulti-layer Perceptron (NN)\nData                 : intuit75k\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.768\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\nintuit75k[\"pred_nn0\"] = clf0.predict(intuit75k)[\"prediction\"]\n\n\nclf0.plot('vimp')\n\n\n\n\n\n\n\n\n\nclf0.plot(\"pred\")\n\n\n\n\n\n\n\n\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn0\")\n\n\n\n\n\n\n\n\n\n\nNN with 1 Node and 1 Hidden Layer (significant variables)\n\nclf1 = rsm.model.mlp(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (1,),\n    mod_type = 'classification'\n)\nclf1.summary()\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.768\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\nintuit75k[\"pred_nn1\"] = clf1.predict(intuit75k)[\"prediction\"]\n\n\nclf1.plot('vimp')\n\n\n\n\n\n\n\n\nThe permutation importance plot reveals that the geographic segmentation zip_bins, the timing of the last event (last), and the upgrade status (upgraded) are the most influential factors in predicting the yes/no response variable in your Multi-layer Perceptron model. The importance of zip_bins suggests that location-based characteristics are crucial in the model‚Äôs decision-making process. The prominence of last indicates that recent interactions or activities have a significant impact on the predictions, and the positive trend with upgraded implies that customers who have upgraded are more likely to yield a positive response.\n\nclf1.plot(\"pred\")\n\n\n\n\n\n\n\n\nThe partial dependence plots show a nuanced view of feature influences. A steeper slope for the zip_bins plot underscores its importance, aligning with the permutation importance results. Other variables like numords, dollars, version1, owntaxprod, and upgraded appear to have a more muted effect on the model‚Äôs output. Overall, the model‚Äôs predictive behavior is mostly swayed by where the customer is located, with other factors playing secondary roles.\nWe wanted to first create a base line neural network model. However, as we can see by the prediction plots, this nn model does not provide much in terms of the predictability based on various variables. We will want to build a neural network with more nodes and more hidden layers, and make our model a bit more flexible.\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn1\")\n\n\n\n\n\n\n\n\n\n\nNN with hidden layers sizes: (2, )\n\nclf2 = rsm.model.mlp(\n    data= {'wave1':intuit75k.query('training == 1')},\n    rvar=\"res1_yes\",\n    evar=[\n        'zip_bins',\n        'bizflag',\n        'sex',\n        'sincepurch',\n        'numords',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes=(2,),\n    mod_type=\"classification\",\n)\nclf2.summary()\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, bizflag, sex, sincepurch, numords, dollars, last, owntaxprod, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (2,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.777\n\nRaw data             :\nzip_bins  bizflag  sex  sincepurch  numords  dollars  last  owntaxprod  version1  upgraded  zip801  zip804\n      18        0 Male          12        2    109.5     5           0         0         0       0       0\n       1        0 Male           1        1     22.0    17           0         0         0       0       0\n       3        0 Male          17        1     20.0    17           0         0         1       0       0\n      11        1 Male          17        1     24.5     4           0         1         0       0       0\n       5        0 Male           9        3     73.5    10           0         0         0       0       0\n\nEstimation data      :\n bizflag  sincepurch   numords   dollars      last  owntaxprod  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n       0   -0.360809 -0.057079  0.207902 -1.137183           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n       0   -1.458194 -0.863720 -0.873735  0.119783           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       0    0.138003 -0.863720 -0.898458  0.119783           0         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       1    0.138003 -0.863720 -0.842831 -1.241930           0         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n       0   -0.660096  0.749561 -0.237115 -0.613447           0         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\nclf2.plot(\"vimp\")\n\n\n\n\n\n\n\n\nIn the second neural network model, which now includes two hidden layers with one node each, the permutation importance plot indicates a shift in feature influence. Zip_bins retains its position as the most impactful feature, but upgraded has risen in importance, surpassing last, which was more significant in the first model. This suggests that the model is now giving more weight to the upgrade status of a customer when predicting the outcome. Features like numords and version1 also contribute to the model‚Äôs predictions, but to a lesser extent, while dollars and owntaxprod remain less influential.\n\nclf2.plot(\"pred\")\n\n\n\n\n\n\n\n\n\nintuit75k['pred_nn2'] = clf2.predict(intuit75k)['prediction']\n\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn2\")\n\n\n\n\n\n\n\n\nThe partial dependence plots offer a more detailed look into the relationships between each feature and the model‚Äôs predictions. The zip_bins plot continues to show that lower bin values are associated with a higher probability of a positive response, reinforcing its top importance. The upgraded feature shows an upward trend, which aligns with its increased importance, suggesting that customers who have upgraded are more likely to have a positive response. Other features, such as numords, last, version1, and owntaxprod, show relatively flat trends, indicating a less significant effect on the prediction. These plots further confirm the model‚Äôs reliance on geographical segmentation and upgrade status, while time since last activity has become relatively less important compared to the first model.\nAs indicated by the prediction plots above, we see that by adding more hidden layers, our neural network become more flexible. However, the flexibility of the model can still be increased, and we will want to consider building an even more flexible model.\n\n\nNN with 1 hidden layer and 3 nodes\n\nclf3a = rsm.model.mlp(\n    data= {'wave1':intuit75k.query('training == 1')},\n    rvar=\"res1_yes\",\n    evar=[\n        'zip_bins',\n        'bizflag',\n        'sex',\n        'sincepurch',\n        'numords',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes=(3,),\n    mod_type=\"classification\",\n)\nclf3a.summary()\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, bizflag, sex, sincepurch, numords, dollars, last, owntaxprod, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (3,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.778\n\nRaw data             :\nzip_bins  bizflag  sex  sincepurch  numords  dollars  last  owntaxprod  version1  upgraded  zip801  zip804\n      18        0 Male          12        2    109.5     5           0         0         0       0       0\n       1        0 Male           1        1     22.0    17           0         0         0       0       0\n       3        0 Male          17        1     20.0    17           0         0         1       0       0\n      11        1 Male          17        1     24.5     4           0         1         0       0       0\n       5        0 Male           9        3     73.5    10           0         0         0       0       0\n\nEstimation data      :\n bizflag  sincepurch   numords   dollars      last  owntaxprod  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n       0   -0.360809 -0.057079  0.207902 -1.137183           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n       0   -1.458194 -0.863720 -0.873735  0.119783           0         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       0    0.138003 -0.863720 -0.898458  0.119783           0         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n       1    0.138003 -0.863720 -0.842831 -1.241930           0         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n       0   -0.660096  0.749561 -0.237115 -0.613447           0         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\nclf3a.plot('vimp')\n\n\n\n\n\n\n\n\nIn the third model, which is a neural network with one hidden layer and three nodes, the permutation importance plot shows a slight rearrangement in feature importance. Zip_bins continues to be the leading factor, suggesting that geographic segmentation remains a key predictor in the model. The feature upgraded retains substantial importance, affirming its strong predictive power regarding the response variable. Interestingly, version1 has increased in importance compared to the previous models, hinting that the specific version of a product or service is becoming more relevant in the model‚Äôs predictions. Last and numords follow in importance, indicating they are useful but less decisive in influencing the model‚Äôs outcome.\n\nclf3a.plot('pred')\n\n\n\n\n\n\n\n\nThe partial dependence plots reveal the following insights:\n\nZip_bins shows a sharp decline, indicating a strong relationship between lower bin values and the likelihood of a positive response.\nThe effect of numords is relatively flat, suggesting a limited influence on the predicted outcome.\nThe last feature shows a downward trend, implying that more recent interactions might correspond to a lower probability of a positive outcome, which is consistent with its position in the permutation importance plot.\nThe upgraded plot has an upward trend, reinforcing its role as an influential predictor, as higher values correlate with a greater likelihood of a positive outcome.\n\nThe consistency of zip_bins and upgraded across all models as top predictors underlines their significance in determining the response variable. The rise of version1 in importance could indicate that the model is beginning to capture more nuanced patterns as the complexity of the network increases. However, features like dollars and owntaxprod remain less impactful, suggesting that they might not be as crucial for the model‚Äôs decision-making process or that their effects are possibly being captured indirectly through interactions with other features.\n\nintuit75k['pred_nn3a'] = clf3a.predict(intuit75k)['prediction']\n\n\n\nNN with 2 nodes and 10 hidden layers\n\nclf10 = rsm.model.mlp(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    hidden_layer_sizes = (10,10),\n    mod_type = 'classification'\n)\nclf10.summary()\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (10, 10)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.838\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\n\nclf10.plot('vimp')\n\n\n\n\n\n\n\n\n\nclf10.plot('pred')\n\n\n\n\n\n\n\n\n\nintuit75k['pred_nn10'] = clf10.predict(intuit75k)['prediction']\n\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn10\")\n\n\n\n\n\n\n\n\nFor the latest neural network model with two nodes in each of ten hidden layers, the summary and permutation importance plot provide insights into its learning process. This more complex model architecture did not converge, which suggests that the optimization algorithm (likely L-BFGS) was unable to find a solution that minimally satisfies the problem constraints within the iteration limit. This might be due to an insufficient number of iterations, a need for better scaling of input data, or an overly complex model for the given data.\nThe permutation importance plot for this non-converged model indicates that zip_bins remains the most critical feature, followed by upgraded and last, which is consistent with previous models. This underscores the model‚Äôs reliance on geographic data, upgrade status, and recent activity as key determinants of the predicted outcome. Other features such as numords, version1, and dollars continue to be relevant, but their impact on the model‚Äôs predictions is less pronounced.\nWithout convergence, however, the reliability of these importance scores might be compromised. It‚Äôs also important to note that without a properly converged model, any interpretation of the feature importances and relationships should be taken with caution. Adjusting the model‚Äôs parameters or preprocessing the input data could help achieve convergence and provide more reliable insights into the importance and effects of different features on the model‚Äôs predictions.\n\n\nEvaluating Neural Network Performance from Gains Chart\n\nintuit75k.head()\n\n\n\n\n\n\n\n\n\nid\nzip5\nzip_bins\nsex\nbizflag\nnumords\ndollars\nlast\nsincepurch\nversion1\n...\ntraining\nres1_yes\nzip801\nzip804\npred_logit\npred_nn0\npred_nn1\npred_nn2\npred_nn3a\npred_nn10\n\n\n\n\n0\n1\n94553\n18\nMale\n0\n2\n109.5\n5\n12\n0\n...\n1\n0\n0\n0\n0.044161\n0.039253\n0.039253\n0.046232\n0.043931\n0.040564\n\n\n1\n2\n53190\n10\nUnknown\n0\n1\n69.5\n4\n3\n0\n...\n0\n0\n0\n0\n0.022178\n0.020067\n0.020067\n0.029727\n0.027086\n0.018918\n\n\n2\n3\n37091\n8\nMale\n0\n4\n93.0\n14\n29\n0\n...\n0\n0\n0\n0\n0.091385\n0.094208\n0.094208\n0.075088\n0.073340\n0.151937\n\n\n3\n4\n02125\n1\nMale\n0\n1\n22.0\n17\n1\n0\n...\n1\n0\n0\n0\n0.011381\n0.011572\n0.011572\n0.012489\n0.008148\n0.022096\n\n\n4\n5\n60201\n11\nMale\n0\n1\n24.5\n2\n3\n0\n...\n0\n0\n0\n0\n0.025147\n0.024115\n0.024115\n0.021093\n0.024610\n0.009841\n\n\n\n\n5 rows √ó 23 columns\n\n\n\n\n\nfig = rsm.gains_plot(intuit75k[intuit75k.training == 1],\n    \"res1\", \"Yes\",\n    [\"pred_logit\", \"pred_nn0\",\"pred_nn1\", \"pred_nn2\", \"pred_nn3a\", \"pred_nn10\"])\n\n\n\n\n\n\n\n\nThe gains chart provided compares the performance of three predictive models: a logistic regression model (pred_logit) and two neural network models (pred_nn1 and pred_nn2). The chart plots the percentage of positive outcomes (buyers) captured against the percentage of the population targeted based on the model‚Äôs predictions.\nAll models perform above the diagonal line, which represents random chance; this indicates that each model has predictive power beyond mere guessing. The chart shows that the logistic regression and the first neural network model (pred_nn1) have similar performance, capturing nearly the same percentage of buyers across the population targeted. The second neural network model (pred_nn2) appears to perform slightly better, capturing a marginally higher percentage of buyers for most of the population percentages targeted. This suggests that pred_nn2 may have a more nuanced understanding of the data, possibly due to a more complex model structure, and is able to more accurately identify potential buyers.\nHowever, the differences between the models are not drastic, which implies that the complexity added in the neural network models may not be translating into significantly better performance over the logistic regression in this case. The closeness of their performances also suggests that the relationships within the data might be captured almost as effectively by the simpler logistic regression model as by the more complex neural network models.\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", [\"pred_logit\"])\n\n\n\n\n\n\n\n\nBoth the training and test curves lie above the diagonal, indicating that the logistic regression model has learned to identify buyers more effectively than random chance. The closeness of the two lines suggests that the model generalizes well; it performs similarly on both the training data (seen data) and the test data (unseen data). This is indicative of a good model fit without overfitting, as overfitting would typically be revealed by a high performance on the training data but a significantly lower performance on the test data.\nThe chart shows that as you target more of the population based on the model‚Äôs predictions, you capture a higher percentage of buyers. Both lines show a relatively steady and linear increase, suggesting that the model ranks potential buyers effectively across the entire dataset. There‚Äôs no sharp increase at the beginning, which would suggest that there‚Äôs a subset of the population with a significantly higher likelihood of buying. Instead, the model‚Äôs predictions are spread out across the population, capturing buyers at a fairly uniform rate as more of the population is targeted.\n\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn0\")\n\n\n\n\n\n\n\n\n\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn1\")\n\n\n\n\n\n\n\n\n\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn2\")\n\n\n\n\n\n\n\n\nThis gains chart displays the performance of the second neural network model (pred_nn2) on both the training and test datasets. The chart demonstrates that the model is effective at identifying buyers, as indicated by the curves lying well above the diagonal line representing random selection.\nThe performance on the training data is very close to the performance on the test data, which suggests that the model generalizes well and is not overfitting. Overfitting would be indicated by a high performance on the training data but significantly worse performance on the test data. The fact that both curves are almost superimposed on each other indicates that the patterns the model has learned from the training data are applicable to the unseen test data.\nAdditionally, the curves are steeper at the beginning, which means that targeting a smaller percentage of the population based on the model‚Äôs predictions can capture a larger percentage of buyers. This is a desirable trait in a model used for prioritizing which segments of a population to target for marketing or intervention efforts. The model seems to rank potential buyers effectively, identifying those most likely to purchase early on as the population is targeted incrementally. This can be particularly valuable in applications where it is cost-effective to target only a subset of the entire population.\n\n\nModel Tuning\n\nfrom sklearn.model_selection import GridSearchCV\nhls = [(1,), (2,), (3,), (3, 3), (4, 2), (5, 5)]\n\nparam_grid = {\"hidden_layer_sizes\": hls}\nscoring = {\"AUC\": \"roc_auc\"}\n\nclf_cv = GridSearchCV(\n    clf0.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5\n)\nclf0.data_onehot.mean().round(3)\nclf0.data.res1_yes\nclf_cv.fit(clf0.data_onehot, clf0.data.res1_yes)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n\n\nGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5)]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') ¬†MLPClassifier?Documentation for MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') \n\n\n\npd.DataFrame(clf_cv.cv_results_).iloc[:, 5:].sort_values(\"rank_test_AUC\")\n\n\n\n\n\n\n\n\n\nparams\nsplit0_test_AUC\nsplit1_test_AUC\nsplit2_test_AUC\nsplit3_test_AUC\nsplit4_test_AUC\nmean_test_AUC\nstd_test_AUC\nrank_test_AUC\n\n\n\n\n1\n{'hidden_layer_sizes': (2,)}\n0.758255\n0.770282\n0.782662\n0.765579\n0.773062\n0.769968\n0.008090\n1\n\n\n2\n{'hidden_layer_sizes': (3,)}\n0.754724\n0.764858\n0.778597\n0.761457\n0.774358\n0.766799\n0.008654\n2\n\n\n3\n{'hidden_layer_sizes': (3, 3)}\n0.742551\n0.755067\n0.780358\n0.769203\n0.773344\n0.764104\n0.013578\n3\n\n\n0\n{'hidden_layer_sizes': (1,)}\n0.748561\n0.756562\n0.773532\n0.763431\n0.777387\n0.763895\n0.010621\n4\n\n\n4\n{'hidden_layer_sizes': (4, 2)}\n0.735937\n0.756075\n0.769769\n0.757250\n0.745491\n0.752904\n0.011456\n5\n\n\n5\n{'hidden_layer_sizes': (5, 5)}\n0.718748\n0.740427\n0.741733\n0.735261\n0.751256\n0.737485\n0.010699\n6\n\n\n\n\n\n\n\n\n\nclf_cv.best_params_\n\n{'hidden_layer_sizes': (2,)}\n\n\n\n\nRetrain model with hidden layers sizes: (3, 3)\n\nclf3 = rsm.model.mlp(\n    data = {'wave1':intuit75k[intuit75k.training == 1]},\n    rvar=\"res1_yes\",\n    evar= ['zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804' \n    ],\n    hidden_layer_sizes = (3,3),\n    mod_type = 'classification'\n)\nclf3.summary()\n\nMulti-layer Perceptron (NN)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nModel type           : classification\nNr. of features      : (12, 31)\nNr. of observations  : 52,500\nHidden_layer_sizes   : (3, 3)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.782\n\nRaw data             :\nzip_bins  sex  numords  bizflag  dollars  last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804\n      18 Male        2        0    109.5     5           0          12         0         0       0       0\n       1 Male        1        0     22.0    17           0           1         0         0       0       0\n       3 Male        1        0     20.0    17           0          17         0         1       0       0\n      11 Male        1        1     24.5     4           0          17         1         0       0       0\n       5 Male        3        0     73.5    10           0           9         0         0       0       0\n\nEstimation data      :\n  numords  bizflag   dollars      last  owntaxprod  sincepurch  version1  upgraded  zip801  zip804  zip_bins_2  zip_bins_3  zip_bins_4  zip_bins_5  zip_bins_6  zip_bins_7  zip_bins_8  zip_bins_9  zip_bins_10  zip_bins_11  zip_bins_12  zip_bins_13  zip_bins_14  zip_bins_15  zip_bins_16  zip_bins_17  zip_bins_18  zip_bins_19  zip_bins_20  sex_Male  sex_Unknown\n-0.057079        0  0.207902 -1.137183           0   -0.360809         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False         True        False        False      True        False\n-0.863720        0 -0.873735  0.119783           0   -1.458194         0         0       0       0       False       False       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        0 -0.898458  0.119783           0    0.138003         0         1       0       0       False        True       False       False       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n-0.863720        1 -0.842831 -1.241930           0    0.138003         1         0       0       0       False       False       False       False       False       False       False       False        False         True        False        False        False        False        False        False        False        False        False      True        False\n 0.749561        0 -0.237115 -0.613447           0   -0.660096         0         0       0       0       False       False       False        True       False       False       False       False        False        False        False        False        False        False        False        False        False        False        False      True        False\n\n\nAn AUC (Area Under the ROC Curve) of 0.761 means that the model has a good ability to distinguish between the positive and negative classes. In other words, when the model predicts whether a new, unseen instance belongs to one class or the other, it is correct 76.1% of the time.\n\nclf3.plot('vimp')\n\n\n\n\n\n\n\n\nIn this plot, zip_bins appears to be the most important feature, followed by upgraded and last. Numords and Version1 also have an impact on the model, but not as much as the preceding 3. The length of the bars indicates that zip_bins has the highest impact on model performance, whereas dollars and owntaxprod have the least impact among the shown features.\n\nintuit75k['pred_nn3'] = clf3.predict(intuit75k)['prediction']\n\n\ndct = {\"train\": intuit75k[intuit75k.training == 1], \"test\": intuit75k[intuit75k.training == 0]}\nfig = rsm.gains_plot(dct, \"res1\", \"Yes\", \"pred_nn3\")\n\n\n\n\n\n\n\n\nBoth lines exhibit a steep curve towards the lower percentage of the population targeted, indicating that the model is effectively ranking buyers higher than non-buyers. The performance on the training set is better than on the test set, which is typical as models tend to perform better on the data they were trained on. However, since the test line follows closely to the training line, it suggests that the model generalizes well and is not overfitting significantly.\n\nfrom sklearn import metrics\n\n# prediction on training set\npred = intuit75k.loc[intuit75k.training == 1, \"pred_nn3\"]\nactual = intuit75k.loc[intuit75k.training == 1, \"res1_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.782\n\n\nAn AUC (Area Under the ROC Curve) of 0.761 means that the model has a good ability to distinguish between the positive and negative classes. In other words, when the model predicts whether a new, unseen instance belongs to one class or the other, it is correct 76.1% of the time.\n\n# prediction on test set\npred = intuit75k.loc[intuit75k.training == 0, \"pred_nn3\"]\nactual = intuit75k.loc[intuit75k.training == 0, \"res1_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.762\n\n\nAn AUC (Area Under the ROC Curve) of 0.755 means that the model has a good ability to distinguish between the positive and negative classes. In other words, when the model predicts whether a new, unseen instance belongs to one class or the other, it is correct 75.5% of the time.\nThe difference between the training AUC and the test AUC is 0.007, which is very small. This means that the model is not overfitting.\n\nfig = rsm.gains_plot(\n    intuit75k[intuit75k.training == 1],\n    \"res1\", \"Yes\",\n    [\"pred_logit\", \"pred_nn3\"]\n)\n\n\n\n\n\n\n\n\n\nInterpretation of model tuning\nBoth curves in the graphs are increasing and following a similar trends, which suggests that the models are likely effective at targeting buyers. The distance between the two curves suggests that the neural network model outperforming the logistic regression model.Let‚Äôs try to improve the logistic regression model by checking the interactons between the variables.\n\nInteractions: version 1, numords and last\n\nclf3.plot('pred', incl = [], incl_int = [\"last:version1\", \"numords:version1\"])\n\n\n\n\n\n\n\n\n\nlr.plot('pred', incl = [], incl_int = [\"last:version1\", \"numords:version1\"])\n\n\n\n\n\n\n\n\n\nlr_int = rsm.model.logistic(\n    data = {'wave1':intuit75k.query('training == 1')},\n    rvar = 'res1_yes',\n    evar = [\n        'zip_bins',\n        'sex',\n        'numords',\n        'bizflag',\n        'dollars',\n        'last',\n        'owntaxprod',\n        'sincepurch',\n        'version1',\n        'upgraded',\n        'zip801',\n        'zip804'\n    ],\n    ivar = [\"numords:version1\", \"last:version1\"]\n)\nlr_int.summary()\n\nLogistic regression (GLM)\nData                 : wave1\nResponse variable    : res1_yes\nLevel                : None\nExplanatory variables: zip_bins, sex, numords, bizflag, dollars, last, owntaxprod, sincepurch, version1, upgraded, zip801, zip804\nNull hyp.: There is no effect of x on res1_yes\nAlt. hyp.: There is an effect of x on res1_yes\n\n                      OR      OR%  coefficient  std.error  z.value p.value     \nIntercept          0.021   -97.9%        -3.85      0.176  -21.910  &lt; .001  ***\nzip_bins[2]        1.296    29.6%         0.26      0.181    1.436   0.151     \nzip_bins[3]        1.034     3.4%         0.03      0.187    0.181   0.857     \nzip_bins[4]        1.180    18.0%         0.17      0.182    0.912   0.362     \nzip_bins[5]        1.015     1.5%         0.01      0.187    0.078   0.938     \nzip_bins[6]        1.099     9.9%         0.09      0.183    0.514   0.607     \nzip_bins[7]        1.055     5.5%         0.05      0.185    0.287   0.774     \nzip_bins[8]        1.118    11.8%         0.11      0.183    0.608   0.543     \nzip_bins[9]        1.074     7.4%         0.07      0.185    0.388   0.698     \nzip_bins[10]       1.060     6.0%         0.06      0.185    0.314   0.753     \nzip_bins[11]       1.122    12.2%         0.12      0.184    0.627   0.531     \nzip_bins[12]       1.504    50.4%         0.41      0.177    2.307   0.021    *\nzip_bins[13]       0.985    -1.5%        -0.02      0.188   -0.081   0.936     \nzip_bins[14]       1.129    12.9%         0.12      0.183    0.664   0.507     \nzip_bins[15]       1.019     1.9%         0.02      0.186    0.102   0.919     \nzip_bins[16]       1.123    12.3%         0.12      0.183    0.633   0.527     \nzip_bins[17]       1.119    11.9%         0.11      0.183    0.614   0.539     \nzip_bins[18]       1.604    60.4%         0.47      0.175    2.695   0.007   **\nzip_bins[19]       1.243    24.3%         0.22      0.181    1.201    0.23     \nzip_bins[20]       1.087     8.7%         0.08      0.184    0.451   0.652     \nsex[Male]          0.992    -0.8%        -0.01      0.055   -0.154   0.878     \nsex[Unknown]       0.964    -3.6%        -0.04      0.078   -0.471   0.638     \nnumords            1.162    16.2%         0.15      0.022    6.700  &lt; .001  ***\nbizflag            1.048     4.8%         0.05      0.050    0.923   0.356     \ndollars            1.001     0.1%         0.00      0.000    4.208  &lt; .001  ***\nlast               0.966    -3.4%        -0.03      0.003  -12.244  &lt; .001  ***\nowntaxprod         1.496    49.6%         0.40      0.105    3.849  &lt; .001  ***\nsincepurch         1.002     0.2%         0.00      0.004    0.531   0.595     \nversion1           1.474    47.4%         0.39      0.151    2.572    0.01    *\nupgraded           2.719   171.9%         1.00      0.088   11.371  &lt; .001  ***\nzip801            25.908  2490.8%         3.25      0.164   19.831  &lt; .001  ***\nzip804            18.400  1740.0%         2.91      0.250   11.661  &lt; .001  ***\nnumords:version1   1.397    39.7%         0.33      0.035    9.442  &lt; .001  ***\nlast:version1      0.959    -4.1%        -0.04      0.006   -7.089  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.156\nPseudo R-squared (McFadden adjusted): 0.153\nArea under the RO Curve (AUC): 0.774\nLog-likelihood: -8479.762, AIC: 17027.524, BIC: 17329.055\nChi-squared: 3130.12, df(33), p.value &lt; 0.001 \nNr obs: 52,500\n\n\nThe output shows the summary of a logistic regression model indicating that all included explanatory variables and their interactions (version1:zip_bins, zip_bins:upgraded, owntaxprod:zip_bins) are statistically significant in predicting the response variable res1_yes, with the model achieving an AUC of 0.717, which suggests a good predictive performance.\n\nintuit75k['pred_logit_int'] = lr_int.predict(intuit75k)['prediction']\n\n\nfig = rsm.gains_plot(\n    intuit75k[intuit75k.training == 1],\n    \"res1\", \"Yes\",\n    [\"pred_logit_int\", \"pred_nn2\"]\n)\n\n\n\n\n\n\n\n\nBoth models show an increasing trend, meaning they are effectively ranking individuals more likely to be positive cases higher. The pred_logit_int model performs slightly better than pred_nn3, particularly at lower percentages of the population targeted, indicating a more efficient prioritization of likely positive cases by the logistic regression model with interactions (pred_logit_int).\n\n\n\n\nCalculating the Accuracy of the models\n\ndef classification(actual, predicted):\n    TP = FP = TN = FN = 0\n    for i, j in zip(actual, predicted):\n        if i == 1 and j == 1:\n            TP += 1\n        elif i == 0 and j == 1:\n            FP += 1\n        elif i == 0 and j == 0:\n            TN += 1\n        elif i == 1 and j == 0:\n            FN += 1\n    metrics = {\n        \"Accuracy\": (TN + TP) / (TN + TP + FN + FP) if (TN + TP + FN + FP) else 0, \n        \"Precision\": TP / (TP + FP) if (TP + FP) else 0,\n        \"Recall\": TP / (TP + FN) if (TP + FN) else 0\n    }\n    return metrics\n\ndef prediction_metrics(data, models, predictions, break_even):\n    results = []\n    for model, prediction in zip(models, predictions):\n        data['prediction'] = [1 if i &gt; break_even else 0 for i in data[prediction]]\n        actual = data['res1_yes']\n        predicted = data['prediction']\n        metrics = classification(actual, predicted)\n        metrics['Model'] = model  # Add model name to metrics\n        results.append(metrics)\n    column_order = ['Model', 'Accuracy', 'Precision', 'Recall']\n    results_df = pd.DataFrame(results, columns=column_order)\n    return results_df\n\n\n# Find break even\ncost = 1.41\nmargin = 60\nbreak_even = cost / margin\nbreak_even\n\n0.0235\n\n\n\ntest_data = intuit75k[intuit75k.training == 0]\ntrue_response = test_data['res1_yes']\n\n\n# Run the function accross all models and predictions\nmodels = [\"logit\", \"nn0\", \"nn1\", \"nn2\", \"nn3a\", \"nn10\"]\npredictions = [\n    \"pred_logit\",\n    \"pred_nn0\",\n    \"pred_nn1\",\n    \"pred_nn2\",\n    \"pred_nn3a\",\n    \"pred_nn10\",\n    \"pred_logit_int\",\n]\n\n\npred_metrics = prediction_metrics(test_data, models,predictions, break_even)\npred_metrics\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1757352677.py:22: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\n\n\n\n\n0\nlogit\n0.429689\n0.070272\n0.869447\n\n\n1\nnn0\n0.460311\n0.072358\n0.846782\n\n\n2\nnn1\n0.460311\n0.072358\n0.846782\n\n\n3\nnn2\n0.475822\n0.074030\n0.842248\n\n\n4\nnn3a\n0.465156\n0.073707\n0.856754\n\n\n5\nnn10\n0.574844\n0.078410\n0.713509"
  },
  {
    "objectID": "projects/intuit/index.html#calculate-the-profit-of-each-model",
    "href": "projects/intuit/index.html#calculate-the-profit-of-each-model",
    "title": "Intuit Upgrade Notebook",
    "section": "Calculate the profit of each model",
    "text": "Calculate the profit of each model\nNote that in wave 2, every response probability is only 50%\n\nfor prediction in predictions:\n    test_data[f'mailto_wave2_{prediction}'] = test_data[prediction] &gt; break_even\n\n\ndef calculate_performance(dat, models, predictions, perf = \"res1_yes\", break_even = break_even, lev=1):\n    results = []\n    for model, prediction in zip(models, predictions):\n        # Use the boolean column indicating if prediction is above break_even\n        boolean_column = f'mailto_wave2_{prediction}'\n        mail_to_rate = len(dat[dat[boolean_column]]) / len(dat)\n        response_rate = (len(dat[(dat[boolean_column]) & (dat[perf] == lev)]) / len(dat[dat[boolean_column]]))/2 if len(dat[dat[boolean_column]]) else 0\n        nr_mail = 763334 * mail_to_rate\n        mail_cost = 1.4 * nr_mail\n        revenue = 60 * nr_mail * response_rate\n        expected_profit = revenue - mail_cost\n        expected_ROME = expected_profit / mail_cost if mail_cost else 0\n        results.append({\n            \"model\": model,\n            \"profit\": expected_profit,\n            \"ROME\": expected_ROME,\n            \"mail_to_rate\": mail_to_rate,\n            \"response_rate\": response_rate\n        })\n    \n    return pd.DataFrame(results)\n\n\nperformance_data = calculate_performance(test_data, models, predictions, perf = \"res1_yes\", break_even = break_even, lev=1)\nperformance_data\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/1120089769.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\nmodel\nprofit\nROME\nmail_to_rate\nresponse_rate\n\n\n\n\n0\nlogit\n327867.219680\n0.505825\n0.606533\n0.035136\n\n\n1\nnn0\n337522.546631\n0.550533\n0.573689\n0.036179\n\n\n2\nnn1\n337522.546631\n0.550533\n0.573689\n0.036179\n\n\n3\nnn2\n349484.838560\n0.586353\n0.557733\n0.037015\n\n\n4\nnn3a\n352850.293351\n0.579440\n0.569822\n0.036854\n\n\n5\nnn10\n324271.068391\n0.680212\n0.446089\n0.039205"
  },
  {
    "objectID": "projects/intuit/index.html#calculate-the-profit-which-is-scale-to-801821---38487-763334-businesses",
    "href": "projects/intuit/index.html#calculate-the-profit-which-is-scale-to-801821---38487-763334-businesses",
    "title": "Intuit Upgrade Notebook",
    "section": "Calculate the profit, which is scale to 801821 - 38487 = 763334 businesses",
    "text": "Calculate the profit, which is scale to 801821 - 38487 = 763334 businesses\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'serif'  # e.g., 'Times New Roman', 'Arial', 'Serif'\nplt.rcParams['font.size'] = 12\nplt.figure(figsize=(12, 8))\nplt.clf()\nax = sns.barplot(x=\"model\", y=\"profit\", data=performance_data, palette=\"inferno\")\nax.set(xlabel=\"Model Type\", ylabel=\"Profit ($)\")\nax.set_title('Model Performance by Profit', fontsize=16, fontweight='bold')\n\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width() / 2., (p.get_height() - 1), f\"${int(p.get_height()):,}\", \n            ha='center', va='bottom', color='black')\n\nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/3822392447.py:8: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nplt.rcParams['font.family'] = 'serif'  # e.g., 'Times New Roman', 'Arial', 'Serif'\nplt.rcParams['font.size'] = 10\nplt.figure(figsize=(12, 8))\nplt.clf()\nax = sns.barplot(x=\"model\", y=\"ROME\", data=performance_data, palette=\"inferno\")\nax.set(xlabel=\"Model Type\", ylabel=\"ROME (%)\")\nax.set_title('Model Performance by ROME', fontsize=16, fontweight='bold')\n\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width() / 2., (p.get_height()), f\"{(p.get_height()):.2f}%\", \n            ha='center', va='bottom', color='black')\n\nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5280/2081715165.py:5: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nBased on the results above, the profit of NN3 is highest, this means that the NN3 model is the best model to use in this case."
  },
  {
    "objectID": "projects/cg_prop/index.html",
    "href": "projects/cg_prop/index.html",
    "title": "CG Propensity",
    "section": "",
    "text": "CG has tasked me with developing a predictive model utilizing the ‚ÄúSpace Pirates‚Äù game data. The primary objective is to identify which gamers are more or less inclined to purchase the ‚ÄúZalon‚Äù campaign. This involves analyzing player behavior, engagement levels, and other relevant metrics to forecast their purchasing decisions accurately.\n\n\nTo achieve the objective, the following predictive analytics models are being considered and implemented throughout the campaign:\n\nLogistic Regression Model: This model is used for binary classification tasks. In this context, it will predict the likelihood of a gamer purchasing the Zalon campaign, categorizing predictions into two groups: likely buyers and unlikely buyers.\nNeural Network Model: This model, known for its ability to capture complex nonlinear relationships, is employed to understand deeper patterns in gamer behavior that may influence their decision to purchase. It leverages layers of processing units to learn from the data, offering a more nuanced prediction compared to traditional models.\nRandom Forest Model: A robust ensemble learning method that uses multiple decision trees to make predictions. It is particularly useful for handling large datasets with numerous variables, providing insights into the importance of different features affecting the purchasing decision.\n\nEach of these models offers unique strengths in data analysis and prediction accuracy. By comparing their performance, we aim to identify the most effective approach to predict gamer behavior regarding the Zalon campaign purchase decision.\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pyrsm as rsm \nimport numpy as np \nfrom scipy import stats\nfrom sklearn.preprocessing import power_transform\n\n\n\n\nCode\n# load a custom python module\n#from utils.functions import example\n\n\n## setup functions for autoreload\n#%reload_ext autoreload\n#%autoreload 2\n#%aimport utils.functions"
  },
  {
    "objectID": "projects/cg_prop/index.html#section",
    "href": "projects/cg_prop/index.html#section",
    "title": "CG Propensity",
    "section": "",
    "text": "CG has tasked me with developing a predictive model utilizing the ‚ÄúSpace Pirates‚Äù game data. The primary objective is to identify which gamers are more or less inclined to purchase the ‚ÄúZalon‚Äù campaign. This involves analyzing player behavior, engagement levels, and other relevant metrics to forecast their purchasing decisions accurately.\n\n\nTo achieve the objective, the following predictive analytics models are being considered and implemented throughout the campaign:\n\nLogistic Regression Model: This model is used for binary classification tasks. In this context, it will predict the likelihood of a gamer purchasing the Zalon campaign, categorizing predictions into two groups: likely buyers and unlikely buyers.\nNeural Network Model: This model, known for its ability to capture complex nonlinear relationships, is employed to understand deeper patterns in gamer behavior that may influence their decision to purchase. It leverages layers of processing units to learn from the data, offering a more nuanced prediction compared to traditional models.\nRandom Forest Model: A robust ensemble learning method that uses multiple decision trees to make predictions. It is particularly useful for handling large datasets with numerous variables, providing insights into the importance of different features affecting the purchasing decision.\n\nEach of these models offers unique strengths in data analysis and prediction accuracy. By comparing their performance, we aim to identify the most effective approach to predict gamer behavior regarding the Zalon campaign purchase decision.\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pyrsm as rsm \nimport numpy as np \nfrom scipy import stats\nfrom sklearn.preprocessing import power_transform\n\n\n\n\nCode\n# load a custom python module\n#from utils.functions import example\n\n\n## setup functions for autoreload\n#%reload_ext autoreload\n#%autoreload 2\n#%aimport utils.functions"
  },
  {
    "objectID": "projects/cg_prop/index.html#data-description",
    "href": "projects/cg_prop/index.html#data-description",
    "title": "CG Propensity",
    "section": "Data Description",
    "text": "Data Description\n\n\nCode\n## loading the data\ncg_organic = pd.read_parquet(\"./data/cg_organic.parquet\")"
  },
  {
    "objectID": "projects/cg_prop/index.html#creative-gaming",
    "href": "projects/cg_prop/index.html#creative-gaming",
    "title": "CG Propensity",
    "section": "Creative gaming",
    "text": "Creative gaming\nGame telemetry dataset used for the Creative Gaming: Propensity-to-Buy Modeling case\n\nFeature descriptions\n\nconverted: Purchased the Zalon campain (‚Äúyes‚Äù or ‚Äúno‚Äù)\nGameLevel: Highest level of game achieved by the user\nNumGameDays: Number of days user played the game in last month (with or without network connection)\nNumGameDays4Plus: Number of days user played the game in last month with 4 or more total users (this implies using a network connection)\nNumInGameMessagesSent: Number of in-game messages sent to friends\nNumFriends: Number of friends to which the user is connected (necessary to crew together in multiplayer mode)\nNumFriendRequestIgnored: Number of friend requests this user has not replied to since game inception\nNumSpaceHeroBadges: Number of ‚ÄúSpace Hero‚Äù badges, the highest distinction for gameplay in Space Pirates\nAcquiredSpaceship: Flag if the user owns a spaceship, i.e., does not have to crew on another user‚Äôs or NPC‚Äôs space ship (‚Äúno‚Äù or ‚Äúyes‚Äù)\nAcquiredIonWeapon: Flag if the user owns the powerful ‚Äúion weapon‚Äù (‚Äúno‚Äù or ‚Äúyes‚Äù)\nTimesLostSpaceship: The number of times the user destroyed his/her spaceship during gameplay. Spaceships need to be re-acquired if destroyed.\nTimesKilled: Number of times the user was killed during gameplay\nTimesCaptain: Number of times in last month that the user played in the role of a captain\nTimesNavigator: Number of times in last month that the user played in the role of a navigator\nPurchasedCoinPackSmall: Flag if the user purchased a small pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nPurchasedCoinPackLarge: Flag if the user purchased a large pack of Zathium in last month (‚Äúno‚Äù or ‚Äúyes‚Äù)\nNumAdsClicked: Number of in-app ads the user has clicked on\nDaysUser: Number of days since user established a user ID with Creative Gaming (for Space Pirates or previous games)\nUserConsole: Flag if the user plays Creative Gaming games on a console (‚Äúno‚Äù or ‚Äúyes‚Äù)\nUserHasOldOS: Flag if the user has iOS version 10 or earlier (‚Äúno‚Äù or ‚Äúyes‚Äù)\nrnd_30k: Dummy variable that randomly selects 30K customers (1) and the remaining 90K (0)"
  },
  {
    "objectID": "projects/cg_prop/index.html#part-1",
    "href": "projects/cg_prop/index.html#part-1",
    "title": "CG Propensity",
    "section": "Part 1",
    "text": "Part 1\n\n\nCode\ncg_organic[\"converted_yes\"] = cg_organic[\"converted\"].map({\"yes\": 1, \"no\": 0})\n\n\nWe want to turn the yes‚Äôs and no‚Äôs into integers to allow for us to take the average of the converted column and calculate the average probability of converting. To do this, we use the map function to map the yes‚Äôs and no‚Äôs numerically represent.\n\nWhat is the probability of organically converting to Zalon\n\n\nCode\n#prob_converting = cg_organic[\"converted_1_0\"].mean()\nprob_converting = cg_organic[\"converted\"].value_counts(normalize=True)\nprob_converting = prob_converting[\"yes\"]\nprint(prob_converting)\n\n\n0.05753333333333333\n\n\nWe can find the probability of organically converting to Zalon by taking the average of the coverted_yes column in the cq_organic cg_organicbase. This probability is 5.8%.\n\n\nBasic summary statistics for each feature\n\n\nCode\ncg_organic.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30000 entries, 34184 to 51934\nData columns (total 22 columns):\n #   Column                   Non-Null Count  Dtype   \n---  ------                   --------------  -----   \n 0   converted                30000 non-null  category\n 1   GameLevel                30000 non-null  int32   \n 2   NumGameDays              30000 non-null  int32   \n 3   NumGameDays4Plus         30000 non-null  int32   \n 4   NumInGameMessagesSent    30000 non-null  int32   \n 5   NumSpaceHeroBadges       30000 non-null  int32   \n 6   NumFriendRequestIgnored  30000 non-null  int32   \n 7   NumFriends               30000 non-null  int32   \n 8   AcquiredSpaceship        30000 non-null  category\n 9   AcquiredIonWeapon        30000 non-null  category\n 10  TimesLostSpaceship       30000 non-null  int32   \n 11  TimesKilled              30000 non-null  int32   \n 12  TimesCaptain             30000 non-null  int32   \n 13  TimesNavigator           30000 non-null  int32   \n 14  PurchasedCoinPackSmall   30000 non-null  category\n 15  PurchasedCoinPackLarge   30000 non-null  category\n 16  NumAdsClicked            30000 non-null  int32   \n 17  DaysUser                 30000 non-null  int32   \n 18  UserConsole              30000 non-null  category\n 19  UserHasOldOS             30000 non-null  category\n 20  training                 30000 non-null  int32   \n 21  converted_yes            30000 non-null  category\ndtypes: category(8), int32(14)\nmemory usage: 2.1+ MB\n\n\nUsing the cg_organic.info() code, we are able to populate a summary overview of the variables that are present in the cq_organic database. As we can see, there seems to be no missing values, which allows us to begin our exploratory analytics right away without needing to conduct cg_organic cleaning. In addition, info output provides an overview of what type of variable each of the columns are, enabling us to manipulate the types if needed. For example, we created a new column called converted_yes to calculate the average conversion rate of cg gaming organic users.\n\n\nCode\ncg_organic.describe()\n\n\n\n\n\n\n\n\n\n\nGameLevel\nNumGameDays\nNumGameDays4Plus\nNumInGameMessagesSent\nNumSpaceHeroBadges\nNumFriendRequestIgnored\nNumFriends\nTimesLostSpaceship\nTimesKilled\nTimesCaptain\nTimesNavigator\nNumAdsClicked\nDaysUser\ntraining\n\n\n\n\ncount\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n\n\nmean\n6.246733\n12.241400\n1.257267\n73.777400\n0.443933\n29.589100\n47.733833\n4.435567\n0.293400\n1.583033\n1.403967\n9.494600\n2626.371933\n0.700000\n\n\nstd\n2.774055\n7.097305\n3.191210\n107.441593\n1.517762\n33.990783\n94.329939\n11.546392\n3.415129\n8.770534\n7.950200\n7.395841\n661.428007\n0.458265\n\n\nmin\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n244.000000\n0.000000\n\n\n25%\n4.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n4.000000\n2162.000000\n0.000000\n\n\n50%\n7.000000\n13.000000\n0.000000\n26.000000\n0.000000\n16.000000\n5.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2557.000000\n1.000000\n\n\n75%\n9.000000\n18.000000\n0.000000\n112.000000\n0.000000\n53.000000\n43.000000\n4.000000\n0.000000\n0.000000\n0.000000\n12.000000\n3105.000000\n1.000000\n\n\nmax\n10.000000\n28.000000\n24.000000\n1227.000000\n12.000000\n121.000000\n486.000000\n298.000000\n178.000000\n429.000000\n545.000000\n38.000000\n4139.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\nCode\n# Non-numeric columns\nnon_numeric_columns = cg_organic.select_dtypes(include=['object', 'category']).columns\n\n\n\n\nCode\n# Find the number of Distinct values in each column\ncg_organic[non_numeric_columns].nunique()\n\n\nconverted                 2\nAcquiredSpaceship         2\nAcquiredIonWeapon         2\nPurchasedCoinPackSmall    2\nPurchasedCoinPackLarge    2\nUserConsole               2\nUserHasOldOS              2\nconverted_yes             2\ndtype: int64\n\n\n\n\nCode\n# Find the most common levels in each column\ncg_organic[non_numeric_columns].apply(lambda x: x.value_counts().idxmax())\n\n\nconverted                  no\nAcquiredSpaceship          no\nAcquiredIonWeapon          no\nPurchasedCoinPackSmall     no\nPurchasedCoinPackLarge     no\nUserConsole               yes\nUserHasOldOS               no\nconverted_yes               0\ndtype: object\n\n\n\n\nGenerate histograms for all numeric variables and frequency plots for non numeric variables\n\n\nCode\ncg_organic.hist(figsize=(15, 15))\n\n\narray([[&lt;Axes: title={'center': 'GameLevel'}&gt;,\n        &lt;Axes: title={'center': 'NumGameDays'}&gt;,\n        &lt;Axes: title={'center': 'NumGameDays4Plus'}&gt;,\n        &lt;Axes: title={'center': 'NumInGameMessagesSent'}&gt;],\n       [&lt;Axes: title={'center': 'NumSpaceHeroBadges'}&gt;,\n        &lt;Axes: title={'center': 'NumFriendRequestIgnored'}&gt;,\n        &lt;Axes: title={'center': 'NumFriends'}&gt;,\n        &lt;Axes: title={'center': 'TimesLostSpaceship'}&gt;],\n       [&lt;Axes: title={'center': 'TimesKilled'}&gt;,\n        &lt;Axes: title={'center': 'TimesCaptain'}&gt;,\n        &lt;Axes: title={'center': 'TimesNavigator'}&gt;,\n        &lt;Axes: title={'center': 'NumAdsClicked'}&gt;],\n       [&lt;Axes: title={'center': 'DaysUser'}&gt;,\n        &lt;Axes: title={'center': 'training'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nThe histograms above show the distribution of each variable present in our dataset. This is a critical step within the data preprocessing part of building a model. We can use the distributions to identify skew in our variables, and transform the variables so they are normally distributed.\nFrom the histogram above, we see many variables in this dataset are heavily skewed to the left. DaysUser and GameLevel are the only variables with a slight right skew. Because of the skews in the variables, this will give us an inaccurate model if we were to use the variables with their current distributions. Therefore, we need to transform the variables based on the skew they are exhibiting in the histograms. To correct the heavy left skew that is present in DaysUser and GameLevel, Before we begin building any models, we will want to transform these variables.\nBecause of the heavy skew present in the columns in the data, we will want to transform them to normalize their distribution. Logistic regression does not assume a normal distribution of the independent variables, however, transformations can still help in stabilizing the variance and making relationships more linear. Therefore, we will want to consider transforming the variables to create a more linear relationship.\n\nPlotting the frequency of the non-numeric variables\n\n\nCode\nfor var in non_numeric_columns:\n    cg_organic[var].value_counts().plot(kind='bar', figsize=(5, 5))\n    plt.title(f'Frequency plot of {var}')\n    plt.show()"
  },
  {
    "objectID": "projects/cg_prop/index.html#part-ii-predictive-model",
    "href": "projects/cg_prop/index.html#part-ii-predictive-model",
    "title": "CG Propensity",
    "section": "Part II: Predictive Model",
    "text": "Part II: Predictive Model\n\n\nCode\ncg_train = cg_organic[cg_organic[\"training\"] == 1]\n\n\n\n\nCode\nlr = rsm.logistic(\n    data = {'CG Train Data': cg_train},\n    rvar = \"converted\",\n    lev = \"yes\",\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ],\n)\nlr.summary()\n\n\nLogistic regression (GLM)\nData                 : CG Train Data\nResponse variable    : converted\nLevel                : yes\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nNull hyp.: There is no effect of x on converted\nAlt. hyp.: There is an effect of x on converted\n\n                                OR     OR%  coefficient  std.error  z.value p.value     \nIntercept                    0.009  -99.1%        -4.75      0.190  -24.997  &lt; .001  ***\nAcquiredSpaceship[yes]       1.713   71.3%         0.54      0.072    7.486  &lt; .001  ***\nAcquiredIonWeapon[yes]       1.613   61.3%         0.48      0.203    2.353   0.019    *\nPurchasedCoinPackSmall[yes]  0.912   -8.8%        -0.09      0.070   -1.316   0.188     \nPurchasedCoinPackLarge[yes]  1.270   27.0%         0.24      0.073    3.257   0.001   **\nUserConsole[yes]             1.203   20.3%         0.19      0.091    2.025   0.043    *\nUserHasOldOS[yes]            0.847  -15.3%        -0.17      0.124   -1.341    0.18     \nGameLevel                    1.109   10.9%         0.10      0.014    7.380  &lt; .001  ***\nNumGameDays                  1.034    3.4%         0.03      0.005    6.058  &lt; .001  ***\nNumGameDays4Plus             1.042    4.2%         0.04      0.009    4.648  &lt; .001  ***\nNumInGameMessagesSent        1.001    0.1%         0.00      0.000    3.961  &lt; .001  ***\nNumFriends                   1.001    0.1%         0.00      0.000    4.201  &lt; .001  ***\nNumFriendRequestIgnored      0.988   -1.2%        -0.01      0.001   -9.181  &lt; .001  ***\nNumSpaceHeroBadges           1.519   51.9%         0.42      0.013   31.960  &lt; .001  ***\nTimesLostSpaceship           0.961   -3.9%        -0.04      0.005   -7.624  &lt; .001  ***\nTimesKilled                  0.984   -1.6%        -0.02      0.017   -0.940   0.347     \nTimesCaptain                 0.997   -0.3%        -0.00      0.004   -0.873   0.383     \nTimesNavigator               0.977   -2.3%        -0.02      0.007   -3.608  &lt; .001  ***\nNumAdsClicked                1.027    2.7%         0.03      0.004    7.183  &lt; .001  ***\nDaysUser                     1.000   -0.0%        -0.00      0.000   -0.135   0.893     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.184\nPseudo R-squared (McFadden adjusted): 0.18\nArea under the RO Curve (AUC): 0.82\nLog-likelihood: -3719.041, AIC: 7478.081, BIC: 7637.127\nChi-squared: 1682.617, df(19), p.value &lt; 0.001 \nNr obs: 21,000\n\n\nBased on the logistic regression summary output above, we can use the odd‚Äôs ratio, p_value\nThe odds ratio is the exponentiation of the logistic regression coefficient and represents the change in odds resulting from a one-unit change in the independent variable. An odds ratio greater than 1 indicates an increase in odds with a one-unit increase in teh independent variable (conversion), while an odds ratio less than 1 indicates a decrease. Variables with small p-values and odds ratios significantly different from 1 (either much greater than 1 or less than 1) are generally considered the most important, as they indicate a statistically significant effect on the dependent variable. Based on the output above, we see that the variables significantly different than 1 and small p-values are:\n\n\n`` - OR: 1.261, p-value less than 0.001\nNumFriends\n\nNumFriendRequestIgnored\n\nNumSpaceHeroBadges\nAcquiredSpaceship_1_0\nPurchasedCoinPackLarge_1_0\n\n\n\n\nCode\nlr.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\nWhat is the most 5 important variables in the model?\n\n\nCode\nlr.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\nThe bar chart above showing the permutation importance of various features or variables. Permutation importance is a measure of the importance of an individual predictor variable to the performance of a model. It‚Äôs calculated by observing how random reordering (or permuting) of the feature‚Äôs values affects the model performance; the idea is that a more important feature will have a more significant effect on model performance when its values are permuted.\nBased on the chart, here are the top 5 most important variables, listed in descending order of importance:\nNumSpaceHeroBadges NumAdsClicked NumFriendRequestsIgnored TimesLostSpaceship GameLevel\n\n\nGenerate prediction plots for each of these 5 features\n\n\nCode\nlr.plot(\"pred\", incl = [\"NumSpaceHeroBadges\", \"NumFriendRequestIgnored\", \"TimesLostSpaceship\", \"GameLevel\", \"NumFriends\"])\n\n\n\n\n\n\n\n\n\n\n\nCreate a new variable pred_logit\n\n\nCode\ncg_organic['pred_logit'] = lr.predict(cg_organic)['prediction']\n\n\n\n\nPlot gains curves for both traning and test set\n\n\nCode\ndct = {\"train\": cg_organic[cg_organic['training'] == 1], \"test\": cg_organic[cg_organic['training'] == 0]}\nfig = rsm.gains_plot(dct, \"converted\", \"yes\", \"pred_logit\")\n\n\n\n\n\n\n\n\n\nBoth the training and test lines follow a similar trajectory, which indicates good generalization of the model from training to unseen data. This is evident because there‚Äôs not a significant gap between the two lines, suggesting the model is not overfitting to the training data.\nThe rapid increase at the start of both curves suggests that the model is effective in identifying buyers when a small percentage of the population is targeted. As the percentage of the population targeted increases, the gain in percentage of buyers starts to plateau, indicating diminishing returns ‚Äì targeting additional individuals beyond a certain point yields fewer new buyers.\nIn summary, this gain chart shows a model that performs well on both the training and test sets, with good generalization and effective identification of individuals who are likely buyers, particularly within certain segments of the population\n\n\nReport AUC-ROC score of the model\n\n\nCode\nfrom sklearn import metrics\n\n# prediction on training set\npred = cg_organic[cg_organic['training'] == 1]['pred_logit']\nactual = cg_organic[cg_organic['training'] == 1]['converted_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n\n0.82\n\n\n\n\nCode\n# prediction on test set\npred = cg_organic[cg_organic['training'] == 0]['pred_logit']\nactual = cg_organic[cg_organic['training'] == 0]['converted_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n\n0.803\n\n\nWe can conclude that:\nThe AUC for the training set is 0.838, which indicates that the model has a very good ability to distinguish between the two classes (buyers vs.¬†non-buyers, for example) in the training data.\nThe AUC for the test set is 0.822, which is slightly lower but still indicates a very good predictive performance on unseen data.\nThe fact that the test AUC is close to the training AUC suggests that the model is generalizing well and not overfitting significantly to the training data. A small decrease from training to test set performance is normal because models will usually perform slightly better on the data they were trained on."
  },
  {
    "objectID": "projects/cg_prop/index.html#part-3",
    "href": "projects/cg_prop/index.html#part-3",
    "title": "CG Propensity",
    "section": "Part 3",
    "text": "Part 3\n\n\nCode\n# Read the data organic control group\ncg_organic_control = pd.read_parquet('./data/cg_organic_control.parquet')\ngroup1 = cg_organic_control\nrsm.md(\"./data/cg_organic_control_description.md\")\n\n\n./data/cg_organic_control_description.md\n\n\n\n\nCode\n# Read the ad treatment group\ncg_ad_treatment = pd.read_parquet('./data/cg_ad_treatment.parquet')\nrsm.md(\"./data/cg_ad_treatment_description.md\")\n\n\n./data/cg_ad_treatment_description.md\n\n\n\n\nCode\ncg_ad_treatment['converted_yes'] = rsm.ifelse(cg_ad_treatment.converted == \"yes\", 1, rsm.ifelse(cg_ad_treatment.converted == 'no', 0, np.nan))\n\n\n\nCalculate the response rate and profit of group 1.\n\n\nCode\ngroup1.converted.value_counts()\n\n\nconverted\nno     28294\nyes     1706\nName: count, dtype: int64\n\n\n\n\nCode\n# Group 1 response rate\n\ngroup1_rr = group1.value_counts('converted')[1] / group1.shape[0]\ngroup1_rr\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/553912631.py:3: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n0.05686666666666667\n\n\n\n\nCode\n# Group 1 profit\n# Profit = (price - cost) * number of customers\nprice = 14.99\ncost = 1.50\nquantity = group1.shape[0] * group1_rr\ngroup1_profit = price * quantity\ngroup1_profit\n\n\n25572.94\n\n\n\n\nCalculate the response rate and profit of group 2\n\n\nCode\n# Create group 2 variable\ngroup2 = cg_ad_treatment[cg_ad_treatment.rnd_30k == 1]\ngroup2.shape\n\n\n(30000, 22)\n\n\n\n\nCode\ndef profit_calc(data, price = 14.99, cost = 1.50):\n    response_rate = data.value_counts('converted')[1] / data.shape[0]\n    revenue = price * response_rate * data.shape[0]\n    total_cost = cost * data.shape[0]\n    profit = revenue - total_cost\n    return response_rate, profit\n\n\n\n\nCode\ngroup2_rr, group2_profit = profit_calc(group2)\ngroup2_rr, group2_profit\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/2151466375.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n(0.13043333333333335, 13655.870000000003)\n\n\n\n\nCalculate the response rate and profit of group 3\n\n\nCode\n# Predict the probability of purchasing the Zalon campaign for group3\ncg_ad_treatment[\"pred_logit\"] = lr.predict(cg_ad_treatment)['prediction']\n\n\n\n\nCode\n# Create group 3 variable\nnon_group2 = cg_ad_treatment[cg_ad_treatment.rnd_30k == 0]\nnon_group2 = non_group2.sort_values(by='pred_logit', ascending=False)\ngroup3 = non_group2.head(30000)\n\n\n\n\nCode\ngroup3_rr, group3_profit = profit_calc(group3)\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/2151466375.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\nCode\n# create a dataframe with the results\nresults = pd.DataFrame({\n    'group': ['group1', 'group2', 'group3'],\n    'response_rate': [group1_rr, group2_rr, group3_rr],\n    'profit': [group1_profit, group2_profit, group3_profit]\n})\nresults\n\n\n\n\n\n\n\n\n\n\ngroup\nresponse_rate\nprofit\n\n\n\n\n0\ngroup1\n0.056867\n25572.94\n\n\n1\ngroup2\n0.130433\n13655.87\n\n\n2\ngroup3\n0.215067\n51715.48\n\n\n\n\n\n\n\n\n\n\nCode\nnon_group2[\"converted_yes\"] = non_group2[\"converted\"].map({\"yes\": 1, \"no\": 0})\nnon_group2[\"converted_yes\"] = non_group2[\"converted_yes\"].astype(\"int\")\n\n\n\n\nCode\nfig = rsm.gains_plot(non_group2, \"converted_yes\", 1, \"pred_logit\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# non_group2 AUC\npred = non_group2['pred_logit']\nactual =non_group2['converted_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr,tpr).round(3)\n\n\n0.644\n\n\nThe observed difference in the Area Under the Curve (AUC) values between part 2 and part 3 provides a compelling insight into the efficacy of using predictive models for campaign targeting. Specifically, the AUC of 0.809 in part 2 significantly outperforms the AUC of 0.592 in part 3. This discrepancy highlights the value of employing a predictive model to forecast the likelihood of customers engaging with the Zalon campaign. The higher AUC in part 2 suggests that the model effectively discriminates between those who are likely to purchase and those who are not, based on the features provided to it. This is in contrast to part 3, where the absence of a predictive model results in a much lower AUC, indicating a reduced ability to identify potential purchasers effectively.\nMoreover, the comparison of gains curves between the two parts further emphasizes the advantage of utilizing a predictive model. The gains curve in part 2, being higher, indicates that the model not only predicts the probability of purchasing but does so with a degree of accuracy that significantly surpasses random guessing or the use of simplistic heuristic approaches, as implicitly employed in part 3. This visual representation corroborates the numerical evidence provided by the AUC, underscoring the model‚Äôs capability to prioritize customers more likely to respond positively to the campaign.\nOne of the advantages of selling an in-app purchase was that Creative Gaming had detailed data on how players play the game. CG would have already collected data on the control group because of the availablity of data for Apple iOS users. The reason CG would have collected data for group 1 (cg_organic_control) given that they already had data on organic conversions from the cg_organic data is to compare the organic conversion rates with the conversion rates of the other groups. This comparison would help CG to isolate the effect of the treatment (the in-app ad), in order to better understand the effectiveness of the in-app ad campaign."
  },
  {
    "objectID": "projects/cg_prop/index.html#part-4",
    "href": "projects/cg_prop/index.html#part-4",
    "title": "CG Propensity",
    "section": "Part 4",
    "text": "Part 4\n\nRetrain the logistic regression model from Part II on the sample of 30k customers\n\n\nCode\ncg_ad_treatment['converted_yes'] = rsm.ifelse(cg_ad_treatment.converted == \"yes\", 1, rsm.ifelse(cg_ad_treatment.converted == 'no', 0, np.nan))\n\n\n\n\nCode\nlr2 = rsm.model.logistic(\n    data = {\"cg_ad_treatment\" : cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = \"converted_yes\",\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ])\nlr2.summary()\n\n\nLogistic regression (GLM)\nData                 : cg_ad_treatment\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nNull hyp.: There is no effect of x on converted_yes\nAlt. hyp.: There is an effect of x on converted_yes\n\n                                OR     OR%  coefficient  std.error  z.value p.value     \nIntercept                    0.028  -97.2%        -3.57      0.101  -35.155  &lt; .001  ***\nAcquiredSpaceship[yes]       1.092    9.2%         0.09      0.041    2.143   0.032    *\nAcquiredIonWeapon[yes]       0.879  -12.1%        -0.13      0.139   -0.929   0.353     \nPurchasedCoinPackSmall[yes]  1.036    3.6%         0.04      0.038    0.924   0.356     \nPurchasedCoinPackLarge[yes]  1.176   17.6%         0.16      0.041    3.980  &lt; .001  ***\nUserConsole[yes]             0.951   -4.9%        -0.05      0.049   -1.030   0.303     \nUserHasOldOS[yes]            0.814  -18.6%        -0.21      0.068   -3.022   0.003   **\nGameLevel                    1.054    5.4%         0.05      0.008    7.046  &lt; .001  ***\nNumGameDays                  1.015    1.5%         0.02      0.003    5.155  &lt; .001  ***\nNumGameDays4Plus             1.009    0.9%         0.01      0.005    1.617   0.106     \nNumInGameMessagesSent        1.000   -0.0%        -0.00      0.000   -0.845   0.398     \nNumFriends                   1.002    0.2%         0.00      0.000   10.983  &lt; .001  ***\nNumFriendRequestIgnored      1.000    0.0%         0.00      0.001    0.052   0.958     \nNumSpaceHeroBadges           1.022    2.2%         0.02      0.008    2.699   0.007   **\nTimesLostSpaceship           0.994   -0.6%        -0.01      0.002   -3.138   0.002   **\nTimesKilled                  0.999   -0.1%        -0.00      0.005   -0.241   0.809     \nTimesCaptain                 1.006    0.6%         0.01      0.002    2.983   0.003   **\nTimesNavigator               0.998   -0.2%        -0.00      0.003   -0.791   0.429     \nNumAdsClicked                1.093    9.3%         0.09      0.002   39.636  &lt; .001  ***\nDaysUser                     1.000    0.0%         0.00      0.000    0.738    0.46     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.094\nPseudo R-squared (McFadden adjusted): 0.093\nArea under the RO Curve (AUC): 0.707\nLog-likelihood: -10520.156, AIC: 21080.313, BIC: 21246.492\nChi-squared: 2192.265, df(19), p.value &lt; 0.001 \nNr obs: 30,000\n\n\n\n\nCode\nlr2.summary(main = False, vif = True)\n\n\n\nPseudo R-squared (McFadden): 0.094\nPseudo R-squared (McFadden adjusted): 0.093\nArea under the RO Curve (AUC): 0.707\nLog-likelihood: -10520.156, AIC: 21080.313, BIC: 21246.492\nChi-squared: 2192.265, df(19), p.value &lt; 0.001 \nNr obs: 30,000\n\nVariance inflation factors:\n\n                           vif    Rsq\nNumFriendRequestIgnored  1.623  0.384\nNumInGameMessagesSent    1.587  0.370\nNumGameDays              1.351  0.260\nGameLevel                1.265  0.210\nNumGameDays4Plus         1.264  0.209\nAcquiredSpaceship        1.161  0.139\nNumFriends               1.158  0.137\nNumSpaceHeroBadges       1.134  0.118\nTimesLostSpaceship       1.103  0.094\nTimesNavigator           1.094  0.086\nTimesCaptain             1.078  0.073\nDaysUser                 1.068  0.063\nPurchasedCoinPackSmall   1.046  0.044\nPurchasedCoinPackLarge   1.033  0.032\nUserConsole              1.033  0.032\nNumAdsClicked            1.029  0.029\nUserHasOldOS             1.002  0.002\nTimesKilled              1.001  0.001\nAcquiredIonWeapon        1.001  0.001\n\n\n\n\nCode\n# Generate predictions for all 150,000 customers\ncg_ad_treatment['pred_logit_ad'] = lr2.predict(cg_ad_treatment)['prediction']\n\n\nThe model‚Äôs summary shows that several variables are statistically significant in predicting conversion, as indicated by the p-values (for example, PurchasedCoinPackLarge and UserHasOldOS). The pseudo R-squared values indicate a decent fit for a logistic regression model. Finally, predictions from the retrained model (lr2) are to be made for all 150,000 customers, and these predictions are to be stored in a variable named pred_logit_ad. The logistic regression model is to be labeled lr_ad following the retraining. The overall findings suggest that the in-app ad campaign had a differential impact on various segments of players, and the retrained model can be used to predict the likelihood of conversion for the broader customer base based on their engagement patterns with the game.\n\n\nCompare the performance of original organic model and new ad model\n\n\nCode\nfig = rsm.gains_plot(\n    cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0],\n    \"converted\", \"yes\",\n    [\"pred_logit\", \"pred_logit_ad\"]\n)\n\n\n\n\n\n\n\n\n\nThe graph shows the performance comparison between the original model (‚Äòorganic‚Äô) and the new model (‚Äòad‚Äô) using the Area Under the Curve (AUC) metric and gains curves. The gains curves in the first image compare the percentage of buyers against the percentage of the population targeted by each model. The line representing the ‚Äòad‚Äô model (‚Äòpred_logit_ad‚Äô) consistently lies above the ‚Äòorganic‚Äô model (‚Äòpred_logit‚Äô), suggesting that the ‚Äòad‚Äô model predicts conversions more effectively across the entire range of the targeted population.\n\n\nCode\n#AUC for organic model across the 120k customers that are not in group 2\npred = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['pred_logit']\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['converted_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nprint(metrics.auc(fpr, tpr).round(3))\n\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['converted']\nprint(rsm.auc(actual, pred, 'yes').round(3))\n\n\n0.644\n0.644\n\n\n\n\nCode\n# AUC for new ad model across the 120k customers that are not in group 2\npred = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['pred_logit_ad']\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['converted_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nprint(metrics.auc(fpr, tpr).round(3))\n\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0]['converted']\nprint(rsm.auc(actual, pred, 'yes').round(3))\n\n\n0.703\n0.703\n\n\nThe ‚Äòorganic‚Äô model has an AUC of 0.592, indicating a fair but not strong discriminatory ability. In contrast, the ‚Äòad‚Äô model achieves a higher AUC of 0.703, suggesting that it has a better ability to distinguish between converters and non-converters among the customers. This improvement in AUC by the ‚Äòad‚Äô model supports the premise that incorporating in-app ad data into the model training process can enhance its predictive performance, particularly for identifying potential customers who respond to in-app advertisements.\n\n\nCalculate the profit improvement of using ad model instead of the organic\n\n\nCode\nmargin = 14.99\ncost = 1.5\n\n\n\n\nCode\n# Converting using the ad model to target the best 30000 customers in the cg_ad_treatment dataset that not in 'rnd_30k == 1'\nad_best30000 = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0].nlargest(30000, 'pred_logit_ad')\nads_response_rate, ads_profit = profit_calc(ad_best30000)\nads_response_rate, ads_profit\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/2151466375.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n(0.2739, 78172.82999999997)\n\n\n\n\nCode\nmod_pef = pd.DataFrame({\n    \"model\": [\"Organic\", \"Ads\"],\n    \"Response Rate\": [group3_rr, ads_response_rate],\n    \"Profit\": [group3_profit, ads_profit]\n})\nmod_pef\n\n\n\n\n\n\n\n\n\n\nmodel\nResponse Rate\nProfit\n\n\n\n\n0\nOrganic\n0.215067\n51715.48\n\n\n1\nAds\n0.273900\n78172.83\n\n\n\n\n\n\n\n\n\n\nCode\n# The profit improvement from using the ad model\nprofit_improvement = ads_profit - group3_profit\nprofit_improvement\n\n\n26457.349999999977\n\n\nThe above calculates the profit improvement of using the ‚Äúad‚Äù model over the ‚Äúorganic‚Äù model. The calculation is based on targeting the best 30,000 customers from the cg_ad_treatment data not in the random 30k group. The code computes the expected profit for the top-scoring customers according to each model‚Äôs predictions, using a defined margin and cost to determine the break-even point.\nThe profit from the ‚Äúorganic‚Äù model is calculated and then compared to the profit from the ‚Äúad‚Äù model. The results show that the ‚Äúad‚Äù model yields a higher profit. Specifically, the ‚Äúorganic‚Äù model achieves a profit of approximately $51,715 with a ~22% response rate, whereas the ‚Äúad‚Äù model yields about $78,173 with a ~27% response rate, resulting in a profit improvement of around $26,457 and response rate improvement of ~51.16% when using the ‚Äúad‚Äù model.\nIn summary, the ‚Äúad‚Äù model, which incorporates the in-app ad data, significantly outperforms the ‚Äúorganic‚Äù model in terms of profit when targeting the same top 30,000 customers outside of the initial random 30k group. This suggests that the ‚Äúad‚Äù model is more effective at identifying customers who are likely to convert, leading to a more profitable targeting strategy.\n\n\nCompare the permutation importance plot of the organic and the ad model\n\n\nCode\n# Permutation importance plot of the organic model\nlr.plot('vimp')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Permutation importance plot of the ad model\nlr2.plot('vimp')\n\n\n\n\n\n\n\n\n\nThe permutation importance plots for both the ‚Äúorganic‚Äù and the ‚Äúad‚Äù models reveal differences in the relative importance of features when predicting customer conversion. In the ‚Äúorganic‚Äù model, the most important features seem to be NumSpaceHeroBadges, NumFriendRequestsIgnored, and TimesLostSpaceship, whereas in the ‚Äúad‚Äù model, NumAdsClicked appears to be the most significant feature, followed by NumFriends and GameLevel.\nThe discrepancy in feature importance between the two models can be attributed to the different nature of the datasets they were trained on. The ‚Äúorganic‚Äù model likely did not have access to data on customer interactions with in-app ads, which explains why gameplay-related features such as badges and friends are more predictive in this model. On the other hand, the ‚Äúad‚Äù model includes data from the in-app ad campaign, making NumAdsClicked a crucial predictor, as it directly relates to the customer‚Äôs engagement with the ad content that is hypothesized to influence conversion.\nFurthermore, the change in the ranking of features such as GameLevel and NumFriends between the two models suggests that the context in which these features affect conversion may have shifted when ad engagement data is included. It‚Äôs possible that the behavior patterns associated with clicking ads are more indicative of conversion likelihood than organic game engagement metrics when ad data is present. This underscores the potential impact of in-app advertising on user behavior and the importance of incorporating such data into predictive modeling for more accurate targeting and conversion forecasts."
  },
  {
    "objectID": "projects/cg_prop/index.html#part-5",
    "href": "projects/cg_prop/index.html#part-5",
    "title": "CG Propensity",
    "section": "Part 5",
    "text": "Part 5\nBased on the output of the logistic regression, we believe that the neural network and random forest models will perform better than the logistic regression model. We will train and tune a neural network and random forest model on the ad data and compare their performance to the logistic regression model.\nWe first started by creating a simple neural network with 1 hidden layer and 1 node.\n\n\nCode\nclf = rsm.model.mlp(\n    data = {'cg_ad_campaign': cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = 'converted_yes',\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ],\n    hidden_layer_sizes = (1, ),\n    mod_type = 'classification'\n)\n\nclf.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_ad_campaign\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 30,000\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.706\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         2            8                 0                      0           4                        5                   0                no                no                   0            0             8               0                    yes                     no              3      1889          no          yes\n         5           15                 0                    179         362                       50                   0               yes                no                  22            0             4               4                     no                     no              2      1308         yes           no\n         7            7                 0                    267           0                       64                   0                no                no                   5            0             0               0                     no                    yes              1      3562         yes           no\n         4            4                 0                     36           0                        0                   0                no                no                   0            0             0               0                     no                     no              2      2922         yes           no\n         8           17                 0                    222          20                       63                  10               yes                no                  10            0             9               6                    yes                     no              4      2192         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n -1.562310    -0.622508         -0.410992              -0.691781   -0.482059                -0.729964           -0.371999           -0.320636    -0.081651      0.868067       -0.206064      -0.842604 -1.120787                  False                  False                        True                       False            False              True\n -0.478309     0.360651         -0.410992               0.963396    3.191052                 0.581947           -0.371999            1.366772    -0.081651      0.332358        0.410387      -0.988806 -1.997150                   True                  False                       False                       False             True             False\n  0.244358    -0.762959         -0.410992               1.777114   -0.523100                 0.990097           -0.371999            0.062866    -0.081651     -0.203351       -0.206064      -1.135009  1.402716                  False                  False                       False                        True             True             False\n -0.839643    -1.184313         -0.410992              -0.358896   -0.523100                -0.875731           -0.371999           -0.320636    -0.081651     -0.203351       -0.206064      -0.988806  0.437359                  False                  False                       False                       False             True             False\n  0.605692     0.641553         -0.410992               1.361008   -0.317898                 0.960943            4.094315            0.446368    -0.081651      1.001994        0.718613      -0.696401 -0.663751                   True                  False                        True                       False             True             False\n\n\nNN with 1 hidden layer and 2 nodes\n\n\nCode\nclf1 = rsm.model.mlp(\n    data = {'cg_ad_campaign': cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = 'converted_yes',\n    evar = [\"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\",\n    ],\n    hidden_layer_sizes = (2, ),\n    mod_type = 'classification'\n)\n\nclf1.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_ad_campaign\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 30,000\nHidden_layer_sizes   : (2,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.748\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         2            8                 0                      0           4                        5                   0                no                no                   0            0             8               0                    yes                     no              3      1889          no          yes\n         5           15                 0                    179         362                       50                   0               yes                no                  22            0             4               4                     no                     no              2      1308         yes           no\n         7            7                 0                    267           0                       64                   0                no                no                   5            0             0               0                     no                    yes              1      3562         yes           no\n         4            4                 0                     36           0                        0                   0                no                no                   0            0             0               0                     no                     no              2      2922         yes           no\n         8           17                 0                    222          20                       63                  10               yes                no                  10            0             9               6                    yes                     no              4      2192         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n -1.562310    -0.622508         -0.410992              -0.691781   -0.482059                -0.729964           -0.371999           -0.320636    -0.081651      0.868067       -0.206064      -0.842604 -1.120787                  False                  False                        True                       False            False              True\n -0.478309     0.360651         -0.410992               0.963396    3.191052                 0.581947           -0.371999            1.366772    -0.081651      0.332358        0.410387      -0.988806 -1.997150                   True                  False                       False                       False             True             False\n  0.244358    -0.762959         -0.410992               1.777114   -0.523100                 0.990097           -0.371999            0.062866    -0.081651     -0.203351       -0.206064      -1.135009  1.402716                  False                  False                       False                        True             True             False\n -0.839643    -1.184313         -0.410992              -0.358896   -0.523100                -0.875731           -0.371999           -0.320636    -0.081651     -0.203351       -0.206064      -0.988806  0.437359                  False                  False                       False                       False             True             False\n  0.605692     0.641553         -0.410992               1.361008   -0.317898                 0.960943            4.094315            0.446368    -0.081651      1.001994        0.718613      -0.696401 -0.663751                   True                  False                        True                       False             True             False\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nhls = [(1,), (2,), (3,), (3, 3), (4, 2), (5, 5)]\nlearning_rate_init = [0.1, 0.01, 0.001, 0.0001]\n\n\nparam_grid = {\"hidden_layer_sizes\": hls, \"learning_rate_init\": learning_rate_init}\nscoring = {\"AUC\": \"roc_auc\"}\n\nclf_cv = GridSearchCV(\n    clf.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5\n)\n\n\n\n\nCode\nclf.data_onehot.mean().round(3)\nclf.data.converted_yes.mean()\n\n\n0.13043333333333335\n\n\n\n\nCode\nclf_cv.fit(clf.data_onehot, clf.data.converted_yes)\n\n\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n\n\nGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5)],\n                         'learning_rate_init': [0.1, 0.01, 0.001, 0.0001]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=MLPClassifier(activation='tanh', hidden_layer_sizes=(1,),\n                                     max_iter=10000, random_state=1234,\n                                     solver='lbfgs'),\n             n_jobs=4,\n             param_grid={'hidden_layer_sizes': [(1,), (2,), (3,), (3, 3),\n                                                (4, 2), (5, 5)],\n                         'learning_rate_init': [0.1, 0.01, 0.001, 0.0001]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') ¬†MLPClassifier?Documentation for MLPClassifierMLPClassifier(activation='tanh', hidden_layer_sizes=(1,), max_iter=10000,\n              random_state=1234, solver='lbfgs') \n\n\n\n\nCode\nclf_cv.best_params_\n\n\n{'hidden_layer_sizes': (4, 2), 'learning_rate_init': 0.1}\n\n\n\n\nCode\nclf_cv.best_score_.round(3)\n\n\n0.782\n\n\n\n\nCode\nclf42 = rsm.model.mlp(\n    data = {'cg_ad_campaign': cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = 'converted_yes',\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ],\n    hidden_layer_sizes = (4, 2),\n    learning_rate_init = 0.1,\n    mod_type = 'classification'\n)\n\nclf42.summary()\n\n\nMulti-layer Perceptron (NN)\nData                 : cg_ad_campaign\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 30,000\nHidden_layer_sizes   : (4, 2)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.1\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.792\n\nRaw data             :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges AcquiredSpaceship AcquiredIonWeapon  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator PurchasedCoinPackSmall PurchasedCoinPackLarge  NumAdsClicked  DaysUser UserConsole UserHasOldOS\n         2            8                 0                      0           4                        5                   0                no                no                   0            0             8               0                    yes                     no              3      1889          no          yes\n         5           15                 0                    179         362                       50                   0               yes                no                  22            0             4               4                     no                     no              2      1308         yes           no\n         7            7                 0                    267           0                       64                   0                no                no                   5            0             0               0                     no                    yes              1      3562         yes           no\n         4            4                 0                     36           0                        0                   0                no                no                   0            0             0               0                     no                     no              2      2922         yes           no\n         8           17                 0                    222          20                       63                  10               yes                no                  10            0             9               6                    yes                     no              4      2192         yes           no\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n -1.562310    -0.622508         -0.410992              -0.691781   -0.482059                -0.729964           -0.371999           -0.320636    -0.081651      0.868067       -0.206064      -0.842604 -1.120787                  False                  False                        True                       False            False              True\n -0.478309     0.360651         -0.410992               0.963396    3.191052                 0.581947           -0.371999            1.366772    -0.081651      0.332358        0.410387      -0.988806 -1.997150                   True                  False                       False                       False             True             False\n  0.244358    -0.762959         -0.410992               1.777114   -0.523100                 0.990097           -0.371999            0.062866    -0.081651     -0.203351       -0.206064      -1.135009  1.402716                  False                  False                       False                        True             True             False\n -0.839643    -1.184313         -0.410992              -0.358896   -0.523100                -0.875731           -0.371999           -0.320636    -0.081651     -0.203351       -0.206064      -0.988806  0.437359                  False                  False                       False                       False             True             False\n  0.605692     0.641553         -0.410992               1.361008   -0.317898                 0.960943            4.094315            0.446368    -0.081651      1.001994        0.718613      -0.696401 -0.663751                   True                  False                        True                       False             True             False\n\n\n\nCompare the performance of the neutral network ad model and the logistic regression ad model\n\n\nCode\ncg_ad_treatment['pred_nn'] = clf42.predict(cg_ad_treatment)['prediction']\n\n\n\n\nCode\nfig = rsm.gains_plot(\n    cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0],\n    \"converted\", \"yes\",\n    [\"pred_nn\", \"pred_logit_ad\"]\n)\n\n\n\n\n\n\n\n\n\nBased on the graph, the neural network is better because for any given percentage of customers, it captures a higher percentage of the total response compared to the logistic regression model.\nIn the neural network model, we use Grid Search to tune the hyperparameters, i.e.¬†the hidden layer size and the learning rate. These two hyperparameters can influence the outcome:\n\nHidden layer size: By finding the right balance through grid search, we can avoid overfitting or underfitting the model, leading to a model that generalizes well to new data.\nLearning rate: the learning rate controls how much the model‚Äôs weights are updated during training. Tuning the learning rate to a level that allows for efficient and effective learning is critical for good performance.\n\nThe combination of these two hyperparameters enables the neural network to capture the complexity of the data without overfitting, leading to better generalization and ultimately better performance on the gain chart.\n\n\nCode\n# prediction on rnd_30k == 0 for neutral network \"ad\" model\npred = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"pred_nn\"]\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"converted_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n\n0.782\n\n\n\n\nCode\n# again, predict on rnd_30k == 0 for logistic regression \"ad\" model\npred = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"pred_logit_ad\"]\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"converted_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n\n0.703\n\n\nThe AUC of the neural network model is 0.782 while the AUC of the logistic regression model is 0.703. Both models have good performance, but the difference in AUC shows that the neural netwrork model has a significant better performance in term of overall measure of accuracy across all thresholds. This confirms the visual interpretation from the gain chart, where the neural network model outperforms the logistic regression model.\n\n\nCalculate the profit improvement of the neutral network ad\n\n\nCode\nnn_group3 = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0].nlargest(30000, 'pred_nn')\nnn_response_rate, nn_profit = profit_calc(nn_group3)\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/2151466375.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\nCode\nmod_pef = pd.DataFrame({\n    \"model\": [\"Neural Network\", \"Ads\"],\n    \"Response Rate\": [nn_response_rate, ads_response_rate],\n    \"Profit\": [nn_profit, ads_profit]\n})\nmod_pef\n\n\n\n\n\n\n\n\n\n\nmodel\nResponse Rate\nProfit\n\n\n\n\n0\nNeural Network\n0.311933\n95276.42\n\n\n1\nAds\n0.273900\n78172.83\n\n\n\n\n\n\n\n\n\n\nCode\nprofit_improvement = nn_profit - ads_profit\nprofit_improvement\n\n\n17103.59000000004\n\n\nThat said, with the better performance of the neural network model, we can expect a better profit improvement compared to the logistic regression model. The profit improvement from using Neural Network model over the logistic regression model ad is $68,954, this represents a 22.05% improvement in profit when using Neural Network model over the logistic regression model ad.\n\n\nTrain and tune random forest model\n\n\nCode\nrf = rsm.model.rforest(\n    data = {'cg_ad_campaign': cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = 'converted_yes',\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ]\n)\nrf.summary()\n\n\nRandom Forest\nData                 : cg_ad_campaign\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 30,000\nmax_features         : sqrt (4)\nn_estimators         : 100\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.759\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         2            8                 0                      0           4                        5                   0                   0            0             8               0              3      1889                  False                  False                        True                       False            False              True\n         5           15                 0                    179         362                       50                   0                  22            0             4               4              2      1308                   True                  False                       False                       False             True             False\n         7            7                 0                    267           0                       64                   0                   5            0             0               0              1      3562                  False                  False                       False                        True             True             False\n         4            4                 0                     36           0                        0                   0                   0            0             0               0              2      2922                  False                  False                       False                       False             True             False\n         8           17                 0                    222          20                       63                  10                  10            0             9               6              4      2192                   True                  False                        True                       False             True             False\n\n\n\nModel Tuning\n\n\nCode\nmax_features = [None, 'auto', 'sqrt', 'log2', 0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0]\nn_estimators = [10, 50, 100, 200, 500, 1000]\n\nparam_grid = {\"max_features\": max_features, \"n_estimators\": n_estimators}\nscoring = {\"AUC\": \"roc_auc\"}\n\nrf_cv = GridSearchCV(rf.fitted, param_grid, scoring=scoring, cv=5, n_jobs=4, refit=\"AUC\", verbose=5)\n\n\n\n\nCode\nrf.data_onehot.mean().round(3)\nrf.data.converted_yes.mean()\n\n\n0.13043333333333335\n\n\n\n\nCode\nrf_cv.fit(rf.data_onehot, rf.data.converted_yes)\n\n\nFitting 5 folds for each of 66 candidates, totalling 330 fits\n[CV 3/5] END hidden_layer_sizes=(1,), learning_rate_init=0.1; AUC: (test=0.714) total time=   0.1s\n[CV 1/5] END hidden_layer_sizes=(1,), learning_rate_init=0.01; AUC: (test=0.696) total time=   0.2s\n[CV 1/5] END hidden_layer_sizes=(1,), learning_rate_init=0.001; AUC: (test=0.696) total time=   0.2s\n[CV 1/5] END hidden_layer_sizes=(1,), learning_rate_init=0.0001; AUC: (test=0.696) total time=   0.2s\n[CV 5/5] END hidden_layer_sizes=(1,), learning_rate_init=0.0001; AUC: (test=0.702) total time=   0.2s\n[CV 4/5] END hidden_layer_sizes=(2,), learning_rate_init=0.1; AUC: (test=0.752) total time=   0.8s\n[CV 5/5] END hidden_layer_sizes=(2,), learning_rate_init=0.1; AUC: (test=0.759) total time=   1.6s\n[CV 2/5] END hidden_layer_sizes=(2,), learning_rate_init=0.01; AUC: (test=0.745) total time=   3.4s\n[CV 1/5] END hidden_layer_sizes=(2,), learning_rate_init=0.001; AUC: (test=0.747) total time=   6.2s\n[CV 4/5] END hidden_layer_sizes=(2,), learning_rate_init=0.0001; AUC: (test=0.752) total time=   0.9s\n[CV 5/5] END hidden_layer_sizes=(2,), learning_rate_init=0.0001; AUC: (test=0.759) total time=   2.1s\n[CV 3/5] END hidden_layer_sizes=(3,), learning_rate_init=0.1; AUC: (test=0.767) total time=   6.5s\n[CV 1/5] END hidden_layer_sizes=(3,), learning_rate_init=0.01; AUC: (test=0.750) total time=   9.5s\n[CV 4/5] END hidden_layer_sizes=(3,), learning_rate_init=0.01; AUC: (test=0.726) total time=   0.9s\n[CV 1/5] END hidden_layer_sizes=(3,), learning_rate_init=0.001; AUC: (test=0.750) total time=   9.0s\n[CV 4/5] END hidden_layer_sizes=(3,), learning_rate_init=0.001; AUC: (test=0.726) total time=   0.7s\n[CV 5/5] END hidden_layer_sizes=(3,), learning_rate_init=0.001; AUC: (test=0.758) total time=   7.4s\n[CV 3/5] END hidden_layer_sizes=(3,), learning_rate_init=0.0001; AUC: (test=0.767) total time=   6.1s\n[CV 1/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.1; AUC: (test=0.770) total time=  12.5s\n[CV 5/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.1; AUC: (test=0.764) total time=  19.7s\n[CV 5/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.01; AUC: (test=0.764) total time=  18.4s\n[CV 5/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.001; AUC: (test=0.764) total time=  18.9s\n[CV 5/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.0001; AUC: (test=0.764) total time=  19.5s\n[CV 5/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.1; AUC: (test=0.779) total time=  12.3s\n[CV 3/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.01; AUC: (test=0.788) total time=  21.0s\n[CV 5/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.001; AUC: (test=0.779) total time=  11.5s\n[CV 3/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.0001; AUC: (test=0.788) total time=  23.1s\n[CV 4/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.1; AUC: (test=0.785) total time=  19.8s\n[CV 1/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.01; AUC: (test=0.760) total time=  31.5s\n[CV 5/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.01; AUC: (test=0.762) total time=  14.1s\n[CV 1/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.001; AUC: (test=0.760) total time=  36.7s\n[CV 5/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.001; AUC: (test=0.762) total time=  13.6s\n[CV 2/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.0001; AUC: (test=0.778) total time=  49.5s\n[CV 2/5] END max_features=None, n_estimators=10; AUC: (test=0.720) total time=   0.8s\n[CV 5/5] END max_features=None, n_estimators=10; AUC: (test=0.720) total time=   0.8s\n[CV 4/5] END max_features=None, n_estimators=50; AUC: (test=0.753) total time=   3.9s\n[CV 3/5] END max_features=None, n_estimators=100; AUC: (test=0.767) total time=   7.6s\n[CV 2/5] END max_features=None, n_estimators=200; AUC: (test=0.766) total time=  16.4s\n[CV 1/5] END max_features=None, n_estimators=500; AUC: (test=0.761) total time=  38.6s\n[CV 4/5] END hidden_layer_sizes=(1,), learning_rate_init=0.1; AUC: (test=0.698) total time=   0.2s\n[CV 2/5] END hidden_layer_sizes=(1,), learning_rate_init=0.01; AUC: (test=0.712) total time=   0.1s\n[CV 4/5] END hidden_layer_sizes=(1,), learning_rate_init=0.01; AUC: (test=0.698) total time=   0.2s\n[CV 3/5] END hidden_layer_sizes=(1,), learning_rate_init=0.001; AUC: (test=0.714) total time=   0.1s\n[CV 2/5] END hidden_layer_sizes=(1,), learning_rate_init=0.0001; AUC: (test=0.712) total time=   0.1s\n[CV 1/5] END hidden_layer_sizes=(2,), learning_rate_init=0.1; AUC: (test=0.747) total time=   6.0s\n[CV 5/5] END hidden_layer_sizes=(2,), learning_rate_init=0.01; AUC: (test=0.759) total time=   1.8s\n[CV 3/5] END hidden_layer_sizes=(2,), learning_rate_init=0.001; AUC: (test=0.768) total time=   2.0s\n[CV 1/5] END hidden_layer_sizes=(2,), learning_rate_init=0.0001; AUC: (test=0.747) total time=   6.5s\n[CV 4/5] END hidden_layer_sizes=(3,), learning_rate_init=0.1; AUC: (test=0.726) total time=   0.8s\n[CV 5/5] END hidden_layer_sizes=(3,), learning_rate_init=0.1; AUC: (test=0.758) total time=   7.3s\n[CV 3/5] END hidden_layer_sizes=(3,), learning_rate_init=0.01; AUC: (test=0.767) total time=   7.0s\n[CV 5/5] END hidden_layer_sizes=(3,), learning_rate_init=0.01; AUC: (test=0.758) total time=   7.7s\n[CV 3/5] END hidden_layer_sizes=(3,), learning_rate_init=0.001; AUC: (test=0.767) total time=   6.7s\n[CV 2/5] END hidden_layer_sizes=(3,), learning_rate_init=0.0001; AUC: (test=0.768) total time=  21.8s\n[CV 4/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.1; AUC: (test=0.794) total time=  13.6s\n[CV 2/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.01; AUC: (test=0.780) total time=  23.9s\n[CV 4/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.001; AUC: (test=0.794) total time=  12.6s\n[CV 2/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.0001; AUC: (test=0.780) total time=  23.9s\n[CV 4/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.1; AUC: (test=0.792) total time=  11.2s\n[CV 2/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.01; AUC: (test=0.780) total time=   5.3s\n[CV 4/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.01; AUC: (test=0.792) total time=  10.7s\n[CV 2/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.001; AUC: (test=0.780) total time=   5.2s\n[CV 3/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.001; AUC: (test=0.788) total time=  21.8s\n[CV 5/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.0001; AUC: (test=0.779) total time=  12.0s\n[CV 3/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.1; AUC: (test=0.780) total time=  45.9s\n[CV 3/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.01; AUC: (test=0.780) total time=  45.9s\n[CV 3/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.001; AUC: (test=0.780) total time=  45.6s\n[CV 4/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.0001; AUC: (test=0.785) total time=  20.6s\n[CV 1/5] END max_features=None, n_estimators=10; AUC: (test=0.709) total time=   0.8s\n[CV 2/5] END max_features=None, n_estimators=50; AUC: (test=0.757) total time=   3.8s\n[CV 5/5] END max_features=None, n_estimators=50; AUC: (test=0.752) total time=   3.8s\n[CV 4/5] END max_features=None, n_estimators=100; AUC: (test=0.758) total time=   7.6s\n[CV 3/5] END max_features=None, n_estimators=200; AUC: (test=0.769) total time=  16.5s\n[CV 2/5] END max_features=None, n_estimators=500; AUC: (test=0.768) total time=  38.4s\n[CV 2/5] END hidden_layer_sizes=(1,), learning_rate_init=0.1; AUC: (test=0.712) total time=   0.1s\n[CV 5/5] END hidden_layer_sizes=(1,), learning_rate_init=0.1; AUC: (test=0.702) total time=   0.2s\n[CV 5/5] END hidden_layer_sizes=(1,), learning_rate_init=0.01; AUC: (test=0.702) total time=   0.2s\n[CV 5/5] END hidden_layer_sizes=(1,), learning_rate_init=0.001; AUC: (test=0.702) total time=   0.2s\n[CV 4/5] END hidden_layer_sizes=(1,), learning_rate_init=0.0001; AUC: (test=0.698) total time=   0.1s\n[CV 3/5] END hidden_layer_sizes=(2,), learning_rate_init=0.1; AUC: (test=0.768) total time=   2.1s\n[CV 1/5] END hidden_layer_sizes=(2,), learning_rate_init=0.01; AUC: (test=0.747) total time=   6.4s\n[CV 4/5] END hidden_layer_sizes=(2,), learning_rate_init=0.001; AUC: (test=0.752) total time=   0.8s\n[CV 5/5] END hidden_layer_sizes=(2,), learning_rate_init=0.001; AUC: (test=0.759) total time=   1.7s\n[CV 3/5] END hidden_layer_sizes=(2,), learning_rate_init=0.0001; AUC: (test=0.768) total time=   2.3s\n[CV 1/5] END hidden_layer_sizes=(3,), learning_rate_init=0.1; AUC: (test=0.750) total time=   8.8s\n[CV 2/5] END hidden_layer_sizes=(3,), learning_rate_init=0.01; AUC: (test=0.768) total time=  23.3s\n[CV 1/5] END hidden_layer_sizes=(3,), learning_rate_init=0.0001; AUC: (test=0.750) total time=   8.3s\n[CV 4/5] END hidden_layer_sizes=(3,), learning_rate_init=0.0001; AUC: (test=0.726) total time=   0.8s\n[CV 5/5] END hidden_layer_sizes=(3,), learning_rate_init=0.0001; AUC: (test=0.758) total time=   7.2s\n[CV 3/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.1; AUC: (test=0.764) total time=   6.9s\n[CV 1/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.01; AUC: (test=0.770) total time=  12.9s\n[CV 3/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.01; AUC: (test=0.764) total time=   6.9s\n[CV 1/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.001; AUC: (test=0.770) total time=  13.0s\n[CV 3/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.001; AUC: (test=0.764) total time=   6.6s\n[CV 1/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.0001; AUC: (test=0.770) total time=  12.2s\n[CV 3/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.0001; AUC: (test=0.764) total time=   7.1s\n[CV 1/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.1; AUC: (test=0.771) total time=  18.7s\n[CV 1/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.01; AUC: (test=0.771) total time=  18.0s\n[CV 1/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.001; AUC: (test=0.771) total time=  17.3s\n[CV 1/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.0001; AUC: (test=0.771) total time=  17.0s\n[CV 1/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.1; AUC: (test=0.760) total time=  31.9s\n[CV 5/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.1; AUC: (test=0.762) total time=  13.8s\n[CV 2/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.01; AUC: (test=0.778) total time=  58.0s\n[CV 4/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.001; AUC: (test=0.785) total time=  23.5s\n[CV 1/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.0001; AUC: (test=0.760) total time=  32.5s\n[CV 5/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.0001; AUC: (test=0.762) total time=  12.7s\n[CV 4/5] END max_features=None, n_estimators=10; AUC: (test=0.711) total time=   0.8s\n[CV 3/5] END max_features=None, n_estimators=50; AUC: (test=0.764) total time=   3.8s\n[CV 2/5] END max_features=None, n_estimators=100; AUC: (test=0.764) total time=   7.5s\n[CV 5/5] END max_features=None, n_estimators=100; AUC: (test=0.758) total time=   7.5s\n[CV 4/5] END max_features=None, n_estimators=200; AUC: (test=0.762) total time=  16.8s\n[CV 3/5] END max_features=None, n_estimators=500; AUC: (test=0.772) total time=  38.5s\n[CV 1/5] END hidden_layer_sizes=(1,), learning_rate_init=0.1; AUC: (test=0.696) total time=   0.2s\n[CV 3/5] END hidden_layer_sizes=(1,), learning_rate_init=0.01; AUC: (test=0.714) total time=   0.1s\n[CV 2/5] END hidden_layer_sizes=(1,), learning_rate_init=0.001; AUC: (test=0.712) total time=   0.2s\n[CV 4/5] END hidden_layer_sizes=(1,), learning_rate_init=0.001; AUC: (test=0.698) total time=   0.2s\n[CV 3/5] END hidden_layer_sizes=(1,), learning_rate_init=0.0001; AUC: (test=0.714) total time=   0.1s\n[CV 2/5] END hidden_layer_sizes=(2,), learning_rate_init=0.1; AUC: (test=0.745) total time=   3.5s\n[CV 3/5] END hidden_layer_sizes=(2,), learning_rate_init=0.01; AUC: (test=0.768) total time=   2.3s\n[CV 4/5] END hidden_layer_sizes=(2,), learning_rate_init=0.01; AUC: (test=0.752) total time=   0.7s\n[CV 2/5] END hidden_layer_sizes=(2,), learning_rate_init=0.001; AUC: (test=0.745) total time=   3.7s\n[CV 2/5] END hidden_layer_sizes=(2,), learning_rate_init=0.0001; AUC: (test=0.745) total time=   3.8s\n[CV 2/5] END hidden_layer_sizes=(3,), learning_rate_init=0.1; AUC: (test=0.768) total time=  23.3s\n[CV 2/5] END hidden_layer_sizes=(3,), learning_rate_init=0.001; AUC: (test=0.768) total time=  21.9s\n[CV 2/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.1; AUC: (test=0.780) total time=  23.5s\n[CV 4/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.01; AUC: (test=0.794) total time=  14.5s\n[CV 2/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.001; AUC: (test=0.780) total time=  23.5s\n[CV 4/5] END hidden_layer_sizes=(3, 3), learning_rate_init=0.0001; AUC: (test=0.794) total time=  14.2s\n[CV 2/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.1; AUC: (test=0.780) total time=   5.2s\n[CV 3/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.1; AUC: (test=0.788) total time=  22.6s\n[CV 5/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.01; AUC: (test=0.779) total time=  11.5s\n[CV 4/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.001; AUC: (test=0.792) total time=  10.7s\n[CV 2/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.0001; AUC: (test=0.780) total time=   5.2s\n[CV 4/5] END hidden_layer_sizes=(4, 2), learning_rate_init=0.0001; AUC: (test=0.792) total time=  11.0s\n[CV 2/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.1; AUC: (test=0.778) total time=  59.3s\n[CV 4/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.01; AUC: (test=0.785) total time=  21.2s\n[CV 2/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.001; AUC: (test=0.778) total time= 1.0min\n[CV 3/5] END hidden_layer_sizes=(5, 5), learning_rate_init=0.0001; AUC: (test=0.780) total time=  36.1s\n[CV 3/5] END max_features=None, n_estimators=10; AUC: (test=0.727) total time=   0.8s\n[CV 1/5] END max_features=None, n_estimators=50; AUC: (test=0.752) total time=   3.9s\n[CV 1/5] END max_features=None, n_estimators=100; AUC: (test=0.757) total time=   7.6s\n[CV 1/5] END max_features=None, n_estimators=200; AUC: (test=0.759) total time=  16.6s\n[CV 5/5] END max_features=None, n_estimators=200; AUC: (test=0.761) total time=  15.4s\n[CV 4/5] END max_features=None, n_estimators=500; AUC: (test=0.765) total time=  38.6s\n[CV 5/5] END max_features=None, n_estimators=500; AUC: (test=0.764) total time=  39.1s\n[CV 4/5] END max_features=None, n_estimators=1000; AUC: (test=0.765) total time= 1.4min\n[CV 3/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.787) total time=  11.4s\n[CV 1/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.772) total time=  23.0s\n[CV 3/5] END max_features=None, n_estimators=1000; AUC: (test=0.772) total time= 1.4min\n[CV 2/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.776) total time=   2.2s\n[CV 4/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.766) total time=   2.2s\n[CV 5/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.770) total time=   2.3s\n[CV 2/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.781) total time=   4.8s\n[CV 4/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.774) total time=   4.6s\n[CV 1/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.771) total time=  11.0s\n[CV 4/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.779) total time=  11.6s\n[CV 2/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.782) total time=  23.1s\n[CV 2/5] END max_features=None, n_estimators=1000; AUC: (test=0.768) total time= 1.3min\n[CV 1/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=10; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=50; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=100; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=200; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=500; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 2/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 3/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 4/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 5/5] END max_features=auto, n_estimators=1000; AUC: (test=nan) total time=   0.0s\n[CV 1/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.717) total time=   0.3s\n[CV 2/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.720) total time=   0.2s\n[CV 3/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.725) total time=   0.2s\n[CV 4/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.708) total time=   0.4s\n[CV 5/5] END max_features=sqrt, n_estimators=10; AUC: (test=0.713) total time=   0.2s\n[CV 1/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.760) total time=   1.2s\n[CV 2/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.764) total time=   1.4s\n[CV 3/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.775) total time=   1.2s\n[CV 4/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.757) total time=   1.2s\n[CV 5/5] END max_features=sqrt, n_estimators=50; AUC: (test=0.766) total time=   1.2s\n[CV 1/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.767) total time=   2.3s\n[CV 3/5] END max_features=sqrt, n_estimators=100; AUC: (test=0.781) total time=   2.3s\n[CV 1/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.771) total time=   4.9s\n[CV 3/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.784) total time=   4.7s\n[CV 5/5] END max_features=sqrt, n_estimators=200; AUC: (test=0.776) total time=   4.6s\n[CV 2/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.781) total time=  11.6s\n[CV 5/5] END max_features=sqrt, n_estimators=500; AUC: (test=0.777) total time=  11.4s\n[CV 3/5] END max_features=sqrt, n_estimators=1000; AUC: (test=0.789) total time=  23.2s\n[CV 1/5] END max_features=log2, n_estimators=10; AUC: (test=0.717) total time=   0.2s\n[CV 3/5] END max_features=log2, n_estimators=10; AUC: (test=0.725) total time=   0.2s\n[CV 5/5] END max_features=log2, n_estimators=10; AUC: (test=0.713) total time=   0.2s\n[CV 2/5] END max_features=log2, n_estimators=50; AUC: (test=0.764) total time=   1.1s\n[CV 4/5] END max_features=log2, n_estimators=50; AUC: (test=0.757) total time=   1.1s\n[CV 1/5] END max_features=log2, n_estimators=100; AUC: (test=0.767) total time=   2.3s\n[CV 3/5] END max_features=log2, n_estimators=100; AUC: (test=0.781) total time=   2.3s\n[CV 5/5] END max_features=log2, n_estimators=100; AUC: (test=0.770) total time=   2.5s\n[CV 2/5] END max_features=log2, n_estimators=200; AUC: (test=0.781) total time=   4.5s\n[CV 4/5] END max_features=log2, n_estimators=200; AUC: (test=0.774) total time=   4.5s\n[CV 2/5] END max_features=log2, n_estimators=500; AUC: (test=0.781) total time=  11.0s\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning:\n\nA worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/ensemble/_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n  warn(\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning:\n\n\n120 fits failed out of a total of 330.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 2.0 instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 3.0 instead.\n\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 1467, in wrapper\n    estimator._validate_params()\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 4.0 instead.\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/sklearn/model_selection/_search.py:1051: UserWarning:\n\nOne or more of the test scores are non-finite: [0.71732593 0.75550047 0.76098577 0.76354732 0.76619619 0.7663353\n        nan        nan        nan        nan        nan        nan\n 0.71661193 0.76449763 0.7718989  0.77712755 0.77905288 0.78021496\n 0.71661193 0.76449763 0.7718989  0.77712755 0.77905288 0.78021496\n 0.71661193 0.76449763 0.7718989  0.77712755 0.77905288 0.78021496\n 0.71795268 0.76238447 0.76870114 0.77133255 0.77269637 0.77393492\n 0.71287995 0.75707753 0.76285869 0.76608659 0.76859789 0.76972414\n 0.71732593 0.75550047 0.76098577 0.76354732 0.76619619 0.7663353\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan        nan]\n\n\n\nGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(oob_score=True,\n                                              random_state=1234),\n             n_jobs=4,\n             param_grid={'max_features': [None, 'auto', 'sqrt', 'log2', 0.25,\n                                          0.5, 0.75, 1.0, 2.0, 3.0, 4.0],\n                         'n_estimators': [10, 50, 100, 200, 500, 1000]},\n             refit='AUC', scoring={'AUC': 'roc_auc'}, verbose=5) estimator: RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) ¬†RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(oob_score=True, random_state=1234) \n\n\n\n\nCode\nrf_cv.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 1000}\n\n\n\n\nRetrain the model with the best hyperparameters\n\n\nCode\nrf = rsm.model.rforest(\n    data = {'cg_ad_campaign': cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 1]},\n    rvar = 'converted_yes',\n    evar = [\n        \"GameLevel\",\n        \"NumGameDays\",\n        \"NumGameDays4Plus\",\n        \"NumInGameMessagesSent\",\n        \"NumFriends\",\n        \"NumFriendRequestIgnored\",\n        \"NumSpaceHeroBadges\",\n        \"AcquiredSpaceship\",\n        \"AcquiredIonWeapon\",\n        \"TimesLostSpaceship\",\n        \"TimesKilled\",\n        \"TimesCaptain\",\n        \"TimesNavigator\",\n        \"PurchasedCoinPackSmall\",\n        \"PurchasedCoinPackLarge\",\n        \"NumAdsClicked\",\n        \"DaysUser\",\n        \"UserConsole\",\n        \"UserHasOldOS\"\n    ],\n    n_estimators = 1000,\n    max_features = 0.25,\n)\nrf.summary()\n\n\nRandom Forest\nData                 : cg_ad_campaign\nResponse variable    : converted_yes\nLevel                : None\nExplanatory variables: GameLevel, NumGameDays, NumGameDays4Plus, NumInGameMessagesSent, NumFriends, NumFriendRequestIgnored, NumSpaceHeroBadges, AcquiredSpaceship, AcquiredIonWeapon, TimesLostSpaceship, TimesKilled, TimesCaptain, TimesNavigator, PurchasedCoinPackSmall, PurchasedCoinPackLarge, NumAdsClicked, DaysUser, UserConsole, UserHasOldOS\nOOB                  : True\nModel type           : classification\nNr. of features      : (19, 19)\nNr. of observations  : 30,000\nmax_features         : 0.25 (0)\nn_estimators         : 1000\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.78\n\nEstimation data      :\n GameLevel  NumGameDays  NumGameDays4Plus  NumInGameMessagesSent  NumFriends  NumFriendRequestIgnored  NumSpaceHeroBadges  TimesLostSpaceship  TimesKilled  TimesCaptain  TimesNavigator  NumAdsClicked  DaysUser  AcquiredSpaceship_yes  AcquiredIonWeapon_yes  PurchasedCoinPackSmall_yes  PurchasedCoinPackLarge_yes  UserConsole_yes  UserHasOldOS_yes\n         2            8                 0                      0           4                        5                   0                   0            0             8               0              3      1889                  False                  False                        True                       False            False              True\n         5           15                 0                    179         362                       50                   0                  22            0             4               4              2      1308                   True                  False                       False                       False             True             False\n         7            7                 0                    267           0                       64                   0                   5            0             0               0              1      3562                  False                  False                       False                        True             True             False\n         4            4                 0                     36           0                        0                   0                   0            0             0               0              2      2922                  False                  False                       False                       False             True             False\n         8           17                 0                    222          20                       63                  10                  10            0             9               6              4      2192                   True                  False                        True                       False             True             False\n\n\n\n\nCode\ncg_ad_treatment['pred_rf'] = rf.predict(cg_ad_treatment)['prediction']\n\n\n\n\nCode\nfig = rsm.gains_plot(\n    cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0],\n    \"converted\", \"yes\",\n    [\"pred_logit_ad\", \"pred_rf\"]\n)\n\n\n\n\n\n\n\n\n\nAt the initial stages (when a small percentage of the population is targeted), the random forest model captures a higher percentage of the total response compared to the logistic regression model. This suggest that the random forest model is better at capturing the response in the early stages of targeting.\nAs more the population is targeted, both models‚Äô curves flatten out, which is expected as the company each out to less likely buyers, the gain from each additional percentage of the population targeted decreases. However, the random forest model, which has been fine-tuned using grid search on hyperparameters like max features and number of estimators, appears to outperform the logistic regression model at all levels of the population targeted.\nThe reason why the random forest perform better:\n\nNon-linearity: Random forest can capture complex, non-linear relationships between the features and the target variable, which logistic regression may not able to model as effectively.\nInteraction effects: Random forest naturally account for interaction between features without needing explicit feature engineering.\nHyperparameter tuning: The Grid Search process likely helped i finding an optimal combination of hyperparameters for the Random Forest model, enhancing its ability to generalize.\n\n\n\nCode\n# prediction on rnd_30k == 0 for neutral network \"ad\" model\npred = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"pred_rf\"]\nactual = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0][\"converted_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n\n0.781\n\n\nThe Random Forest model has AUC of 0.78, indicating that it has a good performance, with 78% chance of correctly distinguishing between a converting and non-converting customer. This is higher than the AUC of the logistic regression model, which is 0.703. This confirms the visual interpretation from the gain chart, where the random forest model outperforms the logistic regression model.\nThis means that the Random Forest model would likely lead to a better identification of positive cases, which could be more efficient and potentially more profitable.\n\n\n\nCalculate the profit improvement of the random forest model\n\n\nCode\nrf_group3 = cg_ad_treatment[cg_ad_treatment['rnd_30k'] == 0].nlargest(30000, 'pred_rf')\nrf_response_rate, rf_profit = profit_calc(rf_group3)\n\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_5776/2151466375.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\nCode\nmod_pef = pd.DataFrame({\n    \"model\": [\"Random Forest\", \"Ads\"],\n    \"Response Rate\": [rf_response_rate, ads_response_rate],\n    \"Profit\": [rf_profit, ads_profit]\n})\nmod_pef\n\n\n\n\n\n\n\n\n\n\nmodel\nResponse Rate\nProfit\n\n\n\n\n0\nRandom Forest\n0.3119\n95261.43\n\n\n1\nAds\n0.2739\n78172.83\n\n\n\n\n\n\n\n\n\n\nCode\nprofit_improvement = rf_profit - ads_profit\nprofit_improvement\n\n\n17088.60000000002\n\n\nBased on the calculation above, the profit improvement from using Random Forest model over the logistic regression model ad is 15,949.36, this represents a 12% improvement in profit when using Random Forest model over the logistic regression model ad.\nThis aligns with the gain chart and AUC, which both show that the Random Forest model outperforms the logistic regression model in terms of capturing the response and overall accuracy."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html",
    "href": "projects/Pentathlon/pentathlon_nptb.html",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "",
    "text": "PROJECT SUMMARY\n‚Ä¢ Project Goal: Optimize profit by tailoring email campaigns for 7 departments to individual customers based on predictive modeling.\nModeling Approach:\n‚Ä¢ Logistic Regression: Utilized for its ability to predict the probability of customer engagement based on email content. ‚Ä¢ Linear Regression: Employed to predict average order size from each customer as a continuous outcome. ‚Ä¢ Random Forest: Chosen for its robustness and ability to capture non-linear patterns and feature interactions. ‚Ä¢ Neural Network: Applied to leverage its deep learning capabilities in identifying complex patterns within customer data. ‚Ä¢ XGBoost:Selectedforitse iciency,modelperformance,andabilitytohandleavarietyofdata types and distributions.\nModel Training and Tuning:\n‚Ä¢ Conducted comprehensive training and hyperparameter tuning for each model to ensure op- timal performance. ‚Ä¢ Employed methods like cross-validation and grid search, particularly for computationally intensive models like Random Forest.\nProfit Analysis: ‚Ä¢ Evaluated the predicted profit increase from each model to identify the most effective ap- proach. ‚Ä¢ Analyzed model-specific trade-offs, such as training time (e.g., Random Forest‚Äôs extensive training period).\nEmail Policy Evaluation: ‚Ä¢ Assessed the impact of each model on the email policy proposal to ensure alignment with profit optimization goals."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#data-preparation",
    "href": "projects/Pentathlon/pentathlon_nptb.html#data-preparation",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nimport pandas as pd\nimport pyrsm as rsm\n\n\n## loading the data - this dataset must NOT be changed\npentathlon_nptb = pd.read_parquet(\"pentathlon_nptb.parquet\")\n\n\nPentathon: Next Product To Buy\nThe available data is based on the last e-mail sent to each Pentathlon customer. Hence, an observation or row in the data is a ‚Äúcustomer-promotional e-mail‚Äù pair. The data contains the following basic demographic information available to Pentathlon:\n\n‚Äúage‚Äù: Customer age(coded in 4 buckets:‚Äú&lt;30‚Äù, ‚Äú30 to 44‚Äù, ‚Äú45 to 59‚Äù, and ‚Äú&gt;=60‚Äù)\n‚Äúfemale‚Äù: Gender identity coded as Female ‚Äúyes‚Äù or ‚Äúno‚Äù\n‚Äúincome‚Äù: Income in Euros, rounded to the nearest EUR5,000\n‚Äúeducation‚Äù: Percentage of college graduates in the customer‚Äôs neighborhood, coded from 0-100\n‚Äúchildren‚Äù: Average number of children in the customer‚Äôs neighborhood\n\nThe data also contains basic historical information about customer purchases, specifically, a department-specific frequency measure.\n\n‚Äúfreq_endurance-freq_racquet‚Äù: Number of purchases in each department in the last year, excluding any purchase in response to the last email.\n\nThe key outcome variables are:\n\n‚Äúbuyer‚Äù: Did the customer click on the e-mail and complete a purchase within two days of receiving the e-mail (‚Äúyes‚Äù or ‚Äúno‚Äù)?\n‚Äútotal_os‚Äù: Total order size (in Euros) conditional on the customer having purchased (buyer == ‚Äúyes‚Äù). This measures spending across all departments, not just the department that sent the message\n\n\nNote: In addition to the six message groups, a seventh group of customers received no promotional e-mails for the duration of the test (‚Äúcontrol‚Äù).\n\n\npentathlon_nptb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 600000 entries, 0 to 599999\nData columns (total 16 columns):\n #   Column            Non-Null Count   Dtype   \n---  ------            --------------   -----   \n 0   custid            600000 non-null  object  \n 1   buyer             600000 non-null  category\n 2   total_os          600000 non-null  int32   \n 3   message           600000 non-null  category\n 4   age               600000 non-null  category\n 5   female            600000 non-null  category\n 6   income            600000 non-null  int32   \n 7   education         600000 non-null  int32   \n 8   children          600000 non-null  float64 \n 9   freq_endurance    600000 non-null  int32   \n 10  freq_strength     600000 non-null  int32   \n 11  freq_water        600000 non-null  int32   \n 12  freq_team         600000 non-null  int32   \n 13  freq_backcountry  600000 non-null  int32   \n 14  freq_racquet      600000 non-null  int32   \n 15  training          600000 non-null  float64 \ndtypes: category(4), float64(2), int32(9), object(1)\nmemory usage: 36.6+ MB\n\n\n\npentathlon_nptb['buyer_yes'] = rsm.ifelse(pentathlon_nptb['buyer'] == 'yes', 1, 0)\npentathlon_nptb['buyer_yes'].value_counts()\n\nbuyer_yes\n0    585600\n1     14400\nName: count, dtype: int64\n\n\n\nCheck the training data and testing data, make sure they are splitted a 70-30 ratio\n\npd.crosstab(pentathlon_nptb[\"message\"], pentathlon_nptb[\"training\"])\n\n\n\n\n\n\n\n\ntraining\n0.0\n1.0\n\n\nmessage\n\n\n\n\n\n\nbackcountry\n26179\n60425\n\n\ncontrol\n26043\n61217\n\n\nendurance\n24773\n58083\n\n\nracquet\n26316\n60772\n\n\nstrength\n25251\n59029\n\n\nteam\n25942\n60850\n\n\nwater\n25496\n59624\n\n\n\n\n\n\n\n\n\npentathlon_nptb.training.value_counts(normalize=True)\n\ntraining\n1.0    0.7\n0.0    0.3\nName: proportion, dtype: float64\n\n\n\npentathlon_nptb.hist(figsize=(10, 10), bins=30)\n\narray([[&lt;Axes: title={'center': 'total_os'}&gt;,\n        &lt;Axes: title={'center': 'income'}&gt;,\n        &lt;Axes: title={'center': 'education'}&gt;],\n       [&lt;Axes: title={'center': 'children'}&gt;,\n        &lt;Axes: title={'center': 'freq_endurance'}&gt;,\n        &lt;Axes: title={'center': 'freq_strength'}&gt;],\n       [&lt;Axes: title={'center': 'freq_water'}&gt;,\n        &lt;Axes: title={'center': 'freq_team'}&gt;,\n        &lt;Axes: title={'center': 'freq_backcountry'}&gt;],\n       [&lt;Axes: title={'center': 'freq_racquet'}&gt;,\n        &lt;Axes: title={'center': 'training'}&gt;,\n        &lt;Axes: title={'center': 'buyer_yes'}&gt;]], dtype=object)"
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#logistic-regression-model",
    "href": "projects/Pentathlon/pentathlon_nptb.html#logistic-regression-model",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nCheck cross tab between the outcome variable and the key predictor variable\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"age\")\nct.summary(output = \"perc_row\")\nct.plot(plots = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, age\nNull hyp : There is no association between buyer and age\nAlt. hyp : There is an association between buyer and age\n\nRow percentages:\n\nage     &lt; 30 30 to 44 45 to 59   &gt;= 60   Total\nbuyer                                         \nyes    9.42%   40.74%   38.18%  11.65%  100.0%\nno     17.7%   31.16%   32.42%  18.72%  100.0%\nTotal  17.5%   31.39%   32.56%  18.55%  100.0%\n\nChi-squared: 1038.94 df(3), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\n\n\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"children\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, children\nNull hyp : There is no association between buyer and children\nAlt. hyp : There is an association between buyer and children\n\nRow percentages:\n\nchildren    0.2    0.3    0.4    0.5    0.6     0.7    0.8     0.9     1.0  \\\nbuyer                                                                        \nyes       1.58%  2.16%  3.52%  4.06%  5.13%   7.48%  7.55%  10.71%  10.63%   \nno        2.98%   4.2%  6.18%  7.23%   7.9%  10.42%  9.56%  10.81%   9.11%   \nTotal     2.94%  4.15%  6.12%  7.15%  7.84%  10.35%  9.51%   10.8%   9.15%   \n\nchildren    1.1  ...   4.5   4.6   4.7    4.8   4.9   5.0   5.1   5.2   5.8  \\\nbuyer            ...                                                          \nyes       9.74%  ...  0.0%  0.0%  0.0%  0.01%  0.0%  0.0%  0.0%  0.0%  0.0%   \nno        7.18%  ...  0.0%  0.0%  0.0%   0.0%  0.0%  0.0%  0.0%  0.0%  0.0%   \nTotal     7.24%  ...  0.0%  0.0%  0.0%   0.0%  0.0%  0.0%  0.0%  0.0%  0.0%   \n\nchildren   Total  \nbuyer             \nyes       100.0%  \nno        100.0%  \nTotal     100.0%  \n\n[3 rows x 53 columns]\n\nChi-squared: 2028.29 df(51), p.value &lt; .001\n700.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"message\")\nct.summary(output = \"perc_row\")\nct.plot(plots = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, message\nNull hyp : There is no association between buyer and message\nAlt. hyp : There is an association between buyer and message\n\nRow percentages:\n\nmessage backcountry control endurance racquet strength    team   water   Total\nbuyer                                                                         \nyes          13.97%  13.27%    15.37%  13.77%   14.85%  14.58%  14.19%  100.0%\nno            14.4%  14.61%    13.79%  14.49%   14.03%  14.49%   14.2%  100.0%\nTotal        14.39%  14.58%    13.83%  14.47%   14.05%  14.49%   14.2%  100.0%\n\nChi-squared: 39.15 df(6), p.value &lt; .001\n0.0% of cells have expected values below 5\n\n\n\n\n\n\n\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_endurance\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_endurance\nNull hyp : There is no association between buyer and freq_endurance\nAlt. hyp : There is an association between buyer and freq_endurance\n\nRow percentages:\n\nfreq_endurance       0       1       2       3      4      5      6      7  \\\nbuyer                                                                        \nyes              24.2%  16.15%  14.66%  12.23%  9.58%  7.76%  5.24%  3.76%   \nno              56.16%   22.7%  10.21%   5.33%  2.82%  1.42%  0.72%  0.34%   \nTotal            55.4%  22.54%  10.32%    5.5%  2.98%  1.58%  0.83%  0.42%   \n\nfreq_endurance      8      9     10     11     12     13     14     15   Total  \nbuyer                                                                           \nyes             2.56%  1.72%  0.97%  0.57%  0.34%  0.14%  0.11%  0.02%  100.0%  \nno              0.16%  0.08%  0.03%  0.01%  0.01%   0.0%   0.0%   0.0%  100.0%  \nTotal           0.22%  0.12%  0.05%  0.03%  0.01%  0.01%   0.0%   0.0%  100.0%  \n\nChi-squared: 21259.53 df(15), p.value &lt; .001\n150.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_strength\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_strength\nNull hyp : There is no association between buyer and freq_strength\nAlt. hyp : There is an association between buyer and freq_strength\n\nRow percentages:\n\nfreq_strength       0       1       2      3      4      5      6      7  \\\nbuyer                                                                      \nyes             21.4%   9.33%   9.92%  8.99%  8.33%  8.32%  6.73%  6.43%   \nno             46.85%   18.9%  11.59%  7.77%  5.32%  3.54%  2.26%  1.51%   \nTotal          46.24%  18.67%  11.55%   7.8%   5.4%  3.65%  2.37%  1.63%   \n\nfreq_strength      8      9  ...     15     16     17     18     19    20  \\\nbuyer                        ...                                            \nyes             5.1%  4.09%  ...  0.65%  0.51%  0.23%  0.25%  0.11%  0.1%   \nno             0.94%  0.55%  ...  0.02%  0.01%  0.01%   0.0%   0.0%  0.0%   \nTotal          1.04%  0.64%  ...  0.03%  0.02%  0.01%  0.01%   0.0%  0.0%   \n\nfreq_strength     21     22     23   Total  \nbuyer                                       \nyes            0.04%  0.05%  0.01%  100.0%  \nno              0.0%   0.0%   0.0%  100.0%  \nTotal           0.0%   0.0%   0.0%  100.0%  \n\n[3 rows x 25 columns]\n\nChi-squared: 22032.18 df(23), p.value &lt; .001\n300.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_water\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_water\nNull hyp : There is no association between buyer and freq_water\nAlt. hyp : There is an association between buyer and freq_water\n\nRow percentages:\n\nfreq_water       0       1      2      3      4      5      6      7      8  \\\nbuyer                                                                         \nyes         64.79%  18.38%   9.3%   4.7%  1.88%  0.66%  0.21%  0.06%  0.01%   \nno          92.53%   5.72%  1.33%  0.32%  0.08%  0.01%   0.0%   0.0%   0.0%   \nTotal       91.87%   6.03%  1.52%  0.42%  0.12%  0.03%  0.01%   0.0%   0.0%   \n\nfreq_water   Total  \nbuyer               \nyes         100.0%  \nno          100.0%  \nTotal       100.0%  \n\nChi-squared: 16794.94 df(8), p.value &lt; .001\n125.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_team\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_team\nNull hyp : There is no association between buyer and freq_team\nAlt. hyp : There is an association between buyer and freq_team\n\nRow percentages:\n\nfreq_team       0       1       2      3      4      5      6      7      8  \\\nbuyer                                                                         \nyes         39.2%  10.78%   9.88%  9.12%  8.45%   6.8%  4.74%  4.07%  2.71%   \nno         59.12%  16.61%  10.14%  6.28%  3.73%  2.08%  1.08%  0.53%  0.24%   \nTotal      58.64%  16.47%  10.14%  6.35%  3.84%  2.19%  1.17%  0.61%   0.3%   \n\nfreq_team      9     10     11     12     13     14     15     16   Total  \nbuyer                                                                      \nyes        1.87%  1.07%  0.62%  0.32%  0.12%   0.2%  0.05%  0.02%  100.0%  \nno         0.11%  0.05%  0.02%  0.01%   0.0%   0.0%   0.0%   0.0%  100.0%  \nTotal      0.16%  0.07%  0.04%  0.02%  0.01%  0.01%   0.0%   0.0%  100.0%  \n\nChi-squared: 13743.4 df(16), p.value &lt; .001\n175.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_backcountry\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_backcountry\nNull hyp : There is no association between buyer and freq_backcountry\nAlt. hyp : There is an association between buyer and freq_backcountry\n\nRow percentages:\n\nfreq_backcountry       0       1       2       3      4      5      6   Total\nbuyer                                                                        \nyes               45.69%  20.77%  18.53%  10.39%  3.85%  0.73%  0.03%  100.0%\nno                61.34%  28.61%   8.24%   1.57%  0.22%  0.02%   0.0%  100.0%\nTotal             60.97%  28.42%   8.49%   1.78%   0.3%  0.03%   0.0%  100.0%\n\nChi-squared: 11914.73 df(6), p.value &lt; .001\n50.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"freq_racquet\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, freq_racquet\nNull hyp : There is no association between buyer and freq_racquet\nAlt. hyp : There is an association between buyer and freq_racquet\n\nRow percentages:\n\nfreq_racquet       0       1       2       3       4      5      6      7  \\\nbuyer                                                                       \nyes           23.83%  18.48%   17.7%  13.82%  10.43%  7.43%  4.32%  2.42%   \nno            48.75%  28.86%  12.68%   5.76%   2.48%  0.97%  0.34%  0.12%   \nTotal         48.15%  28.61%   12.8%   5.95%   2.68%  1.13%  0.44%  0.17%   \n\nfreq_racquet      8      9     10     11   Total  \nbuyer                                             \nyes           0.97%  0.46%  0.13%  0.02%  100.0%  \nno            0.03%  0.01%   0.0%   0.0%  100.0%  \nTotal         0.05%  0.02%   0.0%   0.0%  100.0%  \n\nChi-squared: 18774.72 df(11), p.value &lt; .001\n100.0% of cells have expected values below 5\n\n\n\n\nct = rsm.basics.cross_tabs(pentathlon_nptb.query(\"training == 1\"), \"buyer\", \"income\")\nct.summary(output = \"perc_row\")\n\n\nCross-tabs\nData     : Not provided\nVariables: buyer, income\nNull hyp : There is no association between buyer and income\nAlt. hyp : There is an association between buyer and income\n\nRow percentages:\n\nincome     0   5000  10000  15000  20000  25000  30000   35000   40000  \\\nbuyer                                                                    \nyes     0.0%  0.01%   0.0%  0.16%  0.26%  0.52%  0.84%    1.3%   2.22%   \nno      0.0%  0.03%  0.19%  0.73%  2.13%  4.78%  8.77%  11.78%   14.0%   \nTotal   0.0%  0.03%  0.18%  0.71%  2.09%  4.68%  8.58%  11.53%  13.72%   \n\nincome   45000  ... 160000 165000 170000 175000 180000 185000 190000 195000  \\\nbuyer           ...                                                           \nyes      3.83%  ...  0.26%  0.37%  0.19%  0.14%  0.19%  0.14%   0.1%  0.11%   \nno      12.71%  ...   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   \nTotal    12.5%  ...  0.01%  0.01%  0.01%  0.01%  0.01%   0.0%   0.0%   0.0%   \n\nincome 200000   Total  \nbuyer                  \nyes     0.54%  100.0%  \nno      0.01%  100.0%  \nTotal   0.02%  100.0%  \n\n[3 rows x 42 columns]\n\nChi-squared: 40347.2 df(40), p.value &lt; .001\n400.0% of cells have expected values below 5\n\n\n\n\npentathlon_nptb[pentathlon_nptb[\"training\"] == 1]\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\nfreq_strength\nfreq_water\nfreq_team\nfreq_backcountry\nfreq_racquet\ntraining\nbuyer_yes\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n4\n0\n4\n0\n1\n1.0\n0\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n0\n0\n0\n2\n2\n1.0\n0\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n0\n0\n0\n0\n0\n1.0\n0\n\n\n5\nU28\nno\n0\nstrength\n&lt; 30\nyes\n25000\n18\n0.3\n0\n0\n0\n0\n0\n0\n1.0\n0\n\n\n8\nU59\nno\n0\nstrength\n&gt;= 60\nyes\n65000\n36\n1.2\n1\n1\n0\n2\n0\n3\n1.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599989\nU3462835\nno\n0\ncontrol\n30 to 44\nyes\n45000\n20\n0.8\n1\n0\n0\n0\n0\n1\n1.0\n0\n\n\n599992\nU3462858\nno\n0\ncontrol\n45 to 59\nyes\n40000\n16\n1.1\n0\n0\n0\n0\n0\n0\n1.0\n0\n\n\n599993\nU3462877\nno\n0\nbackcountry\n&lt; 30\nno\n65000\n30\n1.0\n2\n3\n1\n0\n1\n1\n1.0\n0\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n0\n0\n0\n0\n0\n1.0\n0\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n5\n0\n2\n1\n2\n1.0\n0\n\n\n\n\n420000 rows √ó 17 columns\n\n\n\n\n\n\nBuild a logistic regression model\n\nevar = pentathlon_nptb.columns.to_list()\nevar = evar[evar.index(\"message\"):]\nevar = evar[:evar.index(\"freq_racquet\")+1]\nevar\n\n['message',\n 'age',\n 'female',\n 'income',\n 'education',\n 'children',\n 'freq_endurance',\n 'freq_strength',\n 'freq_water',\n 'freq_team',\n 'freq_backcountry',\n 'freq_racquet']\n\n\n\nlr = rsm.model.logistic(\n    data = {\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar = \"buyer_yes\",\n    evar = evar,\n)\nlr.summary()\n\nLogistic regression (GLM)\nData                 : pentathlon_nptb\nResponse variable    : buyer_yes\nLevel                : None\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nNull hyp.: There is no effect of x on buyer_yes\nAlt. hyp.: There is an effect of x on buyer_yes\n\n                       OR      OR%  coefficient  std.error  z.value p.value     \nIntercept           0.000  -100.0%        -8.28      0.064 -128.837  &lt; .001  ***\nmessage[control]    0.906    -9.4%        -0.10      0.042   -2.335    0.02    *\nmessage[endurance]  1.250    25.0%         0.22      0.041    5.462  &lt; .001  ***\nmessage[racquet]    0.993    -0.7%        -0.01      0.042   -0.171   0.864     \nmessage[strength]   1.162    16.2%         0.15      0.041    3.660  &lt; .001  ***\nmessage[team]       1.035     3.5%         0.03      0.041    0.823    0.41     \nmessage[water]      1.056     5.6%         0.05      0.042    1.311    0.19     \nage[30 to 44]       2.386   138.6%         0.87      0.039   22.353  &lt; .001  ***\nage[45 to 59]       2.186   118.6%         0.78      0.040   19.555  &lt; .001  ***\nage[&gt;= 60]          1.201    20.1%         0.18      0.048    3.805  &lt; .001  ***\nfemale[no]          1.353    35.3%         0.30      0.024   12.574  &lt; .001  ***\nincome              1.000     0.0%         0.00      0.000   22.105  &lt; .001  ***\neducation           1.037     3.7%         0.04      0.001   32.809  &lt; .001  ***\nchildren            1.618    61.8%         0.48      0.030   16.210  &lt; .001  ***\nfreq_endurance      1.101    10.1%         0.10      0.005   18.300  &lt; .001  ***\nfreq_strength       1.115    11.5%         0.11      0.003   33.044  &lt; .001  ***\nfreq_water          1.178    17.8%         0.16      0.014   12.026  &lt; .001  ***\nfreq_team           1.050     5.0%         0.05      0.004   10.797  &lt; .001  ***\nfreq_backcountry    1.188    18.8%         0.17      0.010   17.313  &lt; .001  ***\nfreq_racquet        1.110    11.0%         0.10      0.007   15.866  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.261\nPseudo R-squared (McFadden adjusted): 0.26\nArea under the RO Curve (AUC): 0.884\nLog-likelihood: -35159.009, AIC: 70358.018, BIC: 70576.978\nChi-squared: 24788.884, df(19), p.value &lt; 0.001 \nNr obs: 420,000\n\n\n\nlr.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\nOR\nOR%\ncoefficient\nstd.error\nz.value\np.value\n\n\n\n\n\n0\nIntercept\n0.000\n-99.975\n-8.283\n0.064\n-128.837\n0.000\n***\n\n\n1\nmessage[T.control]\n0.906\n-9.372\n-0.098\n0.042\n-2.335\n0.020\n*\n\n\n2\nmessage[T.endurance]\n1.250\n24.991\n0.223\n0.041\n5.462\n0.000\n***\n\n\n3\nmessage[T.racquet]\n0.993\n-0.711\n-0.007\n0.042\n-0.171\n0.864\n\n\n\n4\nmessage[T.strength]\n1.162\n16.227\n0.150\n0.041\n3.660\n0.000\n***\n\n\n5\nmessage[T.team]\n1.035\n3.458\n0.034\n0.041\n0.823\n0.410\n\n\n\n6\nmessage[T.water]\n1.056\n5.606\n0.055\n0.042\n1.311\n0.190\n\n\n\n7\nage[T.30 to 44]\n2.386\n138.587\n0.870\n0.039\n22.353\n0.000\n***\n\n\n8\nage[T.45 to 59]\n2.186\n118.557\n0.782\n0.040\n19.555\n0.000\n***\n\n\n9\nage[T.&gt;= 60]\n1.201\n20.099\n0.183\n0.048\n3.805\n0.000\n***\n\n\n10\nfemale[T.no]\n1.353\n35.328\n0.303\n0.024\n12.574\n0.000\n***\n\n\n11\nincome\n1.000\n0.002\n0.000\n0.000\n22.105\n0.000\n***\n\n\n12\neducation\n1.037\n3.736\n0.037\n0.001\n32.809\n0.000\n***\n\n\n13\nchildren\n1.618\n61.764\n0.481\n0.030\n16.210\n0.000\n***\n\n\n14\nfreq_endurance\n1.101\n10.083\n0.096\n0.005\n18.300\n0.000\n***\n\n\n15\nfreq_strength\n1.115\n11.550\n0.109\n0.003\n33.044\n0.000\n***\n\n\n16\nfreq_water\n1.178\n17.847\n0.164\n0.014\n12.026\n0.000\n***\n\n\n17\nfreq_team\n1.050\n4.978\n0.049\n0.004\n10.797\n0.000\n***\n\n\n18\nfreq_backcountry\n1.188\n18.795\n0.172\n0.010\n17.313\n0.000\n***\n\n\n19\nfreq_racquet\n1.110\n11.008\n0.104\n0.007\n15.866\n0.000\n***\n\n\n\n\n\n\n\n\n\nlr.summary(main = False, vif = True)\n\n\nPseudo R-squared (McFadden): 0.261\nPseudo R-squared (McFadden adjusted): 0.26\nArea under the RO Curve (AUC): 0.884\nLog-likelihood: -35159.009, AIC: 70358.018, BIC: 70576.978\nChi-squared: 24788.884, df(19), p.value &lt; 0.001 \nNr obs: 420,000\n\nVariance inflation factors:\n\n                    vif    Rsq\neducation         3.219  0.689\nincome            2.814  0.645\nfreq_endurance    1.686  0.407\nfreq_racquet      1.603  0.376\nfreq_strength     1.470  0.320\nchildren          1.423  0.297\nfreq_team         1.339  0.253\nage               1.307  0.235\nfreq_water        1.268  0.212\nfreq_backcountry  1.145  0.126\nfemale            1.112  0.101\nmessage           1.001  0.001\n\n\n\nlr.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\npentathlon_nptb['pred_lr'] = lr.predict(pentathlon_nptb)['prediction']\n\n\ndct = {\"train\" : pentathlon_nptb.query(\"training == 1\"), \"test\" : pentathlon_nptb.query(\"training == 0\")}\nfig = rsm.gains_plot(dct, \"buyer\", \"yes\", \"pred_lr\")\n\n\n\n\n\n\n\n\n\nfrom sklearn import metrics\n\n# prediction on training set\npred = pentathlon_nptb.query(\"training == 1\")['pred_lr']\nactual = pentathlon_nptb.query(\"training == 1\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.884\n\n\n\n# prediction on test set\npred = pentathlon_nptb.query(\"training == 0\")['pred_lr']\nactual = pentathlon_nptb.query(\"training == 0\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.883\n\n\nWe can conclude that:\nThe AUC for the training set is 0.884, which indicates that the model has a very good ability to distinguish between the two classes in the training data.\nThe AUC for the test set is 0.883, which is slightly lower but still indicates a very good predictive performance on unseen data.\nThe fact that the test AUC is close to the training AUC suggests that the model is generalizing well and not overfitting significantly to the training data. A small decrease from training to test set performance is normal because models will usually perform slightly better on the data they were trained on.\n\n\n1. For each customer determine the message (i.e., endurance, strength, water, team, backcountry, racquet, or no-message) predicted to lead to the highest probability of purchase\n\n# Check the department of the message variable\npentathlon_nptb[\"message\"].value_counts()\n\nmessage\ncontrol        87260\nracquet        87088\nteam           86792\nbackcountry    86604\nwater          85120\nstrength       84280\nendurance      82856\nName: count, dtype: int64\n\n\n\n# Create predictions\npentathlon_nptb[\"p_control_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb[\"p_racquet_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb[\"p_team_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb[\"p_backcountry_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb[\"p_water_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb[\"p_strength_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb[\"p_endurance_lr\"] = lr.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\npentathlon_nptb\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\ntraining\nbuyer_yes\npred_lr\np_control_lr\np_racquet_lr\np_team_lr\np_backcountry_lr\np_water_lr\np_strength_lr\np_endurance_lr\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\n1.0\n0\n0.013031\n0.011433\n0.012512\n0.013031\n0.012601\n0.013298\n0.014615\n0.015700\n\n\n1\nU3\nno\n0\nbackcountry\n45 to 59\nno\n35000\n22\n1.0\n0\n...\n0.0\n0\n0.005123\n0.004645\n0.005086\n0.005299\n0.005123\n0.005408\n0.005949\n0.006395\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\n1.0\n0\n0.011948\n0.008692\n0.009515\n0.009910\n0.009582\n0.010114\n0.011120\n0.011948\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\n1.0\n0\n0.002361\n0.002027\n0.002220\n0.002313\n0.002236\n0.002361\n0.002598\n0.002793\n\n\n4\nU25\nno\n0\nracquet\n&gt;= 60\nyes\n65000\n32\n1.1\n1\n...\n0.0\n0\n0.011717\n0.010706\n0.011717\n0.012203\n0.011800\n0.012453\n0.013688\n0.014705\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\n1.0\n0\n0.002182\n0.001873\n0.002052\n0.002138\n0.002067\n0.002182\n0.002401\n0.002582\n\n\n599996\nU3462900\nno\n0\nteam\n&lt; 30\nno\n55000\n32\n0.9\n3\n...\n0.0\n0\n0.007347\n0.006442\n0.007053\n0.007347\n0.007103\n0.007499\n0.008247\n0.008863\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\n1.0\n0\n0.009142\n0.008017\n0.008776\n0.009142\n0.008839\n0.009330\n0.010258\n0.011023\n\n\n599998\nU3462916\nno\n0\nteam\n&lt; 30\nno\n50000\n35\n0.6\n2\n...\n0.0\n0\n0.006615\n0.005799\n0.006350\n0.006615\n0.006395\n0.006751\n0.007425\n0.007981\n\n\n599999\nU3462922\nno\n0\nendurance\n30 to 44\nyes\n50000\n25\n0.7\n1\n...\n0.0\n0\n0.010293\n0.007484\n0.008194\n0.008535\n0.008252\n0.008710\n0.009578\n0.010293\n\n\n\n\n600000 rows √ó 25 columns\n\n\n\n\nWe then extend the prediction to the full database and see how the predictions are distributed.\n\npentathlon_nptb[\"message_lr\"] = pentathlon_nptb[[\n    \"p_control_lr\", \n    \"p_endurance_lr\", \n    \"p_backcountry_lr\", \n    \"p_racquet_lr\", \n    \"p_strength_lr\", \n    \"p_team_lr\", \n    \"p_water_lr\"]].idxmax(axis=1)\npentathlon_nptb[\"message_lr\"].value_counts()\n\nmessage_lr\np_endurance_lr    600000\nName: count, dtype: int64\n\n\nAfter checking the distribution, we can see that the model predicts that every customer should be sent the same product. The mistake in the analysis above was that the specified model is not sufficiently flexible to allow customization across customers!\nThe output of the logistic regression above shows that there are indeed some interaction effects taking place in the data. We want to create a model that can capture these interaction effects, which we have done. We are now going to use the model to predict the next product to buy for the full database.\n\nivar=[f\"{e}:message\" for e in evar if e != \"message\"]\nivar\n\n['age:message',\n 'female:message',\n 'income:message',\n 'education:message',\n 'children:message',\n 'freq_endurance:message',\n 'freq_strength:message',\n 'freq_water:message',\n 'freq_team:message',\n 'freq_backcountry:message',\n 'freq_racquet:message']\n\n\n\nThe model is then:\n\nlr_int = rsm.model.logistic(\n    data={\"penathlon_nptb\": pentathlon_nptb[pentathlon_nptb.training == 1]},\n    rvar=\"buyer\",\n    lev=\"yes\",\n    evar=evar,\n    ivar=ivar\n)\nlr_int.summary()\n\nLogistic regression (GLM)\nData                 : penathlon_nptb\nResponse variable    : buyer\nLevel                : yes\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nNull hyp.: There is no effect of x on buyer\nAlt. hyp.: There is an effect of x on buyer\n\n                                        OR      OR%  coefficient  std.error  z.value p.value     \nIntercept                            0.000  -100.0%        -8.25      0.153  -53.948  &lt; .001  ***\nmessage[control]                     1.115    11.5%         0.11      0.220    0.496    0.62     \nmessage[endurance]                   1.199    19.9%         0.18      0.213    0.850   0.395     \nmessage[racquet]                     1.084     8.4%         0.08      0.217    0.370   0.711     \nmessage[strength]                    0.959    -4.1%        -0.04      0.216   -0.195   0.846     \nmessage[team]                        0.881   -11.9%        -0.13      0.217   -0.581   0.561     \nmessage[water]                       0.903    -9.7%        -0.10      0.219   -0.468    0.64     \nage[30 to 44]                        1.979    97.9%         0.68      0.100    6.802  &lt; .001  ***\nage[45 to 59]                        2.056   105.6%         0.72      0.102    7.033  &lt; .001  ***\nage[&gt;= 60]                           0.992    -0.8%        -0.01      0.127   -0.060   0.952     \nfemale[no]                           1.365    36.5%         0.31      0.064    4.833  &lt; .001  ***\nage[30 to 44]:message[control]       1.196    19.6%         0.18      0.146    1.225    0.22     \nage[45 to 59]:message[control]       1.016     1.6%         0.02      0.150    0.107   0.914     \nage[&gt;= 60]:message[control]          1.155    15.5%         0.14      0.184    0.785   0.432     \nage[30 to 44]:message[endurance]     1.302    30.2%         0.26      0.142    1.855   0.064    .\nage[45 to 59]:message[endurance]     1.142    14.2%         0.13      0.146    0.907   0.364     \nage[&gt;= 60]:message[endurance]        1.473    47.3%         0.39      0.176    2.203   0.028    *\nage[30 to 44]:message[racquet]       1.053     5.3%         0.05      0.144    0.358    0.72     \nage[45 to 59]:message[racquet]       1.004     0.4%         0.00      0.147    0.030   0.976     \nage[&gt;= 60]:message[racquet]          1.269    26.9%         0.24      0.178    1.334   0.182     \nage[30 to 44]:message[strength]      1.307    30.7%         0.27      0.142    1.886   0.059    .\nage[45 to 59]:message[strength]      0.947    -5.3%        -0.05      0.146   -0.375   0.708     \nage[&gt;= 60]:message[strength]         1.084     8.4%         0.08      0.178    0.456   0.649     \nage[30 to 44]:message[team]          1.268    26.8%         0.24      0.144    1.654   0.098    .\nage[45 to 59]:message[team]          1.111    11.1%         0.10      0.147    0.713   0.476     \nage[&gt;= 60]:message[team]             1.164    16.4%         0.15      0.180    0.841     0.4     \nage[30 to 44]:message[water]         1.359    35.9%         0.31      0.147    2.081   0.037    *\nage[45 to 59]:message[water]         1.264    26.4%         0.23      0.151    1.552   0.121     \nage[&gt;= 60]:message[water]            1.362    36.2%         0.31      0.184    1.677   0.094    .\nfemale[no]:message[control]          0.974    -2.6%        -0.03      0.092   -0.283   0.777     \nfemale[no]:message[endurance]        0.884   -11.6%        -0.12      0.089   -1.391   0.164     \nfemale[no]:message[racquet]          1.192    19.2%         0.18      0.092    1.911   0.056    .\nfemale[no]:message[strength]         1.015     1.5%         0.02      0.090    0.168   0.867     \nfemale[no]:message[team]             1.019     1.9%         0.02      0.090    0.206   0.837     \nfemale[no]:message[water]            0.897   -10.3%        -0.11      0.091   -1.190   0.234     \nincome                               1.000     0.0%         0.00      0.000    8.324  &lt; .001  ***\nincome:message[control]              1.000     0.0%         0.00      0.000    0.020   0.984     \nincome:message[endurance]            1.000     0.0%         0.00      0.000    1.688   0.091    .\nincome:message[racquet]              1.000     0.0%         0.00      0.000    0.191   0.849     \nincome:message[strength]             1.000    -0.0%        -0.00      0.000   -0.368   0.713     \nincome:message[team]                 1.000     0.0%         0.00      0.000    0.053   0.958     \nincome:message[water]                1.000    -0.0%        -0.00      0.000   -0.353   0.724     \neducation                            1.041     4.1%         0.04      0.003   13.546  &lt; .001  ***\neducation:message[control]           0.995    -0.5%        -0.01      0.004   -1.273   0.203     \neducation:message[endurance]         0.993    -0.7%        -0.01      0.004   -1.722   0.085    .\neducation:message[racquet]           0.992    -0.8%        -0.01      0.004   -1.959    0.05    .\neducation:message[strength]          1.002     0.2%         0.00      0.004    0.551   0.581     \neducation:message[team]              1.000    -0.0%        -0.00      0.004   -0.071   0.943     \neducation:message[water]             0.997    -0.3%        -0.00      0.004   -0.718   0.473     \nchildren                             1.723    72.3%         0.54      0.078    6.982  &lt; .001  ***\nchildren:message[control]            0.838   -16.2%        -0.18      0.114   -1.552   0.121     \nchildren:message[endurance]          0.885   -11.5%        -0.12      0.109   -1.120   0.263     \nchildren:message[racquet]            0.933    -6.7%        -0.07      0.110   -0.629   0.529     \nchildren:message[strength]           1.023     2.3%         0.02      0.110    0.207   0.836     \nchildren:message[team]               0.960    -4.0%        -0.04      0.111   -0.369   0.712     \nchildren:message[water]              0.935    -6.5%        -0.07      0.112   -0.602   0.547     \nfreq_endurance                       1.081     8.1%         0.08      0.014    5.617  &lt; .001  ***\nfreq_endurance:message[control]      1.029     2.9%         0.03      0.020    1.442   0.149     \nfreq_endurance:message[endurance]    0.990    -1.0%        -0.01      0.020   -0.512   0.609     \nfreq_endurance:message[racquet]      1.016     1.6%         0.02      0.020    0.792   0.428     \nfreq_endurance:message[strength]     1.016     1.6%         0.02      0.020    0.804   0.421     \nfreq_endurance:message[team]         1.043     4.3%         0.04      0.019    2.157   0.031    *\nfreq_endurance:message[water]        1.036     3.6%         0.04      0.020    1.807   0.071    .\nfreq_strength                        1.114    11.4%         0.11      0.009   12.347  &lt; .001  ***\nfreq_strength:message[control]       1.001     0.1%         0.00      0.012    0.045   0.964     \nfreq_strength:message[endurance]     0.991    -0.9%        -0.01      0.012   -0.732   0.464     \nfreq_strength:message[racquet]       1.002     0.2%         0.00      0.012    0.142   0.887     \nfreq_strength:message[strength]      0.998    -0.2%        -0.00      0.012   -0.174   0.862     \nfreq_strength:message[team]          1.003     0.3%         0.00      0.012    0.249   0.803     \nfreq_strength:message[water]         1.015     1.5%         0.01      0.012    1.162   0.245     \nfreq_water                           1.139    13.9%         0.13      0.035    3.678  &lt; .001  ***\nfreq_water:message[control]          1.036     3.6%         0.04      0.051    0.699   0.484     \nfreq_water:message[endurance]        1.067     6.7%         0.06      0.051    1.272   0.204     \nfreq_water:message[racquet]          1.028     2.8%         0.03      0.051    0.539    0.59     \nfreq_water:message[strength]         1.014     1.4%         0.01      0.051    0.268   0.788     \nfreq_water:message[team]             0.970    -3.0%        -0.03      0.050   -0.602   0.547     \nfreq_water:message[water]            1.158    15.8%         0.15      0.051    2.859   0.004   **\nfreq_team                            1.037     3.7%         0.04      0.012    3.069   0.002   **\nfreq_team:message[control]           1.005     0.5%         0.01      0.017    0.302   0.763     \nfreq_team:message[endurance]         0.999    -0.1%        -0.00      0.017   -0.057   0.955     \nfreq_team:message[racquet]           1.025     2.5%         0.02      0.017    1.431   0.152     \nfreq_team:message[strength]          1.031     3.1%         0.03      0.017    1.788   0.074    .\nfreq_team:message[team]              0.993    -0.7%        -0.01      0.017   -0.406   0.685     \nfreq_team:message[water]             1.036     3.6%         0.04      0.017    2.081   0.037    *\nfreq_backcountry                     1.210    21.0%         0.19      0.026    7.256  &lt; .001  ***\nfreq_backcountry:message[control]    0.973    -2.7%        -0.03      0.037   -0.727   0.467     \nfreq_backcountry:message[endurance]  0.992    -0.8%        -0.01      0.037   -0.224   0.823     \nfreq_backcountry:message[racquet]    0.968    -3.2%        -0.03      0.037   -0.864   0.388     \nfreq_backcountry:message[strength]   0.982    -1.8%        -0.02      0.037   -0.492   0.623     \nfreq_backcountry:message[team]       0.976    -2.4%        -0.02      0.037   -0.658   0.511     \nfreq_backcountry:message[water]      0.981    -1.9%        -0.02      0.038   -0.517   0.605     \nfreq_racquet                         1.100    10.0%         0.10      0.017    5.456  &lt; .001  ***\nfreq_racquet:message[control]        1.034     3.4%         0.03      0.025    1.356   0.175     \nfreq_racquet:message[endurance]      1.034     3.4%         0.03      0.025    1.375   0.169     \nfreq_racquet:message[racquet]        1.039     3.9%         0.04      0.025    1.523   0.128     \nfreq_racquet:message[strength]       0.975    -2.5%        -0.03      0.025   -1.023   0.306     \nfreq_racquet:message[team]           0.992    -0.8%        -0.01      0.024   -0.345    0.73     \nfreq_racquet:message[water]          0.998    -0.2%        -0.00      0.025   -0.090   0.928     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.262\nPseudo R-squared (McFadden adjusted): 0.26\nArea under the RO Curve (AUC): 0.884\nLog-likelihood: -35102.941, AIC: 70401.883, BIC: 71474.788\nChi-squared: 24901.02, df(97), p.value &lt; 0.001 \nNr obs: 420,000\n\n\n\npentathlon_nptb[\"pred_lr_int\"] = lr_int.predict(pentathlon_nptb)[\"prediction\"]\n\n\ndct = {\"train\": pentathlon_nptb.query(\"training == 1\"), \"test\": pentathlon_nptb.query(\"training == 0\")}\nfig = rsm.gains_plot(dct, \"buyer\", \"yes\", \"pred_lr_int\")\n\n\n\n\n\n\n\n\n\nfig = rsm.gains_plot(\n    pentathlon_nptb,\n    \"buyer\", \"yes\",\n    [\"pred_lr\", \"pred_lr_int\"]\n)\n\n\n\n\n\n\n\n\n\n# prediction on training set\npred = pentathlon_nptb.query(\"training == 1\")['pred_lr_int']\nactual = pentathlon_nptb.query(\"training == 1\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.884\n\n\n\n# prediction on test set\npred = pentathlon_nptb.query(\"training == 0\")['pred_lr_int']\nactual = pentathlon_nptb.query(\"training == 0\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(3)\n\n0.883\n\n\nNow, lets repeat the analysis above with the new model.\n\n# Create predictions\npentathlon_nptb[\"p_controli_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb[\"p_racqueti_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb[\"p_teami_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb[\"p_backcountryi_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb[\"p_wateri_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb[\"p_strengthi_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb[\"p_endurancei_lr\"] = lr_int.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\npentathlon_nptb\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\np_endurance_lr\nmessage_lr\npred_lr_int\np_controli_lr\np_racqueti_lr\np_teami_lr\np_backcountryi_lr\np_wateri_lr\np_strengthi_lr\np_endurancei_lr\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\n0.015700\np_endurance_lr\n0.012008\n0.012022\n0.014499\n0.012008\n0.011131\n0.012604\n0.015452\n0.015682\n\n\n1\nU3\nno\n0\nbackcountry\n45 to 59\nno\n35000\n22\n1.0\n0\n...\n0.006395\np_endurance_lr\n0.005556\n0.004605\n0.005858\n0.005279\n0.005556\n0.004981\n0.005475\n0.006014\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\n0.011948\np_endurance_lr\n0.013884\n0.009140\n0.008789\n0.009533\n0.010718\n0.009680\n0.009343\n0.013884\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\n0.002793\np_endurance_lr\n0.002389\n0.002252\n0.002089\n0.002264\n0.002339\n0.002389\n0.002196\n0.002970\n\n\n4\nU25\nno\n0\nracquet\n&gt;= 60\nyes\n65000\n32\n1.1\n1\n...\n0.014705\np_endurance_lr\n0.012039\n0.010784\n0.012039\n0.011119\n0.011526\n0.011437\n0.011463\n0.019675\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\n0.002582\np_endurance_lr\n0.002042\n0.001966\n0.002118\n0.001946\n0.001947\n0.002042\n0.002089\n0.003217\n\n\n599996\nU3462900\nno\n0\nteam\n&lt; 30\nno\n55000\n32\n0.9\n3\n...\n0.008863\np_endurance_lr\n0.007692\n0.006959\n0.008257\n0.007692\n0.007884\n0.005832\n0.008117\n0.007731\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\n0.011023\np_endurance_lr\n0.008225\n0.008535\n0.008976\n0.008225\n0.010062\n0.008286\n0.009822\n0.011345\n\n\n599998\nU3462916\nno\n0\nteam\n&lt; 30\nno\n50000\n35\n0.6\n2\n...\n0.007981\np_endurance_lr\n0.006796\n0.006378\n0.007352\n0.006796\n0.007164\n0.005272\n0.007284\n0.006971\n\n\n599999\nU3462922\nno\n0\nendurance\n30 to 44\nyes\n50000\n25\n0.7\n1\n...\n0.010293\np_endurance_lr\n0.011857\n0.008527\n0.007495\n0.008401\n0.007500\n0.008214\n0.009222\n0.011857\n\n\n\n\n600000 rows √ó 34 columns\n\n\n\n\nWe then extend the prediction to the full database and see how the predictions are distributed.\n\nrepl = {\n    \"p_controli_lr\": \"control\", \n    \"p_endurancei_lr\": \"endurance\", \n    \"p_backcountryi_lr\": \"backcountry\", \n    \"p_racqueti_lr\": \"racquet\", \n    \"p_strengthi_lr\": \"strength\", \n    \"p_teami_lr\": \"team\", \n    \"p_wateri_lr\": \"water\"}\n\n\npredictions_lr = [\n    \"p_controli_lr\", \n    \"p_endurancei_lr\", \n    \"p_backcountryi_lr\", \n    \"p_racqueti_lr\", \n    \"p_strengthi_lr\", \n    \"p_teami_lr\", \n    \"p_wateri_lr\"]\n\n\n\nFind the highest probability of purchase for each customer, labeled them by the product with the highest probability of purchase\n\npentathlon_nptb[\"messagei_lr\"] = (\n    pentathlon_nptb[predictions_lr]\n    .idxmax(axis=1)\n    .map(repl)\n)\npentathlon_nptb\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\nmessage_lr\npred_lr_int\np_controli_lr\np_racqueti_lr\np_teami_lr\np_backcountryi_lr\np_wateri_lr\np_strengthi_lr\np_endurancei_lr\nmessagei_lr\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\np_endurance_lr\n0.012008\n0.012022\n0.014499\n0.012008\n0.011131\n0.012604\n0.015452\n0.015682\nendurance\n\n\n1\nU3\nno\n0\nbackcountry\n45 to 59\nno\n35000\n22\n1.0\n0\n...\np_endurance_lr\n0.005556\n0.004605\n0.005858\n0.005279\n0.005556\n0.004981\n0.005475\n0.006014\nendurance\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\np_endurance_lr\n0.013884\n0.009140\n0.008789\n0.009533\n0.010718\n0.009680\n0.009343\n0.013884\nendurance\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\np_endurance_lr\n0.002389\n0.002252\n0.002089\n0.002264\n0.002339\n0.002389\n0.002196\n0.002970\nendurance\n\n\n4\nU25\nno\n0\nracquet\n&gt;= 60\nyes\n65000\n32\n1.1\n1\n...\np_endurance_lr\n0.012039\n0.010784\n0.012039\n0.011119\n0.011526\n0.011437\n0.011463\n0.019675\nendurance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\np_endurance_lr\n0.002042\n0.001966\n0.002118\n0.001946\n0.001947\n0.002042\n0.002089\n0.003217\nendurance\n\n\n599996\nU3462900\nno\n0\nteam\n&lt; 30\nno\n55000\n32\n0.9\n3\n...\np_endurance_lr\n0.007692\n0.006959\n0.008257\n0.007692\n0.007884\n0.005832\n0.008117\n0.007731\nracquet\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\np_endurance_lr\n0.008225\n0.008535\n0.008976\n0.008225\n0.010062\n0.008286\n0.009822\n0.011345\nendurance\n\n\n599998\nU3462916\nno\n0\nteam\n&lt; 30\nno\n50000\n35\n0.6\n2\n...\np_endurance_lr\n0.006796\n0.006378\n0.007352\n0.006796\n0.007164\n0.005272\n0.007284\n0.006971\nracquet\n\n\n599999\nU3462922\nno\n0\nendurance\n30 to 44\nyes\n50000\n25\n0.7\n1\n...\np_endurance_lr\n0.011857\n0.008527\n0.007495\n0.008401\n0.007500\n0.008214\n0.009222\n0.011857\nendurance\n\n\n\n\n600000 rows √ó 35 columns\n\n\n\n\n\n# Find the maximum probability of purchase\npentathlon_nptb[\"p_max\"] = pentathlon_nptb[predictions_lr].max(axis=1)\n\nApproach: First of all, We build a logistic regression model to predict the next product to buy, use buyer_yes as the response variable and the other variables as predictors. After relizing that the model predicts that every customer should be sent the same product i.e, endurance, we build another model that can capture these interaction effects.\nWe then extend the prediction to the full database. To predict the probability of purchasing for different messages, we use the data_cmd to predict the probability of purchasing for different messages.\nFinally, we choose the product with the highest probability using idxmax to automatically find the best message for each customer. This command also provides a label for the category with the maximum predicted propability of buying accross all the products. Then, create p_max to store the maximum predicted probability of purchasing for the best message sellected for each customer.\n\n\n2. For each message, report the percentage of customers for whom that message or no-message maximizes their probability of purchase\nLet‚Äôs create a crosstab to see which products should we send to each customer.\n\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].messagei_lr, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nmessagei_lr\n\n\n\n\n\nbackcountry\n1,447\n\n\nendurance\n125,861\n\n\nracquet\n12,299\n\n\nstrength\n36,584\n\n\nteam\n1,680\n\n\nwater\n2,129\n\n\n\n\n\n\n\n\nReport the percentage of customers for whom that message is the best message or not the best message.\n\npentathlon_nptb[\"messagei_lr\"].value_counts(normalize=True)\n\nmessagei_lr\nendurance      0.699675\nstrength       0.202202\nracquet        0.069037\nwater          0.011852\nteam           0.009358\nbackcountry    0.007875\ncontrol        0.000002\nName: proportion, dtype: float64\n\n\n\nedurance: The distribution suggests that endurance - related messages resonate significantly more with the customers than the other messages.\nstrength: Wilte not as dominant as endurance, strength - related messages also play an important role. There might be opportunities to further optimize or tailor these messages to increase their effectiveness.\nOther categories such as racquet, water, team, backcountry have much smaller proportions, suggesting that they are less often the best choice for maximizing the probability of purchase.\ncontrol: The negligible role of control in maximizing the probability suggrest that any engagement, even if not perfectly oplimized, tends to be better than no engagement at all. However, it‚Äôs also essential to consider the context and the potential costs of sending messages to customers.\n\nCreate a table with the average purchase probability if we send the message for each product to everyone.\n\npentathlon_nptb.loc[pentathlon_nptb.training == 0, predictions_lr].agg(\"mean\").sort_values(\n    ascending=False).apply(rsm.format_nr, perc=True)\n\np_endurancei_lr      2.79%\np_strengthi_lr       2.64%\np_wateri_lr          2.44%\np_teami_lr           2.39%\np_backcountryi_lr    2.32%\np_racqueti_lr        2.31%\np_controli_lr        2.14%\ndtype: object\n\n\n\n\n3. Expected profit\n\n# Calculate the actual order size\nordersize = pentathlon_nptb[(pentathlon_nptb.training == 1) & (pentathlon_nptb.buyer_yes == 1)].groupby(\"message\")[\"total_os\"].mean()\nordersize\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/476348601.py:2: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nmessage\nbackcountry    64.034091\ncontrol        49.900598\nendurance      55.584893\nracquet        56.405620\nstrength       56.708751\nteam           56.522449\nwater          61.957343\nName: total_os, dtype: float64\n\n\n\n\nCreat Linear model to predict ordersize\n\nreg = rsm.model.regress(\n    data = {\"pentathlon_nptb\": pentathlon_nptb[(pentathlon_nptb.training == 1) & (pentathlon_nptb.buyer == \"yes\")]},\n    rvar =\"total_os\",\n    evar = evar,\n    ivar = ivar\n)\nreg.summary()\n\nLinear regression (OLS)\nData                 : pentathlon_nptb\nResponse variable    : total_os\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nNull hyp.: the effect of x on total_os is zero\nAlt. hyp.: the effect of x on total_os is not zero\n\n                                     coefficient  std.error  t.value p.value     \nIntercept                                 -9.616      8.976   -1.071   0.284     \nmessage[control]                          17.412     12.953    1.344   0.179     \nmessage[endurance]                         9.106     12.058    0.755    0.45     \nmessage[racquet]                           4.179     12.822    0.326   0.745     \nmessage[strength]                          1.414     12.601    0.112   0.911     \nmessage[team]                             10.566     12.573    0.840   0.401     \nmessage[water]                             6.659     12.905    0.516   0.606     \nage[30 to 44]                              4.166      5.701    0.731   0.465     \nage[45 to 59]                              6.059      5.712    1.061   0.289     \nage[&gt;= 60]                                -0.679      7.066   -0.096   0.923     \nfemale[no]                                 3.578      3.548    1.008   0.313     \nage[30 to 44]:message[control]            -6.037      8.297   -0.728   0.467     \nage[45 to 59]:message[control]            -3.440      8.437   -0.408   0.683     \nage[&gt;= 60]:message[control]                5.342     10.292    0.519   0.604     \nage[30 to 44]:message[endurance]          -3.088      8.017   -0.385     0.7     \nage[45 to 59]:message[endurance]         -12.434      8.132   -1.529   0.126     \nage[&gt;= 60]:message[endurance]              6.071      9.755    0.622   0.534     \nage[30 to 44]:message[racquet]            -1.337      8.176   -0.163    0.87     \nage[45 to 59]:message[racquet]            -8.117      8.242   -0.985   0.325     \nage[&gt;= 60]:message[racquet]               -5.150     10.010   -0.514   0.607     \nage[30 to 44]:message[strength]           -0.696      7.975   -0.087    0.93     \nage[45 to 59]:message[strength]            0.160      8.076    0.020   0.984     \nage[&gt;= 60]:message[strength]               5.451      9.893    0.551   0.582     \nage[30 to 44]:message[team]                8.784      8.077    1.088   0.277     \nage[45 to 59]:message[team]                6.381      8.161    0.782   0.434     \nage[&gt;= 60]:message[team]                   9.798     10.020    0.978   0.328     \nage[30 to 44]:message[water]              -7.117      8.285   -0.859    0.39     \nage[45 to 59]:message[water]             -12.193      8.378   -1.455   0.146     \nage[&gt;= 60]:message[water]                  5.907     10.209    0.579   0.563     \nfemale[no]:message[control]               -4.082      5.058   -0.807    0.42     \nfemale[no]:message[endurance]             -3.209      4.877   -0.658   0.511     \nfemale[no]:message[racquet]               -2.000      5.096   -0.392   0.695     \nfemale[no]:message[strength]              -3.970      4.920   -0.807    0.42     \nfemale[no]:message[team]                  -8.944      4.904   -1.824   0.068    .\nfemale[no]:message[water]                  1.978      4.972    0.398   0.691     \nincome                                     0.000      0.000    3.970  &lt; .001  ***\nincome:message[control]                   -0.000      0.000   -1.166   0.244     \nincome:message[endurance]                 -0.000      0.000   -0.710   0.478     \nincome:message[racquet]                    0.000      0.000    0.702   0.483     \nincome:message[strength]                  -0.000      0.000   -1.205   0.228     \nincome:message[team]                      -0.000      0.000   -1.392   0.164     \nincome:message[water]                     -0.000      0.000   -1.353   0.176     \neducation                                  0.773      0.161    4.812  &lt; .001  ***\neducation:message[control]                -0.370      0.230   -1.613   0.107     \neducation:message[endurance]              -0.238      0.218   -1.090   0.276     \neducation:message[racquet]                -0.292      0.228   -1.283   0.199     \neducation:message[strength]                0.005      0.221    0.025    0.98     \neducation:message[team]                   -0.337      0.224   -1.505   0.132     \neducation:message[water]                  -0.088      0.226   -0.390   0.696     \nchildren                                   7.307      4.406    1.659   0.097    .\nchildren:message[control]                  4.666      6.217    0.750   0.453     \nchildren:message[endurance]                1.572      5.994    0.262   0.793     \nchildren:message[racquet]                  5.236      6.255    0.837   0.403     \nchildren:message[strength]                -0.699      6.269   -0.112   0.911     \nchildren:message[team]                     4.814      6.233    0.772    0.44     \nchildren:message[water]                   11.040      6.482    1.703   0.089    .\nfreq_endurance                            -1.400      0.698   -2.004   0.045    *\nfreq_endurance:message[control]            1.082      0.995    1.088   0.277     \nfreq_endurance:message[endurance]         -0.138      0.974   -0.142   0.887     \nfreq_endurance:message[racquet]           -0.927      1.005   -0.922   0.356     \nfreq_endurance:message[strength]           0.160      0.981    0.163   0.871     \nfreq_endurance:message[team]               1.424      0.963    1.479   0.139     \nfreq_endurance:message[water]             -0.298      0.968   -0.308   0.758     \nfreq_strength                             -2.181      0.437   -4.988  &lt; .001  ***\nfreq_strength:message[control]             0.352      0.626    0.563   0.573     \nfreq_strength:message[endurance]           0.286      0.607    0.471   0.638     \nfreq_strength:message[racquet]             0.029      0.635    0.045   0.964     \nfreq_strength:message[strength]            1.337      0.614    2.179   0.029    *\nfreq_strength:message[team]                0.807      0.615    1.312    0.19     \nfreq_strength:message[water]               1.055      0.614    1.718   0.086    .\nfreq_water                                 2.979      1.679    1.774   0.076    .\nfreq_water:message[control]                0.263      2.417    0.109   0.913     \nfreq_water:message[endurance]             -1.171      2.375   -0.493   0.622     \nfreq_water:message[racquet]               -2.418      2.392   -1.011   0.312     \nfreq_water:message[strength]               0.200      2.369    0.084   0.933     \nfreq_water:message[team]                  -1.236      2.347   -0.527   0.598     \nfreq_water:message[water]                 -3.641      2.368   -1.538   0.124     \nfreq_team                                 -0.075      0.599   -0.125   0.901     \nfreq_team:message[control]                -0.229      0.847   -0.270   0.787     \nfreq_team:message[endurance]               2.188      0.829    2.638   0.008   **\nfreq_team:message[racquet]                -0.563      0.861   -0.654   0.513     \nfreq_team:message[strength]               -0.657      0.850   -0.774   0.439     \nfreq_team:message[team]                    0.320      0.829    0.386     0.7     \nfreq_team:message[water]                  -0.932      0.836   -1.115   0.265     \nfreq_backcountry                           2.686      1.320    2.035   0.042    *\nfreq_backcountry:message[control]         -0.962      1.902   -0.506   0.613     \nfreq_backcountry:message[endurance]        2.745      1.848    1.486   0.137     \nfreq_backcountry:message[racquet]         -0.198      1.887   -0.105   0.916     \nfreq_backcountry:message[strength]        -0.097      1.864   -0.052   0.958     \nfreq_backcountry:message[team]            -0.529      1.858   -0.284   0.776     \nfreq_backcountry:message[water]            1.071      1.862    0.575   0.565     \nfreq_racquet                               0.297      0.868    0.343   0.732     \nfreq_racquet:message[control]             -0.775      1.246   -0.622   0.534     \nfreq_racquet:message[endurance]           -0.021      1.186   -0.018   0.986     \nfreq_racquet:message[racquet]              0.944      1.237    0.764   0.445     \nfreq_racquet:message[strength]             1.148      1.200    0.957   0.339     \nfreq_racquet:message[team]                 0.199      1.207    0.165   0.869     \nfreq_racquet:message[water]                1.503      1.214    1.238   0.216     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.089, Adjusted R-squared: 0.08\nF-statistic: 10.021 df(97, 9982), p.value &lt; 0.001\nNr obs: 10,080\n\n\n\nreg.coef.round(3)\n\n\n\n\n\n\n\n\n\nindex\ncoefficient\nstd.error\nt.value\np.value\n\n\n\n\n\n0\nIntercept\n-9.616\n8.976\n-1.071\n0.284\n\n\n\n1\nmessage[T.control]\n17.412\n12.953\n1.344\n0.179\n\n\n\n2\nmessage[T.endurance]\n9.106\n12.058\n0.755\n0.450\n\n\n\n3\nmessage[T.racquet]\n4.179\n12.822\n0.326\n0.745\n\n\n\n4\nmessage[T.strength]\n1.414\n12.601\n0.112\n0.911\n\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n93\nfreq_racquet:message[T.endurance]\n-0.021\n1.186\n-0.018\n0.986\n\n\n\n94\nfreq_racquet:message[T.racquet]\n0.944\n1.237\n0.764\n0.445\n\n\n\n95\nfreq_racquet:message[T.strength]\n1.148\n1.200\n0.957\n0.339\n\n\n\n96\nfreq_racquet:message[T.team]\n0.199\n1.207\n0.165\n0.869\n\n\n\n97\nfreq_racquet:message[T.water]\n1.503\n1.214\n1.238\n0.216\n\n\n\n\n\n98 rows √ó 6 columns\n\n\n\n\n\npentathlon_nptb[\"pos_control_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb[\"pos_racquet_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb[\"pos_team_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb[\"pos_backcountry_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb[\"pos_water_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb[\"pos_strength_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb[\"pos_endurance_reg\"] = reg.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\n\n\n\nCalculate the expected profit for each product\n\npentathlon_nptb[\"ep_control_lr\"] = pentathlon_nptb[\"p_control_lr\"] * pentathlon_nptb[\"pos_control_reg\"] * 0.4\npentathlon_nptb[\"ep_racquet_lr\"] = pentathlon_nptb[\"p_racquet_lr\"] * pentathlon_nptb[\"pos_racquet_reg\"] * 0.4\npentathlon_nptb[\"ep_team_lr\"] = pentathlon_nptb[\"p_team_lr\"] * pentathlon_nptb[\"pos_team_reg\"] * 0.4\npentathlon_nptb[\"ep_backcountry_lr\"] = pentathlon_nptb[\"p_backcountry_lr\"] * pentathlon_nptb[\"pos_backcountry_reg\"] * 0.4\npentathlon_nptb[\"ep_water_lr\"] = pentathlon_nptb[\"p_water_lr\"] * pentathlon_nptb[\"pos_water_reg\"] * 0.4\npentathlon_nptb[\"ep_strength_lr\"] = pentathlon_nptb[\"p_strength_lr\"] * pentathlon_nptb[\"pos_strength_reg\"] * 0.4\npentathlon_nptb[\"ep_endurance_lr\"] = pentathlon_nptb[\"p_endurance_lr\"] * pentathlon_nptb[\"pos_endurance_reg\"] * 0.4\n\nTo determine the message to send that will maximize the expected profit, we can use the following formula:\n\nexpected_profit_lr = [\n    \"ep_control_lr\", \n    \"ep_endurance_lr\", \n    \"ep_backcountry_lr\", \n    \"ep_racquet_lr\", \n    \"ep_strength_lr\", \n    \"ep_team_lr\", \n    \"ep_water_lr\"]\n\n\nrepl = {\"ep_control_lr\": \"control\", \"ep_endurance_lr\": \"endurance\", \"ep_backcountry_lr\": \"backcountry\", \"ep_racquet_lr\": \"racquet\", \"ep_strength_lr\": \"strength\", \"ep_team_lr\": \"team\", \"ep_water_lr\": \"water\"}\npentathlon_nptb[\"ep_message_lr\"] = (\n    pentathlon_nptb[expected_profit_lr]\n    .idxmax(axis=1)\n    .map(repl)\n)\n\nTo predict the order size, we use the total_os as the response variable and the other variables as predictors as well as the interaction between message and other variables. We then extend the prediction to the full database and see how the predictions are distributed.\nSame as the previous model, we use the data_cmd to predict the order size for different messages.\nAfter that, we calculate the expected profit for each product by multiplying the probability of purchasing and the order size for each product. We then choose the product with the highest expected profit using idxmax to automatically find the best message for each customer. This command also provides a label for the category with the maximum expected profit. Then, create p_max to store the maximum expected profit of purchasing for the best message sellected for each customer.\n\n\n4.Report for each message, i.e., endurance, racket, etc., and no-message, the percentage of customers for whom that (no) message maximizes their expected profit.\n\npentathlon_nptb.ep_message_lr.value_counts(normalize=True)\n\nep_message_lr\nwater          0.340890\nendurance      0.306053\nteam           0.206722\nbackcountry    0.073778\nstrength       0.066898\nracquet        0.004322\ncontrol        0.001337\nName: proportion, dtype: float64\n\n\n\n\n5. Expected profit can we obtain, on average, per customer if we customize the message to each customer?\n\npentathlon_nptb[\"ep_max\"] = pentathlon_nptb[expected_profit_lr].max(axis=1)\npentathlon_nptb[\"ep_max\"].mean()\n\n0.6754439327865267\n\n\nLet create the crosstab\n\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].ep_message_lr, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nep_message_lr\n\n\n\n\n\nbackcountry\n13,379\n\n\ncontrol\n238\n\n\nendurance\n54,956\n\n\nracquet\n740\n\n\nstrength\n12,092\n\n\nteam\n37,539\n\n\nwater\n61,056\n\n\n\n\n\n\n\n\n\n\n6. Expected profit per e-mailed customer if every customer receives the same message\n\n(\n    pentathlon_nptb\n    .loc[pentathlon_nptb.training == 0, [\"ep_control_lr\", \"ep_endurance_lr\", \"ep_backcountry_lr\", \"ep_racquet_lr\", \"ep_strength_lr\", \"ep_team_lr\", \"ep_water_lr\"]]\n    .agg(\"mean\")\n    .sort_values(ascending=False)\n    .apply(rsm.format_nr, sym = \"$\", dec = 2)\n)\n\nep_endurance_lr      $0.62\nep_water_lr           $0.6\nep_strength_lr        $0.6\nep_backcountry_lr    $0.59\nep_team_lr           $0.54\nep_racquet_lr        $0.53\nep_control_lr        $0.43\ndtype: object\n\n\n\n\n\n7. Expected profit per e-mailed customer if every customer is assigned randomly to one of the messages or the no-message condition?\n\n# probabilty of purchase where customer is assigned to a random message\npentathlon_nptb[\"p_random_lr\"] = lr_int.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected avg order size where customer is assigned to a random message\npentathlon_nptb[\"ordersize_random_reg\"] = reg.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected profit where customer is assigned to a random message\npentathlon_nptb[\"ep_random_lr\"] = pentathlon_nptb[\"p_random_lr\"] * pentathlon_nptb[\"ordersize_random_reg\"] * 0.4\n\n# expected profit per customer where customer is assigned to a random message\nrandom_profit_per_customer = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_random_lr\"].mean()\nrandom_profit_per_customer\n\n0.5569904087502335\n\n\n\n# expected profit where no-message is sent (control)\npentathlon_nptb[\"ep_control_lr\"]\n\n# expected profit per customer where no-message is sent (control)\ncontrol_profit_per_customer = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_control_lr\"].mean()\ncontrol_profit_per_customer\n\n0.4339519244630221\n\n\n\n8. Profit for 5,000,000 customers\n\n# Profit where each customer is assigned to the message with the highest expected profit\nprofit_logit = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_max\"].agg(\"mean\") * 5000000\nprofit_logit\n\n3408166.566437483\n\n\n\n# Profit where each customer is sent to a random message\nrandom_profit = random_profit_per_customer * 5000000\nrandom_profit\n\n2784952.0437511676\n\n\n\n# Profit where no message is sent\ncontrol_profit = control_profit_per_customer * 5000000\ncontrol_profit\n\n2169759.6223151106\n\n\n\nprofit_improvement_lr = profit_logit - control_profit\nprofit_improvement_lr\n\n1238406.9441223722\n\n\n\nprofits_dct = {\n    \"Customize Message\": profit_logit,\n    \"Randomly Assign\": random_profit,\n    \"No Message Sent\": control_profit,\n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(profits_dct.items()), columns=['Model', 'Profit'])\nplt.figure(figsize=(10, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"Profit\", data=df, palette=\"magma\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.Profit, f'${row.Profit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1123293812.py:16: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#neural-networks-model",
    "href": "projects/Pentathlon/pentathlon_nptb.html#neural-networks-model",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Neural Networks Model",
    "text": "Neural Networks Model\n\npentathlon_nptb[pentathlon_nptb[\"training\"] == 1]\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\nep_team_lr\nep_backcountry_lr\nep_water_lr\nep_strength_lr\nep_endurance_lr\nep_message_lr\nep_max\np_random_lr\nordersize_random_reg\nep_random_lr\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\n0.170065\n0.152448\n0.165323\n0.127039\n0.216326\nendurance\n0.216326\n0.012008\n32.627819\n0.156712\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\n0.197624\n0.190290\n0.186217\n0.204221\n0.194732\nstrength\n0.204221\n0.013884\n40.744326\n0.226274\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\n0.028568\n0.022970\n0.014892\n0.024216\n0.016298\nteam\n0.028568\n0.002389\n15.768889\n0.015070\n\n\n5\nU28\nno\n0\nstrength\n&lt; 30\nyes\n25000\n18\n0.3\n0\n...\n0.006081\n0.005424\n0.007153\n0.005244\n0.008038\nendurance\n0.008038\n0.000929\n13.078011\n0.004860\n\n\n8\nU59\nno\n0\nstrength\n&gt;= 60\nyes\n65000\n36\n1.2\n1\n...\n0.244028\n0.211679\n0.291270\n0.243492\n0.301579\nendurance\n0.301579\n0.011213\n47.059571\n0.211075\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599989\nU3462835\nno\n0\ncontrol\n30 to 44\nyes\n45000\n20\n0.8\n1\n...\n0.081865\n0.060368\n0.063264\n0.057766\n0.070770\nteam\n0.081865\n0.004871\n32.245840\n0.062828\n\n\n599992\nU3462858\nno\n0\ncontrol\n45 to 59\nyes\n40000\n16\n1.1\n0\n...\n0.056256\n0.041872\n0.041478\n0.040307\n0.037076\nteam\n0.056256\n0.003052\n38.458959\n0.046946\n\n\n599993\nU3462877\nno\n0\nbackcountry\n&lt; 30\nno\n65000\n30\n1.0\n2\n...\n0.176618\n0.232750\n0.283269\n0.223465\n0.266163\nwater\n0.283269\n0.014259\n45.079704\n0.257119\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\n0.030821\n0.023935\n0.033779\n0.028076\n0.036240\nendurance\n0.036240\n0.002042\n38.698478\n0.031609\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\n0.119600\n0.121016\n0.170371\n0.139849\n0.178244\nendurance\n0.178244\n0.008225\n32.707711\n0.107603\n\n\n\n\n420000 rows √ó 55 columns\n\n\n\n\n\nNN = rsm.model.mlp(\n    data={\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar='buyer_yes',\n    evar= evar,\n    hidden_layer_sizes=(1,),  # Simple NN with 1 hidden layer\n    mod_type='classification'\n)\n\nNN.summary()\n\nMulti-layer Perceptron (NN)\nData                 : pentathlon_nptb\nResponse variable    : buyer_yes\nLevel                : None\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nModel type           : classification\nNr. of features      : (12, 19)\nNr. of observations  : 420,000\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.884\n\nRaw data             :\n  message      age female  income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet\n     team 30 to 44     no   55000         19       0.8               0              4           0          4                 0             1\nendurance 45 to 59    yes   45000         33       0.7               0              0           0          0                 2             2\n    water 45 to 59    yes   25000         24       0.2               0              0           0          0                 0             0\n strength     &lt; 30    yes   25000         18       0.3               0              0           0          0                 0             0\n strength    &gt;= 60    yes   65000         36       1.2               1              1           0          2                 0             3\n\nEstimation data      :\n   income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n 0.388655  -0.663713 -0.265321       -0.640336       1.100861   -0.261727   1.892088         -0.690593      0.058975            False              False            False             False          True          False          True         False      False       True\n-0.184052   0.279378 -0.479844       -0.640336      -0.706563   -0.261727  -0.620020          1.958233      0.882489            False               True            False             False         False          False         False          True      False      False\n-1.329467  -0.326895 -1.552460       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False             False         False           True         False          True      False      False\n-1.329467  -0.731077 -1.337937       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False              True         False          False         False         False      False      False\n 0.961362   0.481469  0.592772        0.060592      -0.254707   -0.261727   0.636034         -0.690593      1.706002            False              False            False              True         False          False         False         False       True      False\n\n\n\nNN100 = rsm.model.mlp(\n    data={\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar='buyer_yes',\n    evar= evar,\n    hidden_layer_sizes=(100,),  # more complex NN with 100 hidden layers\n    mod_type='classification'\n)\n\nNN100.summary()\n\nMulti-layer Perceptron (NN)\nData                 : pentathlon_nptb\nResponse variable    : buyer_yes\nLevel                : None\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nModel type           : classification\nNr. of features      : (12, 19)\nNr. of observations  : 420,000\nHidden_layer_sizes   : (100,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.892\n\nRaw data             :\n  message      age female  income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet\n     team 30 to 44     no   55000         19       0.8               0              4           0          4                 0             1\nendurance 45 to 59    yes   45000         33       0.7               0              0           0          0                 2             2\n    water 45 to 59    yes   25000         24       0.2               0              0           0          0                 0             0\n strength     &lt; 30    yes   25000         18       0.3               0              0           0          0                 0             0\n strength    &gt;= 60    yes   65000         36       1.2               1              1           0          2                 0             3\n\nEstimation data      :\n   income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n 0.388655  -0.663713 -0.265321       -0.640336       1.100861   -0.261727   1.892088         -0.690593      0.058975            False              False            False             False          True          False          True         False      False       True\n-0.184052   0.279378 -0.479844       -0.640336      -0.706563   -0.261727  -0.620020          1.958233      0.882489            False               True            False             False         False          False         False          True      False      False\n-1.329467  -0.326895 -1.552460       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False             False         False           True         False          True      False      False\n-1.329467  -0.731077 -1.337937       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False              True         False          False         False         False      False      False\n 0.961362   0.481469  0.592772        0.060592      -0.254707   -0.261727   0.636034         -0.690593      1.706002            False              False            False              True         False          False         False         False       True      False\n\n\n\nModel Tuning\n\n# NN_cv = rsm.load_state(\"./data/NN_cv.pkl\")['NN_cv']\n\nAfter fine-tuning the model in a separate notebook, we preserved the model‚Äôs state with save_state. To reintegrate it into the main notebook, load_state was utilized. However, due to GitHub‚Äôs file evaluation constraints, we are unable to successfully pass the automated tests using the aforementioned code. Should you wish to review our tuned model, please refer to the auxiliary notebooks. You can replicate our steps there by employing the provided code above\n\n# NN_cv.best_params_\n\n\n# NN_cv.best_score_.round(3)\n\n\nNN_best = rsm.model.mlp(\n    data={\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar='buyer_yes',\n    evar= evar,\n    alpha = 0.01,\n    hidden_layer_sizes = (3, 3),\n    mod_type='classification'\n)\n\nNN_best.summary()\n\nMulti-layer Perceptron (NN)\nData                 : pentathlon_nptb\nResponse variable    : buyer_yes\nLevel                : None\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nModel type           : classification\nNr. of features      : (12, 19)\nNr. of observations  : 420,000\nHidden_layer_sizes   : (3, 3)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.01\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nAUC                  : 0.89\n\nRaw data             :\n  message      age female  income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet\n     team 30 to 44     no   55000         19       0.8               0              4           0          4                 0             1\nendurance 45 to 59    yes   45000         33       0.7               0              0           0          0                 2             2\n    water 45 to 59    yes   25000         24       0.2               0              0           0          0                 0             0\n strength     &lt; 30    yes   25000         18       0.3               0              0           0          0                 0             0\n strength    &gt;= 60    yes   65000         36       1.2               1              1           0          2                 0             3\n\nEstimation data      :\n   income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n 0.388655  -0.663713 -0.265321       -0.640336       1.100861   -0.261727   1.892088         -0.690593      0.058975            False              False            False             False          True          False          True         False      False       True\n-0.184052   0.279378 -0.479844       -0.640336      -0.706563   -0.261727  -0.620020          1.958233      0.882489            False               True            False             False         False          False         False          True      False      False\n-1.329467  -0.326895 -1.552460       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False             False         False           True         False          True      False      False\n-1.329467  -0.731077 -1.337937       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False              True         False          False         False         False      False      False\n 0.961362   0.481469  0.592772        0.060592      -0.254707   -0.261727   0.636034         -0.690593      1.706002            False              False            False              True         False          False         False         False       True      False\n\n\n\npentathlon_nptb['pred_NN'] = NN_best.predict(pentathlon_nptb)['prediction']\n\n\ndct = {\"train\" : pentathlon_nptb.query(\"training == 1\"), \"test\" : pentathlon_nptb.query(\"training == 0\")}\nfig = rsm.gains_plot(dct, \"buyer\", \"yes\", \"pred_NN\")\n\n\n\n\n\n\n\n\n\nfrom sklearn import metrics\n\n# prediction on training set\npred = pentathlon_nptb.query(\"training == 1\")['pred_NN']\nactual = pentathlon_nptb.query(\"training == 1\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(5)\n\n0.89011\n\n\n\n# prediction on test set\npred = pentathlon_nptb.query(\"training == 0\")['pred_NN']\nactual = pentathlon_nptb.query(\"training == 0\")['buyer_yes']\nfpr, tpr, thresholds = metrics.roc_curve(actual, pred)\nmetrics.auc(fpr, tpr).round(5)\n\n0.88819\n\n\nWe can conclude that:\nThe AUC for the training set is 0.890, which indicates that the model has a very good ability to distinguish between the two classes in the training data.\nThe AUC for the test set is 0.888, which is nearly the same as the training AUC. This means that the model is generalizing well to new data and not overfitting to the training data. The small decrease from training to test is nearly negligible.\n\n\n1. Create predictions\n\n# Check for all unique message values in the dataset and count how many\npentathlon_nptb[\"message\"].value_counts()\n\nmessage\ncontrol        87260\nracquet        87088\nteam           86792\nbackcountry    86604\nwater          85120\nstrength       84280\nendurance      82856\nName: count, dtype: int64\n\n\n\n# Create predictions for different scenarios where the message variable is set to specific values\npentathlon_nptb[\"p_control_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb[\"p_racquet_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb[\"p_team_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb[\"p_backcountry_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb[\"p_water_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb[\"p_strength_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb[\"p_endurance_nn\"] = NN_best.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\npentathlon_nptb\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\nordersize_random_reg\nep_random_lr\npred_NN\np_control_nn\np_racquet_nn\np_team_nn\np_backcountry_nn\np_water_nn\np_strength_nn\np_endurance_nn\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\n32.627819\n0.156712\n0.011532\n0.009833\n0.011503\n0.011532\n0.011059\n0.010634\n0.011800\n0.011186\n\n\n1\nU3\nno\n0\nbackcountry\n45 to 59\nno\n35000\n22\n1.0\n0\n...\n37.181377\n0.082633\n0.002658\n0.002535\n0.002404\n0.003184\n0.002658\n0.003101\n0.003468\n0.002342\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\n40.744326\n0.226274\n0.008840\n0.006773\n0.007218\n0.008068\n0.007635\n0.008054\n0.009127\n0.008840\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\n15.768889\n0.015070\n0.001161\n0.001065\n0.001360\n0.000875\n0.001357\n0.001161\n0.001291\n0.003342\n\n\n4\nU25\nno\n0\nracquet\n&gt;= 60\nyes\n65000\n32\n1.1\n1\n...\n48.946607\n0.235699\n0.009574\n0.007788\n0.009574\n0.008922\n0.008792\n0.008076\n0.008884\n0.008817\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\n38.698478\n0.031609\n0.001644\n0.001504\n0.001932\n0.001207\n0.001928\n0.001644\n0.001835\n0.004317\n\n\n599996\nU3462900\nno\n0\nteam\n&lt; 30\nno\n55000\n32\n0.9\n3\n...\n29.767369\n0.091593\n0.004586\n0.003494\n0.003388\n0.004586\n0.003605\n0.004077\n0.004503\n0.002707\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\n32.707711\n0.107603\n0.002481\n0.001935\n0.002163\n0.002481\n0.001966\n0.001951\n0.002062\n0.001856\n\n\n599998\nU3462916\nno\n0\nteam\n&lt; 30\nno\n50000\n35\n0.6\n2\n...\n25.117259\n0.068280\n0.003051\n0.002224\n0.002025\n0.003051\n0.002200\n0.002624\n0.002867\n0.001594\n\n\n599999\nU3462922\nno\n0\nendurance\n30 to 44\nyes\n50000\n25\n0.7\n1\n...\n29.102240\n0.138029\n0.002656\n0.003538\n0.003329\n0.004762\n0.003617\n0.004233\n0.004701\n0.002656\n\n\n\n\n600000 rows √ó 63 columns\n\n\n\n\n\nrepl = {\n    \"p_control_nn\": \"control\", \n    \"p_endurance_nn\": \"endurance\", \n    \"p_backcountry_nn\": \"backcountry\", \n    \"p_racquet_nn\": \"racquet\", \n    \"p_strength_nn\": \"strength\", \n    \"p_team_nn\": \"team\", \n    \"p_water_nn\": \"water\"}\n\n\n# extending the prediction to the full database to see distribution\npredictions_nn = [\n    \"p_control_nn\", \n    \"p_endurance_nn\", \n    \"p_backcountry_nn\", \n    \"p_racquet_nn\", \n    \"p_strength_nn\", \n    \"p_team_nn\", \n    \"p_water_nn\"]\n\n\n# extending the prediction to the full database to see distribution\npentathlon_nptb[\"message_nn\"] = pentathlon_nptb[predictions_nn].idxmax(axis=1)\npentathlon_nptb[\"message_nn\"].value_counts()\n\nmessage_nn\np_endurance_nn    321011\np_strength_nn     161291\np_team_nn         106081\np_racquet_nn       11617\nName: count, dtype: int64\n\n\n\n# Find the maximum probability of purchase\npentathlon_nptb[\"p_max_nn\"] = pentathlon_nptb[predictions_nn].max(axis=1)\n\nApproach: First of all, a neural network model was built to predict the next product to buy using buyer_yes as the response variable and the other variables as predictors. The model was trained using the training data and then used to predict the probability of purchasing for the test data. The model was then used to predict the probability of purchasing for the full database. To predict the probability of purchasing for different messages, we use the data_cmd to predict the probability of purchasing for different messages.\nFinally, we choose the product with the highest probability using idxmax to automatically find the best message for each customer. This command also provides a label for the category with the maximum predicted probability of buying across all the products. Then, p_max was created to store the maximum predicted probability of purchasing for the best message selected for each customer.\n\n2. Percentage of customersfor whom that message or not message\n\n# create a crosstab to see which products should we send to each customer.\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].message_nn, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nmessage_nn\n\n\n\n\n\np_endurance_nn\n96,065\n\n\np_racquet_nn\n3,473\n\n\np_strength_nn\n48,593\n\n\np_team_nn\n31,869\n\n\n\n\n\n\n\n\n\n# percentage of customers for whom that message is the best message\npentathlon_nptb[\"message_nn\"].value_counts(normalize=True)\n\nmessage_nn\np_endurance_nn    0.535018\np_strength_nn     0.268818\np_team_nn         0.176802\np_racquet_nn      0.019362\nName: proportion, dtype: float64\n\n\n\nedurance: The distribution suggests that endurance, related messages resonate significantly more with the customers than the other messages.\nteam: While not as dominant as endurance, related messages also play an important role. There might be opportunities to further optimize or tailor these messages to increase their effectiveness.\nOther categories such as strength, racquet, and water, have much smaller proportions, suggesting that they are less often the best choice for maximizing the probability of purchase.\nbackcountry was not included in the original dataset, so it is not surprising that it is not the best message for any customer.`\n\n\n# average purchase probability if we send the message for each product to everyone\npentathlon_nptb.loc[pentathlon_nptb.training == 0, predictions_nn].agg(\"mean\").sort_values(\n    ascending=False).apply(rsm.format_nr, perc=True)\n\np_endurance_nn      2.75%\np_strength_nn       2.64%\np_water_nn          2.42%\np_team_nn           2.37%\np_backcountry_nn    2.33%\np_racquet_nn        2.29%\np_control_nn        2.14%\ndtype: object\n\n\n\n\n3. Expected profit\n\n# Calculate the avg order size/message\nordersize = pentathlon_nptb[(pentathlon_nptb.training == 1) & (pentathlon_nptb.buyer_yes == 1)].groupby(\"message\")[\"total_os\"].mean()\nordersize\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/3828228129.py:2: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nmessage\nbackcountry    64.034091\ncontrol        49.900598\nendurance      55.584893\nracquet        56.405620\nstrength       56.708751\nteam           56.522449\nwater          61.957343\nName: total_os, dtype: float64\n\n\n\nNN_os = rsm.model.mlp(\n    data={\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar = \"total_os\",\n    evar = evar,\n    hidden_layer_sizes = (1,),  # Simple NN with 1 hidden layer\n    mod_type = 'regression'\n)\nNN_os.summary()\n\nMulti-layer Perceptron (NN)\nData                 : pentathlon_nptb\nResponse variable    : total_os\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nModel type           : regression\nNr. of features      : (12, 19)\nNr. of observations  : 420,000\nHidden_layer_sizes   : (1,)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nModel fit            :\n       n     r2    mse    mae\n  420000  0.109  0.891  0.179\n\nRaw data             :\n  message      age female  income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet\n     team 30 to 44     no   55000         19       0.8               0              4           0          4                 0             1\nendurance 45 to 59    yes   45000         33       0.7               0              0           0          0                 2             2\n    water 45 to 59    yes   25000         24       0.2               0              0           0          0                 0             0\n strength     &lt; 30    yes   25000         18       0.3               0              0           0          0                 0             0\n strength    &gt;= 60    yes   65000         36       1.2               1              1           0          2                 0             3\n\nEstimation data      :\n   income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n 0.388655  -0.663713 -0.265321       -0.640336       1.100861   -0.261727   1.892088         -0.690593      0.058975            False              False            False             False          True          False          True         False      False       True\n-0.184052   0.279378 -0.479844       -0.640336      -0.706563   -0.261727  -0.620020          1.958233      0.882489            False               True            False             False         False          False         False          True      False      False\n-1.329467  -0.326895 -1.552460       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False             False         False           True         False          True      False      False\n-1.329467  -0.731077 -1.337937       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False              True         False          False         False         False      False      False\n 0.961362   0.481469  0.592772        0.060592      -0.254707   -0.261727   0.636034         -0.690593      1.706002            False              False            False              True         False          False         False         False       True      False\n\n\n\n\nModel Tuning\n\n# NN_os = rsm.load_state(\"./data/NN_os.pkl\")['NN_os']\n\nAgain, after fine-tuning the model in a separate notebook, we preserved the model‚Äôs state with save_state. To reintegrate it into the main notebook, load_state was utilized. However, due to GitHub‚Äôs file evaluation constraints, we are unable to successfully pass the automated tests using the aforementioned code. Should you wish to review our tuned model, please refer to the auxiliary notebooks. You can replicate our steps there by employing the provided code above\n\n# NN_os.best_params_\n\n\n# NN_cv.best_score_.round(3)\n\n\nNN_os_best = rsm.model.mlp(\n    data={\"pentathlon_nptb\": pentathlon_nptb[pentathlon_nptb[\"training\"] == 1]},\n    rvar=\"total_os\",\n    evar= evar,\n    alpha = 0.0001,\n    hidden_layer_sizes = (3, 3),\n    mod_type='regression'\n)\n\nNN_os_best.summary()\n\nMulti-layer Perceptron (NN)\nData                 : pentathlon_nptb\nResponse variable    : total_os\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nModel type           : regression\nNr. of features      : (12, 19)\nNr. of observations  : 420,000\nHidden_layer_sizes   : (3, 3)\nActivation function  : tanh\nSolver               : lbfgs\nAlpha                : 0.0001\nBatch size           : auto\nLearning rate        : 0.001\nMaximum iterations   : 10000\nrandom_state         : 1234\nModel fit            :\n       n     r2    mse    mae\n  420000  0.114  0.886  0.175\n\nRaw data             :\n  message      age female  income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet\n     team 30 to 44     no   55000         19       0.8               0              4           0          4                 0             1\nendurance 45 to 59    yes   45000         33       0.7               0              0           0          0                 2             2\n    water 45 to 59    yes   25000         24       0.2               0              0           0          0                 0             0\n strength     &lt; 30    yes   25000         18       0.3               0              0           0          0                 0             0\n strength    &gt;= 60    yes   65000         36       1.2               1              1           0          2                 0             3\n\nEstimation data      :\n   income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n 0.388655  -0.663713 -0.265321       -0.640336       1.100861   -0.261727   1.892088         -0.690593      0.058975            False              False            False             False          True          False          True         False      False       True\n-0.184052   0.279378 -0.479844       -0.640336      -0.706563   -0.261727  -0.620020          1.958233      0.882489            False               True            False             False         False          False         False          True      False      False\n-1.329467  -0.326895 -1.552460       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False             False         False           True         False          True      False      False\n-1.329467  -0.731077 -1.337937       -0.640336      -0.706563   -0.261727  -0.620020         -0.690593     -0.764538            False              False            False              True         False          False         False         False      False      False\n 0.961362   0.481469  0.592772        0.060592      -0.254707   -0.261727   0.636034         -0.690593      1.706002            False              False            False              True         False          False         False         False       True      False\n\n\n\npentathlon_nptb[\"pos_control_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb[\"pos_racquet_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb[\"pos_team_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb[\"pos_backcountry_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb[\"pos_water_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb[\"pos_strength_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb[\"pos_endurance_nn\"] = NN_os_best.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\n\n\npentathlon_nptb[\"ep_control_nn\"] = pentathlon_nptb[\"p_control_nn\"] * pentathlon_nptb[\"pos_control_reg\"] * 0.4\npentathlon_nptb[\"ep_racquet_nn\"] = pentathlon_nptb[\"p_racquet_nn\"] * pentathlon_nptb[\"pos_racquet_reg\"] * 0.4\npentathlon_nptb[\"ep_team_nn\"] = pentathlon_nptb[\"p_team_nn\"] * pentathlon_nptb[\"pos_team_reg\"] * 0.4\npentathlon_nptb[\"ep_backcountry_nn\"] = pentathlon_nptb[\"p_backcountry_nn\"] * pentathlon_nptb[\"pos_backcountry_reg\"] * 0.4\npentathlon_nptb[\"ep_water_nn\"] = pentathlon_nptb[\"p_water_nn\"] * pentathlon_nptb[\"pos_water_reg\"] * 0.4\npentathlon_nptb[\"ep_strength_nn\"] = pentathlon_nptb[\"p_strength_nn\"] * pentathlon_nptb[\"pos_strength_reg\"] * 0.4\npentathlon_nptb[\"ep_endurance_nn\"] = pentathlon_nptb[\"p_endurance_nn\"] * pentathlon_nptb[\"pos_endurance_reg\"] * 0.4\n\n\nexpected_profit_nn = [\n    \"ep_control_nn\", \n    \"ep_endurance_nn\", \n    \"ep_backcountry_nn\", \n    \"ep_racquet_nn\", \n    \"ep_strength_nn\", \n    \"ep_team_nn\", \n    \"ep_water_nn\"]\n\n\nrepl = {\"ep_control_nn\": \"control\", \"ep_endurance_nn\": \"endurance\", \"ep_backcountry_nn\": \"backcountry\", \"ep_racquet_nn\": \"racquet\", \"ep_strength_nn\": \"strength\", \"ep_team_nn\": \"team\", \"ep_water_nn\": \"water\"}\npentathlon_nptb[\"ep_message_nn\"] = (\n    pentathlon_nptb[expected_profit_nn]\n    .idxmax(axis=1)\n    .map(repl)\n)\n\n\n\n\n4. Respective message maximizes expected profit\n\npentathlon_nptb.ep_message_nn.value_counts(normalize=True)\n\nep_message_nn\nendurance      0.369478\nteam           0.245130\nwater          0.221107\nstrength       0.110613\nbackcountry    0.024117\nracquet        0.019118\ncontrol        0.010437\nName: proportion, dtype: float64\n\n\nThe message that most customers respond the best to is endurance holding close to almost half of the customer. Messages such as backcountry, racquet, and control are not the best choice for nearly any customer. The remaining three messages, team, strength, and water, are the best choice for a small proportion of customers.\n\n\n5. Expected profit/customer\n\npentathlon_nptb[\"ep_max_nn\"] = pentathlon_nptb[expected_profit_nn].max(axis=1)\npentathlon_nptb[\"ep_max_nn\"].mean()\n\n0.6816084703184525\n\n\n\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].ep_message_nn, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nep_message_nn\n\n\n\n\n\nbackcountry\n4,371\n\n\ncontrol\n1,914\n\n\nendurance\n66,071\n\n\nracquet\n3,429\n\n\nstrength\n19,986\n\n\nteam\n44,334\n\n\nwater\n39,895\n\n\n\n\n\n\n\n\n\n6. Expected profit per e-mailed customer if every customer receives the same message\n\n(\n    pentathlon_nptb\n    .loc[pentathlon_nptb.training == 0, expected_profit_nn]\n    .agg(\"mean\")\n    .sort_values(ascending=False)\n    .apply(rsm.format_nr, sym = \"$\", dec = 2)\n)\n\nep_endurance_nn      $0.61\nep_strength_nn        $0.6\nep_water_nn           $0.6\nep_backcountry_nn    $0.59\nep_team_nn           $0.53\nep_racquet_nn        $0.52\nep_control_nn        $0.43\ndtype: object\n\n\nTo no surprise, sending no message yields the lowest expected profit per customer. The message that maximizes the expected profit per customer are endurance and strength tied. The remaining messages of team, water, backcountry yield slightly less than strength and endurance. Racquet yields slightly more than sending no message but still less than the other messages.\n\n\n\n7. Expected profit per e-mailed customer if every customer is assigned randomly to one of the messages options\n\n# probabilty of purchase where customer is assigned to a random message\npentathlon_nptb[\"p_random_nn\"] = NN_best.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected avg order size where customer is assigned to a random message\npentathlon_nptb[\"ordersize_random_nn\"] = NN_os_best.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected profit where customer is assigned to a random message\npentathlon_nptb[\"ep_random_nn\"] = pentathlon_nptb[\"p_random_nn\"] * pentathlon_nptb[\"ordersize_random_nn\"] * 0.4\n\n# expected profit per customer where customer is assigned to a random message\nrandom_profit_per_customer = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_random_nn\"].mean()\nrandom_profit_per_customer\n\n0.1078448520217575\n\n\n\n# expected profit where no-message is sent (control)\npentathlon_nptb[\"ep_control_nn\"]\n\n# expected profit per customer where no-message is sent (control)\ncontrol_profit_per_customer = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_control_nn\"].mean()\ncontrol_profit_per_customer\n\n0.4335109104691227\n\n\n\n\n8. Expected profit for 5,000,000 customers\n\n# Profit where each customer is assigned to the message with the highest expected profit\nprofit_nn = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_max_nn\"].agg(\"mean\") * 5000000\nprofit_nn\n\n3430076.6444671643\n\n\n\n# Profit where each customer is sent to a random message\nrandom_profit_nn = random_profit_per_customer * 5000000\nrandom_profit_nn\n\n539224.2601087876\n\n\n\n# Profit where no message is sent\ncontrol_profit_nn = control_profit_per_customer * 5000000\ncontrol_profit_nn\n\n2167554.5523456135\n\n\n\nprofit_improvement_nn = profit_nn - control_profit_nn\nprofit_improvement_nn\n\n1262522.0921215508\n\n\n\nprofits_dct = {\n    \"Customize Message\": profit_nn,\n    \"Randomly Assign\": random_profit_nn,\n    \"No Message Sent\": control_profit_nn,\n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(profits_dct.items()), columns=['Model', 'Profit'])\nplt.figure(figsize=(10, 5))  \n# Plot\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"Model\", y=\"Profit\", data=df, palette=\"viridis\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.Profit, f'${row.Profit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45) \nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/3855792413.py:16: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#random-forest-model",
    "href": "projects/Pentathlon/pentathlon_nptb.html#random-forest-model",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Random Forest Model",
    "text": "Random Forest Model\n\nTrain a Random Forest Model to predict the next product to buy\n\nrf = rsm.model.rforest(\n    data = {'NPTB': pentathlon_nptb.query(\"training == 1\")},\n    rvar = 'buyer',\n    lev = 'yes',\n    evar = evar,\n)\nrf.summary()\n\nRandom Forest\nData                 : NPTB\nResponse variable    : buyer\nLevel                : yes\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nOOB                  : True\nModel type           : classification\nNr. of features      : (12, 21)\nNr. of observations  : 420,000\nmax_features         : sqrt (4)\nn_estimators         : 100\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.825\n\nEstimation data      :\n income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_backcountry  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_&lt; 30  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n  55000         19       0.8               0              4           0          4                 0             1                False            False              False            False             False          True          False     False          True         False      False       True\n  45000         33       0.7               0              0           0          0                 2             2                False            False               True            False             False         False          False     False         False          True      False      False\n  25000         24       0.2               0              0           0          0                 0             0                False            False              False            False             False         False           True     False         False          True      False      False\n  25000         18       0.3               0              0           0          0                 0             0                False            False              False            False              True         False          False      True         False         False      False      False\n  65000         36       1.2               1              1           0          2                 0             3                False            False              False            False              True         False          False     False         False         False       True      False\n\n\nWe want to also use a random forest model to predict the next product to buy and see if we can create a robust NPTB model that accurately can predict which product a customer would purchase based on the messaging we send them. Above, we are training our base random forest model, and we will use this base random forest model to tune our hyperparameters, as seen below.\nSome interesting key features of the output of this random forest model is the AUC value. In our base model, we see that the AUC is a 0.818. This is a decent value for AUC, with anything over 0.5 typically being considered a good model.\n\nRandom Forest Model Tuning\n\n#max_features = [None, 'auto', 'sqrt', 'log2', 0.25, 0.5, 0.75, 1.0]\n#n_estimators = [10, 50, 100, 200, 500, 1000]\n\n\n#param_grid = {\"max_features\": max_features, \"n_estimators\": n_estimators}\n#scoring = {\"AUC\": \"roc_auc\"}\n\n#rf_cv = GridSearchCV(rf.fitted, param_grid, scoring=scoring, cv=5, n_jobs=2, refit=\"AUC\", verbose=5)\n\nBelow is the code for the grid search of a very large matrix. I would not recommend running this, as it takes a very long time to run. However, I have included the code for the grid search below.\nrf_cv.fit(rf.data_onehot, rf.data.buyer_yes)\n\n::: {#bdc7b1cb .cell execution_count=107}\n``` {.python .cell-code}\n#rf_cv.fit(rf_treatment.data_onehot, rf_treatment.data.buyer_yes)\n:::\nAbove, we conduct a grid search to identify which hyperparameters are best for our random forest model. However, when running a gridsearch this large, we ran into a computational cost issue that caused either our hyperparameter grid search to run for 4 hours, or caused the kernel to crash. Therefore, we conducted multiple step-wise grid searches where we limited the gridsearch matrix size, and went through the various iterations of combinations to see which parameters created the most robut model. Below, you can see the various rounds of model tuning.\n\n\nRandom Forest Model Tuning - Stepwise, limiting the grid search\n\n#max_features1 = ['sqrt']\n#n_estimators1 = [1000, 2000]\n\n\n#param_grid = {\"max_features\": max_features1, \"n_estimators\": n_estimators1}\n#scoring = {\"AUC\": \"roc_auc\"}\n\n#rf_cv_treatment1 = GridSearchCV(rf_treatment.fitted, param_grid, scoring=scoring, cv=4, n_jobs=2, refit=\"AUC\", verbose=5)\n\nBecause our random forest model took over 4 hours to run previously, we decided to limit the grid search to 9 iterations. Our goal is to create a balance between exploring the hyperparameter space and keeping the computational cost reasonable.\n\n#rf_cv_treatment1.fit(rf_treatment.data_onehot, rf_treatment.data.buyer_yes)\n\n\n#rf_cv_treatment1.best_params_\n\nAs seen above, the grid search identified that the best parameters were as follows: - max_features: ‚Äòsqrt‚Äô - n_estimators: 1000\nWhen we run our random forest model with the best parameters identified from the grid search, the AUC score is approximately 0.862.\n\nrf_tuned = rsm.model.rforest(\n    data = {'NPTB': pentathlon_nptb.query(\"training == 1\")},\n    rvar = 'buyer',\n    lev = 'yes',\n    evar = evar,\n    n_estimators = 1000,\n    max_features = 'sqrt'\n)\nrf_tuned.summary()\n\nRandom Forest\nData                 : NPTB\nResponse variable    : buyer\nLevel                : yes\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nOOB                  : True\nModel type           : classification\nNr. of features      : (12, 21)\nNr. of observations  : 420,000\nmax_features         : sqrt (4)\nn_estimators         : 1000\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : 0.864\n\nEstimation data      :\n income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_backcountry  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_&lt; 30  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n  55000         19       0.8               0              4           0          4                 0             1                False            False              False            False             False          True          False     False          True         False      False       True\n  45000         33       0.7               0              0           0          0                 2             2                False            False               True            False             False         False          False     False         False          True      False      False\n  25000         24       0.2               0              0           0          0                 0             0                False            False              False            False             False         False           True     False         False          True      False      False\n  25000         18       0.3               0              0           0          0                 0             0                False            False              False            False              True         False          False      True         False         False      False      False\n  65000         36       1.2               1              1           0          2                 0             3                False            False              False            False              True         False          False     False         False         False       True      False\n\n\n\npentathlon_nptb['pred_rf'] = rf_tuned.predict(pentathlon_nptb)['prediction']\npentathlon_nptb['pred_rf'].mean()\n\n0.025235874483417724\n\n\n\ndct_rf = {\"train_rf\": pentathlon_nptb.query(\"training == 1\"), \"test_rf\": pentathlon_nptb.query(\"training == 0\")}\nfig = rsm.gains_plot(dct_rf, \"buyer\", \"yes\", \"pred_rf\")\n\n\n\n\n\n\n\n\nThe gains chart above is a visual representation of the AUC score. The gains chart shows the percentage of the total number of cases in a given category that are captured by the model. Based on the steep rise at the beginning of the curve, indicates that the model is effective at ranking the positive events, i.e.. buyers, higher than the negative ones. The more steep the initial part of the curve, the better the model is at identifying the positive events early on when you start targeting the population based on the model‚Äôs scores.\nIn this specific chart, the model seems to perform fairly well, particularly because the initial sections of both curves are quite steep, indicating that a significant percentage of the positive events can be captured by targeting a relatively small percentage of the population.\n\nfrom sklearn import metrics\n\n\ndef apply_rand_message(data, pred_col, message_col):\n    np.random.seed(1234)\n    data[\"random_number\"] = np.random.randint(2, size = len(data))\n    data[\"random_message\"] = rsm.ifelse(data[\"random_number\"] == 1, data[message_col], \"no message\")\n    data[\"random_pred\"] = rsm.ifelse(data[\"random_number\"] == 1, data[pred_col], 0)\n    return data\n\n\n# prediction on training data\npred_rf = pentathlon_nptb.query(\"training == 1\")[\"pred_rf\"]\nactual_rf = pentathlon_nptb.query(\"training == 1\")[\"buyer_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual_rf, pred_rf)\nmetrics.auc(fpr, tpr).round(3)\n\n1.0\n\n\n\n# prediction on test data\npred_rf = pentathlon_nptb.query(\"training == 0\")[\"pred_rf\"]\nactual_rf = pentathlon_nptb.query(\"training == 0\")[\"buyer_yes\"]\nfpr, tpr, thresholds = metrics.roc_curve(actual_rf, pred_rf)\nmetrics.auc(fpr, tpr).round(3)\n\n0.869\n\n\nBased on the Gains Chart, and the AUC values for the test and training set, we can conclude that the model is performing decently well. Because of the perfect AUC score on the training data, there is a slight hesitation that the model may be overfitting. However, the AUC score on the test data is still quite high, indicating that the model is generalizing well.\nThe AUC for the training set is 1, which indicates that the model has a very good ability to distinguish between the two classes in the training data. This is also supported by the steep increase in the first part of the gains chart in the training data. The AUC for the test set is 0.865, which is slightly lower but still indicates a very good predictive performance on unseen data.\n\n\n1. Create prediction for each product\n\npentathlon_nptb['p_control_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb['p_racquet_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb['p_team_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb['p_backcountry_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb['p_water_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb['p_strength_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb['p_endurance_rf'] = rf_tuned.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\n\n\npentathlon_nptb\n\n\n\n\n\n\n\n\n\ncustid\nbuyer\ntotal_os\nmessage\nage\nfemale\nincome\neducation\nchildren\nfreq_endurance\n...\nordersize_random_nn\nep_random_nn\npred_rf\np_control_rf\np_racquet_rf\np_team_rf\np_backcountry_rf\np_water_rf\np_strength_rf\np_endurance_rf\n\n\n\n\n0\nU1\nno\n0\nteam\n30 to 44\nno\n55000\n19\n0.8\n0\n...\n0.404840\n0.001868\n0.001000\n0.004\n0.001\n0.001\n0.008000\n0.012\n0.028\n0.070000\n\n\n1\nU3\nno\n0\nbackcountry\n45 to 59\nno\n35000\n22\n1.0\n0\n...\n0.162739\n0.000173\n0.000000\n0.000\n0.000\n0.000\n0.000000\n0.000\n0.000\n0.000000\n\n\n2\nU13\nno\n0\nendurance\n45 to 59\nyes\n45000\n33\n0.7\n0\n...\n0.604567\n0.002138\n0.001000\n0.002\n0.000\n0.002\n0.001333\n0.000\n0.000\n0.001000\n\n\n3\nU20\nno\n0\nwater\n45 to 59\nyes\n25000\n24\n0.2\n0\n...\n-0.002303\n-0.000001\n0.000000\n0.000\n0.000\n0.000\n0.000000\n0.000\n0.000\n0.000000\n\n\n4\nU25\nno\n0\nracquet\n&gt;= 60\nyes\n65000\n32\n1.1\n1\n...\n0.780294\n0.002988\n0.019000\n0.009\n0.019\n0.007\n0.023000\n0.009\n0.010\n0.005000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599995\nU3462888\nno\n0\nwater\n&gt;= 60\nyes\n40000\n26\n0.6\n0\n...\n-0.482575\n-0.000317\n0.000000\n0.000\n0.000\n0.000\n0.000000\n0.000\n0.000\n0.000000\n\n\n599996\nU3462900\nno\n0\nteam\n&lt; 30\nno\n55000\n32\n0.9\n3\n...\n0.155320\n0.000285\n0.000000\n0.001\n0.001\n0.000\n0.000000\n0.000\n0.001\n0.000000\n\n\n599997\nU3462902\nno\n0\nteam\n&lt; 30\nyes\n55000\n32\n0.9\n0\n...\n0.153595\n0.000152\n0.000000\n0.006\n0.003\n0.000\n0.029000\n0.005\n0.010\n0.003000\n\n\n599998\nU3462916\nno\n0\nteam\n&lt; 30\nno\n50000\n35\n0.6\n2\n...\n-0.042065\n-0.000051\n0.011000\n0.002\n0.005\n0.011\n0.001000\n0.000\n0.000\n0.000000\n\n\n599999\nU3462922\nno\n0\nendurance\n30 to 44\nyes\n50000\n25\n0.7\n1\n...\n0.483980\n0.000514\n0.021667\n0.002\n0.000\n0.005\n0.002000\n0.001\n0.003\n0.021667\n\n\n\n\n600000 rows √ó 92 columns\n\n\n\n\nTo identify which message will lead to the highest probability of purchase, we will use our random forest model to extrapolate predictions to the full database.\n\nrepl = {\n    \"p_control_rf\": \"control\", \n    \"p_endurance_rf\": \"endurance\", \n    \"p_backcountry_rf\": \"backcountry\", \n    \"p_racquet_rf\": \"racquet\", \n    \"p_strength_rf\": \"strength\", \n    \"p_team_rf\": \"team\", \n    \"p_water_rf\": \"water\"}\n\n\npredictions_rf = [\n    \"p_control_rf\", \n    \"p_endurance_rf\", \n    \"p_backcountry_rf\", \n    \"p_racquet_rf\", \n    \"p_strength_rf\", \n    \"p_team_rf\", \n    \"p_water_rf\"]\n\n\npentathlon_nptb['message_rf'] = (\n    pentathlon_nptb[predictions_rf]\n    .idxmax(axis=1)\n    .map(repl)\n)\n\nThe code above is creating a new column in our Pentathlon_NPTB database, called message_rf which identifies which of the predicted messages has the highest probability, and then filling in the message_rf column with the name of the message that has the highest probability.\n\n#plot the distribution of message_rf column\npentathlon_nptb['message_rf'].value_counts().plot(kind='bar', title = 'Distribution of message_rf')\n\n\n\n\n\n\n\n\n\npentathlon_nptb['p_max_rf'] = pentathlon_nptb[predictions_rf].max(axis=1)\n\nApproach  Our approach to creating the targeting messaging is building a random forest model to predict the next product the customer will buy based on a variety of customer features. We will use buyer_yes as the response variable and the other variables as predictors.\nWe use our random forest model to extend the prediction to the full database. To predict the probability of purchasing for different messages, we use the data_cmd to predict the probability of purchasing for different messages.\nFinally, we select the product with the highest probability using idxmax. This command also provides a label for the category with the maximum predicted probability of buying across all the products. Then, create p_max to store the maximum predicted probability of purchasing an item based on the messaging sent to said customer.\n\n\n\n2. Percentage of customers to message and not to message\n\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].message_rf, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nmessage_rf\n\n\n\n\n\nbackcountry\n18,952\n\n\ncontrol\n53,019\n\n\nendurance\n29,327\n\n\nracquet\n18,859\n\n\nstrength\n23,927\n\n\nteam\n18,809\n\n\nwater\n17,107\n\n\n\n\n\n\n\n\n\npentathlon_nptb['message_rf'].value_counts(normalize=True)\n\nmessage_rf\ncontrol        0.299772\nendurance      0.162343\nstrength       0.132712\nracquet        0.104497\nbackcountry    0.103977\nteam           0.102348\nwater          0.094352\nName: proportion, dtype: float64\n\n\n\ncontrol: The distribution suggests that the control_rf control messaging seems to resonate more with the customers than the other messages based on this model.\nendurance, strength: the endurance and strength messages are approximately similar in their values. There might be opportunities to further optimize or tailor these messages to increase their effectiveness.\nbackcountry, racquet, water, team, all have roughly the same proportion of customers to message at around 10%. This suggests that these messages are not as effective as the control and endurance messages.\n\n\npentathlon_nptb['message_rf'].value_counts(normalize=True).plot(kind='bar', title='Message Distribution')\n\n\n\n\n\n\n\n\n\npentathlon_nptb.loc[pentathlon_nptb.training == 0, predictions_rf].agg(\"mean\").sort_values(\n    ascending=False).apply(rsm.format_nr, perc=True)\n\np_endurance_rf      2.91%\np_strength_rf       2.79%\np_water_rf          2.63%\np_team_rf           2.61%\np_backcountry_rf    2.55%\np_racquet_rf        2.55%\np_control_rf        2.43%\ndtype: object\n\n\nAbove is a table with the average purchase probability if we send the message for each product to everyone. Even though we are sending much more control messages compared to the other categories, we see that this does not lead to an increase in the probability of purchase. This is an interesting insight, and we should consider other ways to optimize our messaging strategy to increase the number of messages sent to those in the endurance and strength categories.\n\n\n3. Expected profit\n\n#Average Ordersize per message\nordersize = pentathlon_nptb[(pentathlon_nptb.training == 1) & (pentathlon_nptb.buyer_yes == 1)].groupby(\"message_rf\")[\"total_os\"].mean()\nordersize\n\nmessage_rf\nbackcountry    64.065387\ncontrol        49.908751\nendurance      55.584893\nracquet        56.416727\nstrength       56.684246\nteam           56.522449\nwater          61.926676\nName: total_os, dtype: float64\n\n\n\nCreate another random forest model to Predict the Ordersize\n\nrf_os = rsm.model.rforest(\n    data = {\"pentathlon_nptb\": pentathlon_nptb[(pentathlon_nptb.training == 1) & (pentathlon_nptb.buyer == \"yes\")]},\n    rvar =\"total_os\",\n    evar = evar,\n    n_estimators = 200,\n    max_features = 'sqrt'\n)\nrf_os.summary()\n\nRandom Forest\nData                 : pentathlon_nptb\nResponse variable    : total_os\nLevel                : None\nExplanatory variables: message, age, female, income, education, children, freq_endurance, freq_strength, freq_water, freq_team, freq_backcountry, freq_racquet\nOOB                  : True\nModel type           : classification\nNr. of features      : (12, 21)\nNr. of observations  : 10,080\nmax_features         : sqrt (4)\nn_estimators         : 200\nmin_samples_leaf     : 1\nrandom_state         : 1234\nAUC                  : nan\n\nEstimation data      :\n income  education  children  freq_endurance  freq_strength  freq_water  freq_team  freq_backcountry  freq_racquet  message_backcountry  message_control  message_endurance  message_racquet  message_strength  message_team  message_water  age_&lt; 30  age_30 to 44  age_45 to 59  age_&gt;= 60  female_no\n  65000         52       0.5               3              0           0          2                 1             3                False            False               True            False             False         False          False     False         False          True      False       True\n  55000         43       0.2               1              2           0          0                 2             0                False             True              False            False             False         False          False     False         False          True      False       True\n 160000         82       1.3              13              5           0         15                 5             2                 True            False              False            False             False         False          False     False          True         False      False       True\n  90000         60       1.2               7              2           2          5                 0             1                 True            False              False            False             False         False          False     False         False          True      False       True\n  55000         40       0.7               0              1           0          1                 1             0                False            False              False            False              True         False          False     False          True         False      False       True\n\n\n/Users/duyentran/Library/Python/3.11/lib/python/site-packages/pyrsm/model/perf.py:1240: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\n\n\n\npentathlon_nptb['pos_control_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"control\"})[\"prediction\"]\npentathlon_nptb['pos_racquet_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"racquet\"})[\"prediction\"]\npentathlon_nptb['pos_team_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"team\"})[\"prediction\"]\npentathlon_nptb['pos_backcountry_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"backcountry\"})[\"prediction\"]\npentathlon_nptb['pos_water_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"water\"})[\"prediction\"]\npentathlon_nptb['pos_strength_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"strength\"})[\"prediction\"]\npentathlon_nptb['pos_endurance_rf'] = rf_os.predict(pentathlon_nptb, data_cmd={\"message\": \"endurance\"})[\"prediction\"]\n\n\n\nCalculating the expected profit for each product\n\npentathlon_nptb['ep_control_rf'] = pentathlon_nptb['p_control_rf'] * pentathlon_nptb['pos_control_reg'] * 0.4\npentathlon_nptb['ep_racquet_rf'] = pentathlon_nptb['p_racquet_rf'] * pentathlon_nptb['pos_racquet_reg'] * 0.4\npentathlon_nptb['ep_team_rf'] = pentathlon_nptb['p_team_rf'] * pentathlon_nptb['pos_team_reg'] * 0.4\npentathlon_nptb['ep_backcountry_rf'] = pentathlon_nptb['p_backcountry_rf'] * pentathlon_nptb['pos_backcountry_reg'] * 0.4\npentathlon_nptb['ep_water_rf'] = pentathlon_nptb['p_water_rf'] * pentathlon_nptb['pos_water_reg'] * 0.4\npentathlon_nptb['ep_strength_rf'] = pentathlon_nptb['p_strength_rf'] * pentathlon_nptb['pos_strength_reg'] * 0.4\npentathlon_nptb['ep_endurance_rf'] = pentathlon_nptb['p_endurance_rf'] * pentathlon_nptb['pos_endurance_reg'] * 0.4\n\n\nexpected_profit_rf = [\n    \"ep_control_rf\", \n    \"ep_endurance_rf\", \n    \"ep_backcountry_rf\", \n    \"ep_racquet_rf\", \n    \"ep_strength_rf\", \n    \"ep_team_rf\", \n    \"ep_water_rf\"]\n\n\nrepl = {\"ep_control_rf\": \"control\", \"ep_endurance_rf\": \"endurance\", \"ep_backcountry_rf\": \"backcountry\", \"ep_racquet_rf\": \"racquet\", \"ep_strength_rf\": \"strength\", \"ep_team_rf\": \"team\", \"ep_water_rf\": \"water\"}\npentathlon_nptb[\"ep_message_rf\"] = (\n    pentathlon_nptb[expected_profit_rf]\n    .idxmax(axis=1)\n    .map(repl)\n)\npentathlon_nptb['ep_message_rf'].value_counts()\n\nep_message_rf\ncontrol        171206\nendurance       88843\nwater           72279\nstrength        71154\nbackcountry     68671\nteam            65027\nracquet         62820\nName: count, dtype: int64\n\n\n\n4. Percentage of customer for whom no message maximizes their expected profits\n\npentathlon_nptb.ep_message_rf.value_counts(normalize=True)\n\nep_message_rf\ncontrol        0.285343\nendurance      0.148072\nwater          0.120465\nstrength       0.118590\nbackcountry    0.114452\nteam           0.108378\nracquet        0.104700\nName: proportion, dtype: float64\n\n\n\n\n\n5. Expected profit per email if we customize the message to each customers\n\npentathlon_nptb[\"ep_max_rf\"] = pentathlon_nptb[expected_profit_rf].max(axis=1)\npentathlon_nptb[\"ep_max_rf\"].mean()\n\n1.0516237742883443\n\n\n\npd.crosstab(index=pentathlon_nptb[pentathlon_nptb.training == 0].ep_message_rf, columns=\"count\").apply(rsm.format_nr)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nep_message_rf\n\n\n\n\n\nbackcountry\n21,405\n\n\ncontrol\n50,280\n\n\nendurance\n26,442\n\n\nracquet\n18,786\n\n\nstrength\n21,082\n\n\nteam\n19,690\n\n\nwater\n22,315\n\n\n\n\n\n\n\n\n\n\n6. Expected profit per e-mailed customer if every customer receives the same message\n\n(\n    pentathlon_nptb\n    .loc[pentathlon_nptb.training == 0, expected_profit_rf]\n    .agg(\"mean\")\n    .sort_values(ascending=False)\n    .apply(rsm.format_nr, sym = \"$\", dec = 2)\n)\n\nep_water_rf          $0.65\nep_backcountry_rf    $0.65\nep_endurance_rf      $0.64\nep_strength_rf       $0.63\nep_team_rf           $0.58\nep_racquet_rf        $0.58\nep_control_rf        $0.49\ndtype: object\n\n\n\n\n\n7. Expected profit per e-mailed customer if every customer is assigned randomly to one of the messages or the no-message condition\n\n# probabilty of purchase where customer is assigned to a random message\npentathlon_nptb[\"p_random_rf\"] = rf.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected avg order size where customer is assigned to a random message\npentathlon_nptb[\"ordersize_random_reg\"] = reg.predict(pentathlon_nptb)[\"prediction\"]\n\n# expected profit when customer is assigned to a random message\npentathlon_nptb[\"ep_random_rf\"] = pentathlon_nptb[\"p_random_rf\"] * pentathlon_nptb[\"ordersize_random_reg\"] * 0.4\n\n# expected profit/customer when a customer is assigned to a random message\nrandom_profit_per_customer_rf = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_random_rf\"].mean()\nrandom_profit_per_customer_rf\n\n0.5981461370014217\n\n\n\n# expected profit where no-message is sent (control)\npentathlon_nptb[\"ep_control_rf\"]\n\n# expected profit per customer where no-message is sent (control)\ncontrol_profit_per_customer_rf = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_control_rf\"].mean()\ncontrol_profit_per_customer_rf\n\n0.48744714166587705\n\n\n\n8. Profit for 5,000,000 customers\n\nprofit_rf = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_max_rf\"].agg(\"mean\") * 5000000\nprofit_rf\n\n4947465.355117234\n\n\n\n# Profit when a customer is sent a random message\nrandom_profit_rf = random_profit_per_customer_rf * 5000000\nrandom_profit_rf\n\n# Profit when there is no message sent\ncontrol_profit_rf = control_profit_per_customer_rf * 5000000\ncontrol_profit_rf\n\n2437235.708329385\n\n\n\nprofit_improvement_rf = profit_rf - control_profit_rf\nprofit_improvement_rf\n\n2510229.6467878493\n\n\n\nprofits_dct_rf = {\n    \"Customize Message\": profit_rf,\n    \"Randomly Assign\": random_profit_rf,\n    \"No Message Sent\": control_profit_rf,\n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(profits_dct_rf.items()), columns=['Model', 'Profit'])\nplt.figure(figsize=(10, 5))  # Adjust the width and height to your preference\n# Plot\nsns.set(style=\"white\")\nax = sns.barplot(x=\"Model\", y=\"Profit\", data=df, palette=\"coolwarm\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.Profit, f'${row.Profit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45)  # Rotate x labels for better readability\nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/2755170672.py:16: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#xgboost-model",
    "href": "projects/Pentathlon/pentathlon_nptb.html#xgboost-model",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "XGBoost Model",
    "text": "XGBoost Model\n\nrvar = \"buyer_yes\"\n\nevar = pentathlon_nptb.columns.to_list()\nevar = evar[evar.index(\"message\"):] # all columns after \"message\"\nevar = evar[:evar.index(\"freq_racquet\")+1] # all columns before \"freq_racquet\"\nevar\n\n['message',\n 'age',\n 'female',\n 'income',\n 'education',\n 'children',\n 'freq_endurance',\n 'freq_strength',\n 'freq_water',\n 'freq_team',\n 'freq_backcountry',\n 'freq_racquet']\n\n\n\n# Create X_train, X_test, y_train, y_test using \"training\" column\nX_train = pentathlon_nptb.loc[pentathlon_nptb['training'] == 1, evar]\nX_test = pentathlon_nptb.loc[pentathlon_nptb['training'] == 0, evar]\ny_train = pentathlon_nptb.loc[pentathlon_nptb['training'] == 1, rvar]\ny_test = pentathlon_nptb.loc[pentathlon_nptb['training'] == 0, rvar]\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n\n# Define the parameter grid to be used in RandomizedSearchCV\nparam_grid = {\n    'learning_rate': [0.1,0.01,0.001],  \n    'n_estimators': [100],\n    'max_depth': [3, 6 ,10 ] # Depths from 3 to 10\n}\n\n\n# Initialize the XGBClassifier\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',enable_categorical = True)\n\n\n# Initialize GridSearchCV\nxgb_tuned = GridSearchCV(estimator=xgb, param_grid= param_grid, scoring='roc_auc', cv=5, verbose=5)\n\n\n# Fit the model\nxgb_tuned.fit(X_train, y_train)\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.889 total time=   0.8s\n[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.884 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.890 total time=   0.7s\n[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.882 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.880 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=0.891 total time=   1.1s\n[CV 2/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=0.886 total time=   1.1s\n[CV 3/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=0.892 total time=   1.1s\n[CV 4/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=0.884 total time=   1.1s\n[CV 5/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=0.881 total time=   1.1s\n[CV 1/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.884 total time=   1.6s\n[CV 2/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.879 total time=   1.8s\n[CV 3/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.885 total time=   1.6s\n[CV 4/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.875 total time=   1.7s\n[CV 5/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.874 total time=   1.6s\n[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.864 total time=   0.6s\n[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.864 total time=   0.6s\n[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.870 total time=   0.6s\n[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.856 total time=   0.6s\n[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.854 total time=   0.6s\n[CV 1/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=0.880 total time=   1.0s\n[CV 2/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=0.876 total time=   1.0s\n[CV 3/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=0.882 total time=   1.0s\n[CV 4/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=0.873 total time=   1.0s\n[CV 5/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=0.870 total time=   1.0s\n[CV 1/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.882 total time=   1.6s\n[CV 2/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.876 total time=   1.6s\n[CV 3/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.885 total time=   1.6s\n[CV 4/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.877 total time=   1.6s\n[CV 5/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.875 total time=   1.7s\n[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.850 total time=   0.6s\n[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.840 total time=   0.5s\n[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.848 total time=   0.6s\n[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.837 total time=   0.6s\n[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.837 total time=   0.5s\n[CV 1/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=0.874 total time=   1.0s\n[CV 2/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=0.869 total time=   1.0s\n[CV 3/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=0.876 total time=   1.0s\n[CV 4/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=0.867 total time=   0.9s\n[CV 5/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=0.866 total time=   0.9s\n[CV 1/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=0.874 total time=   1.5s\n[CV 2/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=0.869 total time=   1.6s\n[CV 3/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=0.880 total time=   1.5s\n[CV 4/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=0.864 total time=   1.4s\n[CV 5/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=0.868 total time=   1.5s\n\n\nGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True,\n                                     eval_metric='logloss', feature_types=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=N...\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=None, ...),\n             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n                         'max_depth': [3, 6, 10], 'n_estimators': [100]},\n             scoring='roc_auc', verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True,\n                                     eval_metric='logloss', feature_types=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=N...\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=None, ...),\n             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n                         'max_depth': [3, 6, 10], 'n_estimators': [100]},\n             scoring='roc_auc', verbose=5) estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n# print best parameters\nbest_params = xgb_tuned.best_params_\nprint(best_params)\n\n# print best score\nbest_score = xgb_tuned.best_score_\nprint(best_score)\n\n{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n0.886970840782522\n\n\n\n# Train model on best parameters\nbest_xgb = XGBClassifier(**best_params, use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\n\n# Fit new model to training data\nbest_xgb.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=True, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n# Feature importance of xgb model\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\n\n# Assuming best_xgb is your trained XGBoost model\n\n# Plot feature importance\nplot_importance(best_xgb, importance_type='weight')\nplt.show()"
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#questions",
    "href": "projects/Pentathlon/pentathlon_nptb.html#questions",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Questions",
    "text": "Questions\n\n1. For each customer determine the message (i.e., endurance, strength, water, team, backcountry, racquet, or no-message) predicted to lead to the highest probability of purchase.\nTo determine the message that will lead to the highest probability of purchase for each customer, we can fit the model using only the data for each categorical level in our model and use the fitted model to create predictions for all of the data. Once we have predictions on all of the data for each message, we can select the message with the highest predicted value for the record.\n\n# Create a training copy of the data where 'training' equals 1\npentathlon_nptb_train = pentathlon_nptb[pentathlon_nptb['training'] == 1].copy()\n\n# Define the 7 different \"message\" values\nmessage_values = ['team', 'backcountry', 'endurance', 'water', 'racquet', 'strength', 'control']\n\nfor value in message_values:\n    # Filter the training set for the current message\n    current_train = pentathlon_nptb_train[pentathlon_nptb_train['message'] == value]\n    \n    # Create X_train and y_train for the current message\n    X_train = current_train[evar]\n    y_train = current_train[rvar]\n    \n    # Fit the model to the training data for the current message\n    best_xgb.fit(X_train, y_train)\n    \n    # Predict probabilities for the entire dataset (you may want to adjust this based on your requirements)\n    pentathlon_nptb[f'p_{value}_xgb'] = best_xgb.predict_proba(pentathlon_nptb[evar])[:, 1]\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1516221055.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\nrepl = {\n    \"p_control_xgb\": \"control\", \n    \"p_endurance_xgb\": \"endurance\", \n    \"p_backcountry_xgb\": \"backcountry\", \n    \"p_racquet_xgb\": \"racquet\", \n    \"p_strength_xgb\": \"strength\", \n    \"p_team_xgb\": \"team\", \n    \"p_water_xgb\": \"water\"}\n\n\npredictions_xgb = [\n    \"p_control_xgb\", \n    \"p_endurance_xgb\", \n    \"p_backcountry_xgb\", \n    \"p_racquet_xgb\", \n    \"p_strength_xgb\", \n    \"p_team_xgb\", \n    \"p_water_xgb\"]\n\n\npentathlon_nptb['message_xgb'] = (\n    pentathlon_nptb[predictions_xgb]\n    .idxmax(axis=1)\n    .map(repl)\n)\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/3215817490.py:1: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\npd.crosstab(index=pentathlon_nptb.message_xgb, columns=\"count\").apply(rsm.format_nr).sort_values(\"count\", ascending=True)\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nmessage_xgb\n\n\n\n\n\nendurance\n179,225\n\n\nbackcountry\n56,966\n\n\ncontrol\n58,865\n\n\nwater\n64,209\n\n\nracquet\n70,013\n\n\nteam\n71,928\n\n\nstrength\n98,794\n\n\n\n\n\n\n\n\n\n\n2. For each message, The percentage of customers for whom that message or no-message maximizes their probability of purchase:\nWe can see from the plot that endurance messages most often maximizes the cusomer‚Äôs probability of purchase. After enducance, strength messages are the second most reported value that maximizes purchase probabilities. Surprisingly, despite team, raquet, and water being the next best message groups, none of the remaining messages seem to provide a meaningful improvement over sending no message at all.\n\nmessage_percentages= pentathlon_nptb['message_xgb'].value_counts(normalize=True)\nmessage_percentages\n\nmessage_xgb\nendurance      0.298708\nstrength       0.164657\nteam           0.119880\nracquet        0.116688\nwater          0.107015\ncontrol        0.098108\nbackcountry    0.094943\nName: proportion, dtype: float64\n\n\n\n# Plotting\nplt.figure(figsize=(10, 6))\nmessage_percentages.plot(kind='bar')\nplt.ylabel('Percentage of Customers')\nplt.xlabel('Message Type')\nplt.title('Percentage of Customers for Each Message Value')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor each customer, determine the message (i.e., endurance, strength, water, team, backcountry, racquet, or no-message) predicted to lead to the highest expected profit (COGS is 60%).\nThe message returning the highest profit is Endurance\nTo predict order size we tuned a new xgb model to the data and regressed the likely order size for each record. We then used the predicted order size to calculate the expected profit for each record. We calculated the expected profit by multiplying the predicted order size by the probability of purchase and subtracting the cost of goods sold from the result.\n\nrvar_os = \"total_os\"\nevar = ['message',\n 'age',\n 'female',\n 'income',\n 'education',\n 'children',\n 'freq_endurance',\n 'freq_strength',\n 'freq_water',\n 'freq_team',\n 'freq_backcountry',\n 'freq_racquet']\n\n\n# Create X_train, X_test, y_train, y_test using \"training\" column\nX_train = pentathlon_nptb.loc[pentathlon_nptb['training'] == 1, evar]\nX_test = pentathlon_nptb.loc[pentathlon_nptb['training'] == 0, evar]\ny_train_os = pentathlon_nptb.loc[pentathlon_nptb['training'] == 1, 'total_os']\ny_test_os = pentathlon_nptb.loc[pentathlon_nptb['training'] == 0, 'total_os']\n\n\nfrom xgboost import XGBRegressor\n\n# Define the parameter grid to be used in GridSearchCV\nparam_grid = {\n    'learning_rate': [0.1, 0.01, 0.001],  \n    'n_estimators': [100],\n    'max_depth': [3, 6, 10]  # Depths from 3 to 10\n}\n\n# Initialize the XGBRegressor\nxgb_os = XGBRegressor(use_label_encoder=False, eval_metric='rmse', enable_categorical=True)\n\n# Initialize GridSearchCV with a scoring metric suitable for regression\nxgb_tuned_os = GridSearchCV(estimator=xgb_os, param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=5, verbose=5)\n\n# Assuming X_train and y_train are defined\n# Fit the model\nxgb_tuned_os.fit(X_train, y_train_os)\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=-12.399 total time=   0.4s\n[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=-12.532 total time=   0.4s\n[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=-12.659 total time=   0.4s\n[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=-12.504 total time=   0.4s\n[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=-11.961 total time=   0.5s\n[CV 1/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=-12.700 total time=   0.6s\n[CV 2/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=-12.639 total time=   0.7s\n[CV 3/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=-12.807 total time=   0.7s\n[CV 4/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=-12.660 total time=   0.7s\n[CV 5/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=-12.062 total time=   0.7s\n[CV 1/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=-13.273 total time=   1.1s\n[CV 2/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=-13.104 total time=   1.2s\n[CV 3/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=-13.254 total time=   1.1s\n[CV 4/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=-12.978 total time=   1.3s\n[CV 5/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=-12.550 total time=   1.2s\n[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=-12.550 total time=   0.4s\n[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=-12.691 total time=   0.4s\n[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=-12.816 total time=   0.4s\n[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=-12.654 total time=   0.4s\n[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=-12.135 total time=   0.4s\n[CV 1/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=-12.568 total time=   0.8s\n[CV 2/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=-12.663 total time=   0.7s\n[CV 3/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=-12.795 total time=   0.9s\n[CV 4/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=-12.622 total time=   0.8s\n[CV 5/5] END learning_rate=0.01, max_depth=6, n_estimators=100;, score=-12.095 total time=   0.8s\n[CV 1/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=-12.658 total time=   1.5s\n[CV 2/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=-12.755 total time=   1.5s\n[CV 3/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=-12.850 total time=   1.4s\n[CV 4/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=-12.661 total time=   1.4s\n[CV 5/5] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=-12.113 total time=   1.5s\n[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-12.971 total time=   0.4s\n[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-13.065 total time=   0.4s\n[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-13.206 total time=   0.4s\n[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-13.016 total time=   0.4s\n[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-12.524 total time=   0.5s\n[CV 1/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=-12.966 total time=   0.8s\n[CV 2/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=-13.058 total time=   0.8s\n[CV 3/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=-13.195 total time=   0.7s\n[CV 4/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=-13.005 total time=   0.7s\n[CV 5/5] END learning_rate=0.001, max_depth=6, n_estimators=100;, score=-12.511 total time=   0.8s\n[CV 1/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=-12.957 total time=   1.6s\n[CV 2/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=-13.059 total time=   1.6s\n[CV 3/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=-13.197 total time=   1.5s\n[CV 4/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=-13.001 total time=   1.8s\n[CV 5/5] END learning_rate=0.001, max_depth=10, n_estimators=100;, score=-12.500 total time=   1.6s\n\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=True, eval_metric='rmse',\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n                         'max_depth': [3, 6, 10], 'n_estimators': [100]},\n             scoring='neg_root_mean_squared_error', verbose=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=True, eval_metric='rmse',\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n                         'max_depth': [3, 6, 10], 'n_estimators': [100]},\n             scoring='neg_root_mean_squared_error', verbose=5) estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=True, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=True, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n\n\n# After fitting, get the best parameters and retrain the model\nbest_params_os = xgb_tuned.best_params_\nbest_xgb_os = XGBRegressor(**best_params_os, use_label_encoder=False, eval_metric=\"rmse\", enable_categorical=True)\n\n# Fit the new model to training data\nbest_xgb_os.fit(X_train, y_train_os)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=True, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†XGBRegressoriFittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=True, eval_metric='rmse', feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n\n\n# Create a training copy of the data where 'training' equals 1\npentathlon_nptb_train = pentathlon_nptb[pentathlon_nptb['training'] == 1].copy()\n\n# Define the 7 different \"message\" values\nmessage_values = ['team', 'backcountry', 'endurance', 'water', 'racquet', 'strength', 'control']\n\nfor value in message_values:\n    # Filter the training set for the current message\n    current_train = pentathlon_nptb_train[pentathlon_nptb_train['message'] == value]\n    \n    # Create X_train and y_train for the current message\n    X_train = current_train[evar]\n    y_train = current_train[rvar_os]\n    \n    # Fit the model to the training data for the current message\n    best_xgb_os.fit(X_train, y_train)\n    \n    # Predict orer size for the entire dataset\n    pentathlon_nptb[f'os_{value}_xgb'] = best_xgb_os.predict(pentathlon_nptb[evar]).round(0)\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/777574624.py:19: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\nvalues = ['team', 'backcountry', 'endurance', 'water', 'racquet', 'strength', 'control']\nProfit_margin = 1-0.6\n\n\nfor value in values:\n    pentathlon_nptb[f'ep_{value}_xgb'] = pentathlon_nptb[f'p_{value}_xgb'] * pentathlon_nptb[f'os_{value}_xgb'] * Profit_margin\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1144112266.py:6: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\nexpected_profit_xgb = [\n    \"ep_control_xgb\", \n    \"ep_endurance_xgb\", \n    \"ep_backcountry_xgb\", \n    \"ep_racquet_xgb\", \n    \"ep_strength_xgb\", \n    \"ep_team_xgb\", \n    \"ep_water_xgb\"]\n\n\nrepl = {\"ep_control_xgb\": \"control\", \"ep_endurance_xgb\": \"endurance\", \"ep_backcountry_xgb\": \"backcountry\", \"ep_racquet_xgb\": \"racquet\", \"ep_strength_xgb\": \"strength\", \"ep_team_xgb\": \"team\", \"ep_water_xgb\": \"water\"}\npentathlon_nptb[\"ep_message_xgb\"] = (\n    pentathlon_nptb[expected_profit_xgb]\n    .idxmax(axis=1)\n    .map(repl)\n)\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/3163695219.py:2: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\n# Define a function to look up the profit value based on ep_message_xgb\ndef get_ep(row):\n    # Mapping each ep_message_xgb value to the corresponding profit column\n    mapping = {\n        'control': 'ep_control_xgb',\n        'endurance': 'ep_endurance_xgb',\n        'backcountry': 'ep_backcountry_xgb',\n        'racquet': 'ep_racquet_xgb',\n        'strength': 'ep_strength_xgb',\n        'team': 'ep_team_xgb',\n        'water': 'ep_water_xgb',\n    }\n    \n    # Get the profit column name for the current row's message\n    profit_col = mapping.get(row['ep_message_xgb'])\n    \n    # Return the value from the corresponding profit column for this row\n    if profit_col:\n        return row[profit_col]\n    else:\n        return None\n\n# Apply the function to each row to create a new column with the corresponding profit value\npentathlon_nptb['ep_message_max_p'] = pentathlon_nptb.apply(get_ep, axis=1)\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1040504524.py:24: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n\nep_profit = pentathlon_nptb.groupby('ep_message_xgb')['ep_message_max_p'].sum().sort_values(ascending=False).round(2)\nep_profit\n\nep_message_xgb\nendurance      43977.37\nstrength       37358.12\nwater          30991.18\nbackcountry    26557.67\nteam           21474.36\nracquet        18135.07\ncontrol         9657.12\nName: ep_message_max_p, dtype: float64\n\n\n\n# Histogram of ep_profit\nplt.figure(figsize=(10, 6))\nep_profit.plot(kind='bar')\nplt.ylabel('Expected Profit by Message')\nplt.xlabel('Message Type')\nplt.title('Total Profit')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Report for each message, i.e., endurance, racket, etc., and no-message, the percentage of customers for whom that (no) message maximizes their expected profit.\n\nMessage_max = pentathlon_nptb.ep_message_xgb.value_counts(normalize=True)\nMessage_max\n\nep_message_xgb\ncontrol        0.576705\nstrength       0.091172\nendurance      0.082492\nwater          0.067795\nteam           0.066832\nbackcountry    0.065020\nracquet        0.049985\nName: proportion, dtype: float64\n\n\n\n# Plotting\nplt.figure(figsize=(10, 6))\nMessage_max.plot(kind='bar')\nplt.ylabel('Percentage of Customers')\nplt.xlabel('Message Type')\nplt.title('Percentage of Customers for Each Message Value')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5. The expected profit can we obtain, on average, per customer if we customize the message to each customer\n\navg_profit_pc = pentathlon_nptb[pentathlon_nptb.training == 0].groupby('ep_message_xgb')['ep_message_max_p'].mean().sort_values(ascending=False).round(2) * Profit_margin\navg_profit_pc\n\nep_message_xgb\nendurance      0.336\nstrength       0.284\nwater          0.280\nbackcountry    0.264\nracquet        0.232\nteam           0.196\ncontrol        0.008\nName: ep_message_max_p, dtype: float64\n\n\n\n# Histogram of ep_profit\nplt.figure(figsize=(10, 6))\navg_profit_pc.plot(kind='bar')\nplt.ylabel('Expected Profit Per Customer')\nplt.xlabel('Message Type')\nplt.title('Average Profit')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6. The expected profit per e-mailed customer if every customer receives the same message\n\nexpected_profit_xgb = [\n    \"ep_control_xgb\", \n    \"ep_endurance_xgb\", \n    \"ep_backcountry_xgb\", \n    \"ep_racquet_xgb\", \n    \"ep_strength_xgb\", \n    \"ep_team_xgb\", \n    \"ep_water_xgb\"]\n\n# Expected profit per customer for each message\nepp_same_message = pentathlon_nptb[pentathlon_nptb.training == 0][expected_profit_xgb].mean().sort_values(ascending=False) * Profit_margin\n\n\n# Histogram of ep_profit\nplt.figure(figsize=(10, 6))\nepp_same_message.plot(kind='bar')\nplt.ylabel('Expected Profit Per Customer')\nplt.xlabel('Message Type')\nplt.title('Average Profit')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7. The expected profit per e-mailed customer if every customer is assigned randomly to one of the messages or the no-message condition\n\npentathlon_nptb['ep_random_xgb'] = best_xgb.predict_proba(pentathlon_nptb[evar])[:, 1]\nxgb_random_ppc = pentathlon_nptb[pentathlon_nptb.training == 0]['ep_random_xgb'].mean() * Profit_margin\nxgb_random_ppc\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/3633768350.py:1: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n0.008396312594413757\n\n\n\nexpected_profit_xgb = [\n    \"ep_control_xgb\", \n    \"ep_endurance_xgb\", \n    \"ep_backcountry_xgb\", \n    \"ep_racquet_xgb\", \n    \"ep_strength_xgb\", \n    \"ep_team_xgb\", \n    \"ep_water_xgb\",\n    \"ep_random_xgb\"]\n\n# Expected profit per customer for each message\nepp_same_message = pentathlon_nptb[pentathlon_nptb.training == 0][expected_profit_xgb].mean().sort_values(ascending=False)\n\n\n# Histogram of ep_profit\nplt.figure(figsize=(10, 6))\nepp_same_message.plot(kind='bar')\nplt.ylabel('Expected Profit Per Customer')\nplt.xlabel('Message Type')\nplt.title('Average Profit')\nplt.xticks(rotation=45)  # Rotate labels to make them readable\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8. For the typical promotional e-mail blast to 5,000,000 customers, The improvement (in percent and in total Euros) could Pentathlon achieve by customizing the message (or no-message) to each customer:\n\npentathlon_nptb[\"ep_max_xgb\"] = pentathlon_nptb[expected_profit_xgb].max(axis=1)\npentathlon_nptb[\"ep_max_xgb\"].mean()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1479503728.py:1: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n\n0.31611574\n\n\n\n# Profit where each customer is assigned to the message with the highest expected profit\nprofit_xgb = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_max_xgb\"].agg(\"mean\") * 5000000\nprofit_xgb \n\n1511183.9771270752\n\n\n\n# Profit where each customer is sent to a random message\nrandom_profit_xgb = xgb_random_ppc * 5000000\nrandom_profit_xgb\n\n41981.56297206879\n\n\n\n# expected profit where no-message is sent (control)\npentathlon_nptb[\"ep_control_xgb\"]\n\n# expected profit per customer where no-message is sent (control)\ncontrol_profit_per_customer = pentathlon_nptb.loc[pentathlon_nptb.training == 0, \"ep_control_xgb\"].mean()\ncontrol_profit_per_customer\n\n0.078689866\n\n\n\n# Profit where no message is sent\ncontrol_profit_xgb = control_profit_per_customer * 5000000\ncontrol_profit_xgb\n\n393449.3288397789\n\n\n\nprofit_improvement_xgb = profit_xgb - control_profit_xgb\nprofit_improvement_xgb\n\n1117734.6482872963\n\n\n\nprofits_dct = {\n    \"Customize Message\": profit_xgb,\n    \"No Message Sent\": control_profit_nn,\n    \"Randomly Assign\": random_profit_xgb,\n    \n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(profits_dct.items()), columns=['Model', 'Profit'])\nplt.figure(figsize=(10, 5))  \n# Plot\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"Model\", y=\"Profit\", data=df, palette=\"viridis\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.Profit, f'${row.Profit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Profit by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45) \nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/1574157434.py:17: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect."
  },
  {
    "objectID": "projects/Pentathlon/pentathlon_nptb.html#improvement-profit-for-4-models",
    "href": "projects/Pentathlon/pentathlon_nptb.html#improvement-profit-for-4-models",
    "title": "Pentathlon: Next Product to Buy Models",
    "section": "Improvement Profit for 4 models",
    "text": "Improvement Profit for 4 models\n\nmod_perf = pd.DataFrame({\n    \"model\": [\"logit\", \"nn\", \"rf\", \"xgb\"],\n    \"profit\": [profit_improvement_lr, profit_improvement_nn, profit_improvement_rf, profit_improvement_xgb]\n})\nmod_perf\n\n\n\n\n\n\n\n\n\nmodel\nprofit\n\n\n\n\n0\nlogit\n1.238407e+06\n\n\n1\nnn\n1.262522e+06\n\n\n2\nrf\n2.510230e+06\n\n\n3\nxgb\n1.117735e+06\n\n\n\n\n\n\n\n\n\nprofits_dct = {\n    \"Logit\": profit_improvement_lr,\n    \"Neural Network\": profit_improvement_nn,\n    \"Random Forest\" : profit_improvement_rf,\n    \"XGBoost\": profit_improvement_xgb\n    \n}\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Convert dictionary to DataFrame\ndf = pd.DataFrame(list(profits_dct.items()), columns=['Model', 'Profit'])\nplt.figure(figsize=(10, 5))  \n# Plot\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"Model\", y=\"Profit\", data=df, palette=\"viridis\")\n\n# Annotations\nfor index, row in df.iterrows():\n    ax.text(index, row.Profit, f'${row.Profit:.2f}', ha='center')\n\n# Set labels and title\nax.set_xlabel(\"Model Type\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_ylabel(\"Profit\", fontdict={'family': 'serif', 'color': 'black', 'size': 12})\nax.set_title(\"Profit Improvement by Model\", fontdict={'family': 'serif', 'color': 'black', 'size': 15})\n\nplt.xticks(rotation=45) \nplt.show()\n\n/var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T/ipykernel_31928/2950013552.py:18: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nAccording to the chart, Random Forest outperforms the other models in terms of profit improvement but takes more than 2 hours to tune the model.\n\nWeaknesses of the Random Forest Model:\n\nComputational Complexity: Random Forest can be computationally intensive, especially as the number of trees and the depth of each tree increase. This is likely the cause of the long tunning time. The complexity grows with the size of the dataset, the number of features, and the model parameters.\nScalability: Because of the computational cost, scaling Random Forest to very large datasets or high-dimensional data can be challenging and might require significant computational resources.\nModel Interpretability: While not as complex as some other models like neural networks, Random Forests are still considered a ‚Äúblack box‚Äù compared to simpler models like Logistic Regression. This can make it difficult to understand the decision-making process of the model and to explain individual predictions.\nTuning Required: Random Forest models have several hyperparameters (like the number of trees, max depth, min samples split, etc.) that can greatly influence performance. Finding the optimal settings can be time-consuming and require a lot of trial and error or automated search techniques like grid search or random search, which can further increase the computational burden.\n\nSuggested Improvement:\n\nModel Simplification: We should simplying the model by reducing the number of trees or the depth of each tree, which might reduce training time at the cost of some accuracy.\nFeature Selection: Reducing the number of features through feature selection can significantly decrease training time and sometimes even improve model performance by eliminating noise.\nIncremental Learning: Some models allow for incremental learning, where the model is trained on chunks of the dataset sequentially, which can reduce memory consumption and potentially training time."
  },
  {
    "objectID": "projects/variable_importance/index.html#pearson-correlations",
    "href": "projects/variable_importance/index.html#pearson-correlations",
    "title": "Key Drivers Analysis",
    "section": "Pearson Correlations",
    "text": "Pearson Correlations\nThe formula for calculating the Pearson correlation coefficient ùëü is:\n\\[\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n\\]\nThe Pearson correlation coefficient (r) measures the strength and direction of the linear relationship between two variables. It is widely used in statistics and data analysis to assess whether and how strongly pairs of variables are related. The value of the Pearson correlation coefficient ranges from -1 to 1:\n1: A perfect positive linear relationship\n-1: A perfect negative linear relationship\n0: No linear relationship\n\n# Create correlation matrix\ncorrelation_matrix = X.corrwith(y)\n\n# Normalize correlations\ncorrelation_matrix /= correlation_matrix.sum()\n\n# Convert to percentage format\npearson_correlations = (correlation_matrix * 100).round(1).astype(str) + '%'\n\npearson_correlations_df = pd.DataFrame({\n    'Perception': pearson_correlations.index,\n    'Pearson_Correlation': pearson_correlations.values\n})\n\npearson_correlations_df\n\n\n\n\n\n\n\n\n\nPerception\nPearson_Correlation\n\n\n\n\n0\ntrust\n13.3%\n\n\n1\nbuild\n10.0%\n\n\n2\ndiffers\n9.6%\n\n\n3\neasy\n11.1%\n\n\n4\nappealing\n10.8%\n\n\n5\nrewarding\n10.1%\n\n\n6\npopular\n8.9%\n\n\n7\nservice\n13.0%\n\n\n8\nimpact\n13.2%"
  },
  {
    "objectID": "projects/variable_importance/index.html#standardized-regression-coefficients",
    "href": "projects/variable_importance/index.html#standardized-regression-coefficients",
    "title": "Key Drivers Analysis",
    "section": "Standardized Regression Coefficients",
    "text": "Standardized Regression Coefficients\nThe standardized regression coefficient for predictor \\({X_j}\\) in a multiple regression model is:\n\\[\n\\beta_j = b_j \\cdot \\frac{\\sigma_{X_j}}{\\sigma_Y}\n\\]\nwhere \\({b_j}\\) is the unstandardized regression coefficient, \\({\\sigma_{X_j}}\\) is the standard deviation of \\({X_j}\\) , and \\({\\sigma_Y}\\) is the standard deviation of the outcome variable \\({Y}\\).\nThese are the coefficients obtained from a regression model after standardizing the variables (i.e., converting them to a common scale). This allows for comparison of the relative importance of each predictor.\n\n# Fitting the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Obtaining standardized coefficients\ncoefficients = model.coef_\n\n# Standardized coefficients as a DataFrame for better manipulation\ncoefficients_df = pd.DataFrame({\n    'Perception': X_list,\n    'Standardized Coefficient': coefficients\n})\n#\n# Normalize the coefficients\ncoefficients_df['Standardized Coefficient'] /= coefficients_df['Standardized Coefficient'].sum()\n\n# Convert to percentage format\ncoefficients_df['Standardized Coefficient'] = (\n    coefficients_df['Standardized Coefficient'] * 100).round(1).astype(str) + '%'\n\ncoefficients_df\n\n\n\n\n\n\n\n\n\nPerception\nStandardized Coefficient\n\n\n\n\n0\ntrust\n24.8%\n\n\n1\nbuild\n4.3%\n\n\n2\ndiffers\n6.3%\n\n\n3\neasy\n4.7%\n\n\n4\nappealing\n7.3%\n\n\n5\nrewarding\n1.1%\n\n\n6\npopular\n3.6%\n\n\n7\nservice\n18.9%\n\n\n8\nimpact\n29.1%"
  },
  {
    "objectID": "projects/variable_importance/index.html#usefullness",
    "href": "projects/variable_importance/index.html#usefullness",
    "title": "Key Drivers Analysis",
    "section": "Usefullness",
    "text": "Usefullness\nUsefulness, as measured by \\(\\Delta R^2\\) is a valuable metric for assessing the importance of predictor variables in a regression model. It aids in model simplification, feature selection, interpretability, and resource allocation, ultimately leading to more effective and actionable insights from the analysis.\n\nfull_model_r2 = model.score(X, y)\n# Calculate the R^2 change for each variable\ndelta_r2 = {}\n\nfor variable in X_list:\n    # Exclude the current variable from the predictors\n    X_reduced = X.drop(columns=[variable])\n    \n    # Fit the reduced model\n    reduced_model = LinearRegression()\n    reduced_model.fit(X_reduced, y)\n    reduced_model_r2 = reduced_model.score(X_reduced, y)\n    \n    # Calculate the change in R^2\n    delta_r2[variable] = full_model_r2 - reduced_model_r2\n\n# Create a DataFrame to display the results\ndelta_r2_df = pd.DataFrame.from_dict(delta_r2, orient='index', columns=['Usefullness'])\n\n# Normalize\ndelta_r2_df['Usefullness'] /= delta_r2_df['Usefullness'] .sum()\n\ndelta_r2_df['Usefullness'] = (delta_r2_df['Usefullness'] * 100).round(1).astype(str) + '%'\n\ndelta_r2_df\n\n\n\n\n\n\n\n\n\nUsefullness\n\n\n\n\ntrust\n31.5%\n\n\nbuild\n1.0%\n\n\ndiffers\n2.1%\n\n\neasy\n1.1%\n\n\nappealing\n2.7%\n\n\nrewarding\n0.1%\n\n\npopular\n0.8%\n\n\nservice\n17.9%\n\n\nimpact\n42.8%"
  },
  {
    "objectID": "projects/variable_importance/index.html#shapley-values",
    "href": "projects/variable_importance/index.html#shapley-values",
    "title": "Key Drivers Analysis",
    "section": "Shapley values",
    "text": "Shapley values\nShapley values, originating from cooperative game theory, are a method to fairly distribute the ‚Äúpayout‚Äù among players based on their contribution to the total payout. In the context of machine learning and regression models, Shapley values are used to quantify the contribution of each feature to the prediction of a model.\nFor linear regression, Shapley values provide a way to understand the importance and contribution of each predictor variable to the prediction for each instance. This is done by considering all possible combinations of features and calculating the marginal contribution of each feature.\nThe Shapley value for a feature \\({j}\\) is given by: \\[\n\\phi_j = \\sum_{S \\subseteq N \\setminus \\{j\\}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!} \\left[ v(S \\cup \\{j\\}) - v(S) \\right]\n\\]\nWhere:\n\n\\({N}\\) is the set of all features\n\\({S}\\) is a subset of \\({N}\\) excluding \\({j}\\)\n\\({v(S)}\\) is the value function (e.g., model performance) for subset \\({S}\\).\n\n\nimport shap\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n\n# Calculate Shapley values using the shap library\nexplainer = shap.LinearExplainer(model, X)\nshap_values = explainer(X)\n\n# Get the mean absolute Shapley values for each feature\nshap_values_mean = pd.DataFrame(shap_values.values, columns=X.columns).abs().mean()\n\nshap_values_mean /= shap_values_mean.sum()\n\nshap_values_mean = (shap_values_mean * 100).round(1).astype(str) + '%'\n\nshap_values_df = pd.DataFrame({\n    'Perception': shap_values_mean.index,\n    'Shapley Values': shap_values_mean.values\n})\n\n\nshap_values_df\n\n\n\n\n\n\n\n\n\nPerception\nShapley Values\n\n\n\n\n0\ntrust\n26.7%\n\n\n1\nbuild\n4.5%\n\n\n2\ndiffers\n5.6%\n\n\n3\neasy\n5.1%\n\n\n4\nappealing\n7.6%\n\n\n5\nrewarding\n1.1%\n\n\n6\npopular\n3.8%\n\n\n7\nservice\n19.9%\n\n\n8\nimpact\n25.5%"
  },
  {
    "objectID": "projects/variable_importance/index.html#johnson-relative",
    "href": "projects/variable_importance/index.html#johnson-relative",
    "title": "Key Drivers Analysis",
    "section": "Johnson Relative",
    "text": "Johnson Relative\nDecomposes the model‚Äôs \\({R^2}\\) to assign weights to each predictor, reflecting their relative contribution to the explained variance in the outcome.\nThe relative weight for a predictor \\({X_j}\\) is calculated as: \\[\nRW_j = \\sum_{k=1}^{p} \\left( \\frac{\\lambda_{jk}^2 \\cdot \\text{Var}(Z_k)}{\\text{Var}(Y)} \\right)\n\\]\nwhere:\n\\({\\lambda_{jk}}\\) is the loading of predictor \\({j}\\) on the \\({k}\\)-th principal component\n\\({\\text{Var}(Z_k)}\\) is the variance of the \\({k}\\)-th principal component\n\\({\\text{Var}(Y)}\\) is the variance of the outcome variable.\n\n# Perform relative weights analysis\njohnsons_eps = relativeImp(data, outcomeName= 'satisfaction', driverNames = X_list)\n\n# Drop the 'rawRelaImpt' column\njohnsons_eps = johnsons_eps.drop('rawRelaImpt', axis=1)\n\n# Rename the columns\njohnsons_eps = johnsons_eps.rename(columns={'driver': 'Perception', 'normRelaImpt': \"Johnson's Epsilon\"})\n\n# Reformat the columns\njohnsons_eps[\"Johnson's Epsilon\"] = johnsons_eps[\"Johnson's Epsilon\"].round(1).astype('str') + \"%\"\n\n\njohnsons_eps\n\n\n\n\n\n\n\n\n\nPerception\nJohnson's Epsilon\n\n\n\n\n0\ntrust\n19.8%\n\n\n1\nbuild\n6.6%\n\n\n2\ndiffers\n7.0%\n\n\n3\neasy\n8.2%\n\n\n4\nappealing\n8.3%\n\n\n5\nrewarding\n6.0%\n\n\n6\npopular\n5.4%\n\n\n7\nservice\n16.6%\n\n\n8\nimpact\n22.0%"
  },
  {
    "objectID": "projects/variable_importance/trial.html",
    "href": "projects/variable_importance/trial.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. ‚ÄúUsefulness‚Äù is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost.\n\n1. Dataset Research\nBefore we go into any complex analysis, we first want to get familar with the data.\n\nimport pandas as pd\n\ndata = pd.read_csv('data_for_drivers_analysis.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\nprint(f'The number of rows in the dataset are {data.shape[0]}.')\nprint(f'The number of unique IDs in the dataset are {data.id.nunique()}.')\nprint(f'The number of unique brands in the dataset are {data.brand.nunique()}.')\n\nThe number of rows in the dataset are 2553.\nThe number of unique IDs in the dataset are 940.\nThe number of unique brands in the dataset are 10.\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nAfter looking at our data, we can see that satisfaction is the dependent variable, which is scored based on a range of 1-5. There are numerous different categories which affect the satisfaction score. All those categories are binary based on what the customer thinks about that card brand.\nNow we‚Äôre ready to get into some deeper analysis\n\n\n2. Pearson Correlations\nThe analysis we‚Äôll conduct is determining the Pearson Correlations for this dataset. That is, how each indepenent variable is correlated to satisfaction.\nWe‚Äôll be using the corr function in python to determine our correlations in respect to satisfaction.\n\n# Calculate pearson correlations with satisfaction being the y variable and the rest being the x variables\n\nsatisfaction = data.drop(['id', 'brand'], axis=1)\n\ntable = satisfaction.corr()['satisfaction'].sort_values(ascending=False)\n\n# drop satisfaction from table\n\ntable = table.drop('satisfaction')\n\ntable = pd.DataFrame(table)\n\ntable = table.rename(columns={'satisfaction': 'Pearson_Corr'})\n\ntable['Pearson_Corr_%'] = round(table['Pearson_Corr']/table['Pearson_Corr'].sum(), 3)*100\n\ntable = table.drop('Pearson_Corr', axis = 1)\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\n\n\n\n\ntrust\n13.3\n\n\nimpact\n13.2\n\n\nservice\n13.0\n\n\neasy\n11.1\n\n\nappealing\n10.8\n\n\nrewarding\n10.1\n\n\nbuild\n10.0\n\n\ndiffers\n9.6\n\n\npopular\n8.9\n\n\n\n\n\n\n\n\nFrom our calculations, we see that trust has the highest correlation to satisfaction, followed by impact and service. Popularity seems to be least correlated with satisfaction. This does not mean that trust is the most important variable when looking at satisfaction, however. It simply means it follows the most similar path (up or down) as satisfaction with respect to all the other variables.\n\n\n3. Polychoric Correlations\nPolychoric correlations are most notably used for ordinal variables, or variables that have a specific rank order, for instance military ranks (Captain, Colonel, General, etc).\nOur satisfaction variable is an ordinal variable which ranges from 1-5.\nFor this calculation, we used an R package within python. R has a much simpler package for conducting these calculations.\n\nimport rpy2.robjects as robjects\nfrom rpy2.robjects import pandas2ri\n\n# Activate the pandas2ri conversion\npandas2ri.activate()\n\n\n# Specify the columns of interest\ncolumns_of_interest = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Initialize a list to store the results\ncorrelation = []\n\n# Define the R code for calculating polychoric correlation\nr_code = \"\"\"\nlibrary(polycor)\npolychoric_corr &lt;- function(x, y) {\n    result &lt;- polychor(x, y)\n    return(result)\n}\n\"\"\"\n\n# Load the R code into the R environment\nrobjects.r(r_code)\n\n# Define the R code for installing the package\nr_install_code = \"\"\"\ninstall.packages('polycor')\n\"\"\"\n\n# Run the R code\nrobjects.r(r_install_code)\n\n# Get the polychoric_corr function\npolychoric_corr = robjects.globalenv['polychoric_corr']\n\n# Calculate polychoric correlations between 'satisfaction' and each specified column\nfor col in columns_of_interest:\n    r_corr = polychoric_corr(data['satisfaction'], data[col])\n    correlation.append(r_corr[0])\n\n# Convert correlations to a pandas DataFrame\ncorrelation_df = pd.DataFrame({\n    'Variable': columns_of_interest,\n    'Polychoric_Corr': correlation\n})\n\n# Calculate the sum of the polychoric correlations\nsum_polychoric_correlations = correlation_df['Polychoric_Corr'].sum()\n\n# Calculate the percentage of each polychoric correlation\ncorrelation_df['Polychoric_Corr_%'] = (correlation_df['Polychoric_Corr'] / sum_polychoric_correlations).round(3)*100\n\ncorrelation_df\n\nR[write to console]: Installing package into ‚Äò/Users/duyentran/Library/R/arm64/4.3/library‚Äô\n(as ‚Äòlib‚Äô is unspecified)\n\nR[write to console]: trying URL 'https://cloud.r-project.org/bin/macosx/big-sur-arm64/contrib/4.3/polycor_0.8-1.tgz'\n\nR[write to console]: Content type 'application/x-gzip'\nR[write to console]:  length 72889 bytes (71 KB)\n\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: =\nR[write to console]: \n\nR[write to console]: downloaded 71 KB\n\n\n\n\n\nThe downloaded binary packages are in\n    /var/folders/28/cfl1_cfs3bb536qkz8wkys_w0000gn/T//Rtmp7CywXB/downloaded_packages\n\n\n\n\n\n\n\n\n\n\nVariable\nPolychoric_Corr\nPolychoric_Corr_%\n\n\n\n\n0\ntrust\n0.325066\n12.9\n\n\n1\nbuild\n0.249185\n9.9\n\n\n2\ndiffers\n0.251008\n10.0\n\n\n3\neasy\n0.273529\n10.9\n\n\n4\nappealing\n0.266616\n10.6\n\n\n5\nrewarding\n0.255609\n10.1\n\n\n6\npopular\n0.223711\n8.9\n\n\n7\nservice\n0.327699\n13.0\n\n\n8\nimpact\n0.348443\n13.8\n\n\n\n\n\n\n\n\n\n# make variable column the index\n\ncorrelation_df.set_index('Variable', inplace=True)\n\n# merge correlation_df with correlations\n\ntable = table.merge(correlation_df, left_index=True, right_index=True).drop('Polychoric_Corr', axis=1)\n\n\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\n\n\n\n\ntrust\n13.3\n12.9\n\n\nimpact\n13.2\n13.8\n\n\nservice\n13.0\n13.0\n\n\neasy\n11.1\n10.9\n\n\nappealing\n10.8\n10.6\n\n\nrewarding\n10.1\n10.1\n\n\nbuild\n10.0\n9.9\n\n\ndiffers\n9.6\n10.0\n\n\npopular\n8.9\n8.9\n\n\n\n\n\n\n\n\nAs we can see from the data, the groups are very similar to Pearson‚Äôs Correlations, however they are in slightly different orders. The top three correlated variables from Pearson‚Äôs (Trust, Impact, Service) are still the top 3, however the order is Impact, Service, and Trust.\nAlso, in our bottom 3 variables (Build, Differs, and Popular) from Pearson‚Äôs, Polychoric varies slightly with Differs, Build and Popular being the order from most to least. Overall, both Pearson‚Äôs and Polychoric correlations are very similar.\n\n\n4. Standardized Multiple Regression Coefficients\nStandardized regression coefficients are used for a different purpose than our last two methods. Pearson‚Äôs and Polychoric used correlations, which do not necessarily relate to importance of each variable on the dependent variable. Standardized regression coefficients measure importance for each variable in a regression analysis by scaling all the independent variables so they are now all on equal scales. The scaled indepenent variables are then fit to a regression model where their betas or standardized coefficients are generated.\nFirst, we‚Äôll scale and fit our data to get our coefficients, then find their importance.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize the StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the data\n\nscaler.fit(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Transform the data\n\nscaled_data = scaler.transform(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Convert the scaled data to a DataFrame\n\nscaled_data = pd.DataFrame(scaled_data, columns=data.drop(['id', 'brand', 'satisfaction'], axis=1).columns)\n\n# Add the 'satisfaction' column to the scaled data\n\nscaled_data['satisfaction'] = data['satisfaction']\n\n# Initialize the LinearRegression model\n\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\n\nmodel.fit(scaled_data.drop('satisfaction', axis=1), scaled_data['satisfaction'])\n\ncoefficients = model.coef_\n\ntable['Std_Coefficients'] = coefficients\n\ntable['Std_Coef_%'] = (table['Std_Coefficients']/table['Std_Coefficients'].sum())*100\n\ntable = table.drop('Std_Coefficients', axis=1)\n\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n\n\nimpact\n13.2\n13.8\n4.363461\n\n\nservice\n13.0\n13.0\n6.081798\n\n\neasy\n11.1\n10.9\n4.798205\n\n\nappealing\n10.8\n10.6\n7.389479\n\n\nrewarding\n10.1\n10.1\n1.106526\n\n\nbuild\n10.0\n9.9\n3.628859\n\n\ndiffers\n9.6\n10.0\n19.304269\n\n\npopular\n8.9\n8.9\n28.047294\n\n\n\n\n\n\n\n\nThe ranked order for importance based on Standardized Regression Coefficients changes greatly when compared to our correlations. Popularity, which had the lowest correlational values for the first two calculations, now has the highest value. Trust is still at the top in second place, but Differs is now in the top 3 as well. Impact is also towards the bottom of the pack which was towards the top in correlations.\n\n\n5. Shapley Values for Linear Regression\nShapley Values, like the Standardized Regression Coefficients, also measure importance, but using a different method. They have been popularized in the use of machine learning, but here we will be using them with linear regression. Shapley values measure importance through the R^2 value, which measures how well the variance in the data is explained by the coefficients generated during a linear regression analysis.\nOnce the coefficients are attained through the regression analysis, they are then used to create as many unique combinations of dependent variables in our linear regression analysis. To go into this further, if our regression had 3 explanatory variables labeled a, b, and c, each one would have a coefficient. We would then measure the R2 value for every combination of variables to explain y.\nFor example, one combination would be y = beta-aa + beat-bb + beta-cc.¬†Another combination would be y = beta-aa + beta-bb. Another one would be y = beta-aa. We would calculate the R2 values for both of these models. We would subtract the R2 value from of the equation with variable c from the R2 value from the equaion without c to get the difference. We would do this for every combination of variable, then average the differences in R2 for every variable which we got.\nLets get into the code for calculating Shapley values in python.\nFirst, we‚Äôll make the linear regression model.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load dataset\n\nX = data[columns_of_interest]\ny = data['satisfaction']\n\n# Train a model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nNext, we‚Äôll generate the Shapley Values using the SHAP package on python\n\nimport shap\n\n# Initialize SHAP explainer\nexplainer = shap.LinearExplainer(model, X)\n\n# Calculate Shapley values\nshap_values = explainer.shap_values(X)\n\n# Plot the summary\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\n\n\n\n\n\n\n\n\nFinally, we‚Äôll put our Shapley Values into our table.\n\nshap_values = pd.DataFrame(shap_values, columns=columns_of_interest)\n\n# mean of absolute values of each column\n\nshap_values_avg = shap_values.abs().mean()\n\nshap_values_avg = pd.DataFrame(shap_values_avg)\n\nshap_values_avg = shap_values_avg.rename(columns={0: 'Shapley_Value'})\n\ntable = table.merge(shap_values_avg, left_index=True, right_index=True)\n\ntable['Shapley_Value_%'] = round(table['Shapley_Value'] / table['Shapley_Value'].sum(), 3)*100\n\ntable = table.drop('Shapley_Value', axis=1)\n\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n\n\n\n\n\n\n\n\nThe Shapley Values produced a slightly different order than our standard coefficients. Trust, impact, and service are the top 3 most important features. The remaiining order is much different than previous calculations, with appealing, differs, and easy the following 3, and build, popular, and rewarding following them.\n\n\n6. Johnson‚Äôs Epsilon\n\nfrom relativeImp import relativeImp\n\n\ny = 'satisfaction'\nX = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Perform relative weights analysis\nrel_Imp = relativeImp(data, outcomeName=y, driverNames=X)\n\nrel_Imp.set_index('driver', inplace=True)\n\ntable = table.merge(rel_Imp, left_index=True, right_index=True)\n\ntable = table.drop('rawRelaImpt', axis=1)\n\ntable = table.rename(columns = {'normRelaImpt':'Johnson_Ep_%'})\n\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n\n\n\n\n\n\n\n\n\n\n7. Mean Decrease in RF Gini Coefficient\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(data[columns_of_interest], data['satisfaction'])\n\ngini_importance = model.feature_importances_\n\n# Create a DataFrame for better readability\nfeature_importance_df = pd.DataFrame({\n    'Feature': columns_of_interest,\n    'Gini Importance': gini_importance\n})\n\n# Sort the features by importance\nfeature_importance_df = feature_importance_df.sort_values(by='Gini Importance', ascending=False)\n\nfeature_importance_df.set_index('Feature', inplace = True)\n\nfeature_importance_df['Gini Importance'] = feature_importance_df['Gini Importance']*100\n\ntable = table.merge(feature_importance_df, left_index=True, right_index=True)\n\ntable\n\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\nGini Importance\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n8.982532\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n9.270374\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n10.588220\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n11.249596\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n10.699951\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n11.851860\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n12.371855\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n11.466956\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n13.518656"
  },
  {
    "objectID": "projects/variable_importance/index.html#mean-decrease-in-gini-coefficient",
    "href": "projects/variable_importance/index.html#mean-decrease-in-gini-coefficient",
    "title": "Key Drivers Analysis",
    "section": "Mean Decrease in Gini Coefficient",
    "text": "Mean Decrease in Gini Coefficient\nIn Random Forests, the Gini importance (or Mean Decrease in Gini) is calculated based on the average decrease in impurity (Gini impurity) brought by each feature to the nodes in the trees.\nThe Mean Decrease in Gini for a feature \\({j}\\) is: \\[\n\\text{MDG}_j = \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\Delta Gini(t, j) \\right)\n\\]\nWhere:\n\n\\({T}\\) is the total number of trees\n\\({\\Delta Gini(t, j)}\\) is the decrease in Gini impurity for tree \\({t}\\) due to feature \\({j}\\).\n\n\nnp.random.seed(42)\n# Fit Random Forest model\nrf_model = RandomForestClassifier(n_estimators=50,max_depth=8)\nrf_model.fit(X, y)\n\n# Get Mean Decrease in Gini Coefficient (feature importances)\nrf_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n\n# rf_importances /= rf_importances.sum()\n\nrf_importances_df = pd.DataFrame({\n    'Perception': rf_importances.index,\n    'Mean Decrease in Gini Coefficient': rf_importances.values\n})\n\nrf_importances_df['Mean Decrease in Gini Coefficient'] = (\n    rf_importances_df['Mean Decrease in Gini Coefficient']* 100).round(1).astype(str) + '%'\n\n# Display the feature importances\nrf_importances_df\n\n\n\n\n\n\n\n\n\nPerception\nMean Decrease in Gini Coefficient\n\n\n\n\n0\ntrust\n10.5%\n\n\n1\nbuild\n11.4%\n\n\n2\ndiffers\n10.8%\n\n\n3\neasy\n10.6%\n\n\n4\nappealing\n11.6%\n\n\n5\nrewarding\n11.9%\n\n\n6\npopular\n12.1%\n\n\n7\nservice\n10.9%\n\n\n8\nimpact\n10.2%"
  },
  {
    "objectID": "projects/variable_importance/index.html#xgboost-feature-importance",
    "href": "projects/variable_importance/index.html#xgboost-feature-importance",
    "title": "Key Drivers Analysis",
    "section": "XGBoost Feature Importance",
    "text": "XGBoost Feature Importance\nXGBoost provides three types of feature importance: weight (frequency of a feature in trees), cover (average coverage of the feature), and gain (average gain brought by a feature to the branches it is on).\n\nWeight (frequency) \\[\n\\text{Weight}_j = \\sum_{t=1}^{T} \\sum_{n \\in t} I(n = j)\n\\]\n\nwhere \\({T}\\) is the total number of trees, \\({n}\\) are the nodes in tree \\({t}\\), and \\({I}\\) is an indicator function that is 1 if the node \\({n}\\) uses feature \\({j}\\), otherwise 0.\n\nCover (Average Cover)\n\n\\[\n\\text{Cover}_j = \\sum_{t=1}^{T} \\sum_{n \\in t} \\frac{I(n = j) \\cdot \\text{cover}(n)}{\\sum_{n' \\in t} \\text{cover}(n')}\n\\]\n\nGain (Average Gain)\n\n\\[\n\\text{Gain}_j = \\sum_{t=1}^{T} \\sum_{n \\in t} \\frac{I(n = j) \\cdot \\text{gain}(n)}{\\sum_{n' \\in t} \\text{gain}(n')}\n\\]\n\n# Shift the classes in the target variable to start from 0\ny_shifted = y - 1\n\n# Train the XGBoost model with the shifted target variable\nxgb_model = xgb.XGBClassifier()\nxgb_model.fit(X, y_shifted)\n\n# Get feature importances from the XGBoost model\nxgb_importances = pd.Series(xgb_model.feature_importances_, index=X.columns)\n\nxgb_importances_df = pd.DataFrame({\n    'Perception': xgb_importances.index,\n    'XGBoost': xgb_importances.values\n})\n\nxgb_importances_df['XGBoost'] = (\n    xgb_importances_df['XGBoost']* 100).round(1).astype(str) + '%'\n\n# Display the feature importances\nxgb_importances_df\n\n\n\n\n\n\n\n\n\nPerception\nXGBoost\n\n\n\n\n0\ntrust\n15.1%\n\n\n1\nbuild\n9.1%\n\n\n2\ndiffers\n11.0%\n\n\n3\neasy\n9.5%\n\n\n4\nappealing\n9.9%\n\n\n5\nrewarding\n8.8%\n\n\n6\npopular\n10.2%\n\n\n7\nservice\n10.7%\n\n\n8\nimpact\n15.8%"
  },
  {
    "objectID": "projects/variable_importance/index.html#permutation-importance",
    "href": "projects/variable_importance/index.html#permutation-importance",
    "title": "Key Drivers Analysis",
    "section": "Permutation Importance",
    "text": "Permutation Importance\n\nfrom sklearn.inspection import permutation_importance\n# Fitting the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Calculate permutation importance\nperm_importance = permutation_importance(model, X, y, n_repeats=30, random_state=0)\n\nperm_importance_mean = perm_importance.importances_mean\n\n# Convert to percentage and round for consistency\nperm_importance_pct = (perm_importance_mean / perm_importance_mean.sum() * 100).round(1)\n\nperm_importance_df = pd.DataFrame({\n    'Perception': X_list,\n    'Permutation Importance': perm_importance_pct\n})\n\nperm_importance_df['Permutation Importance'] = perm_importance_df['Permutation Importance'].astype(str) + '%'\n\nperm_importance_df\n\n\n\n\n\n\n\n\n\nPerception\nPermutation Importance\n\n\n\n\n0\ntrust\n33.1%\n\n\n1\nbuild\n0.8%\n\n\n2\ndiffers\n2.1%\n\n\n3\neasy\n1.1%\n\n\n4\nappealing\n3.1%\n\n\n5\nrewarding\n0.1%\n\n\n6\npopular\n0.6%\n\n\n7\nservice\n18.7%\n\n\n8\nimpact\n40.4%"
  },
  {
    "objectID": "projects/variable_importance/index.html#polychoric-correlations",
    "href": "projects/variable_importance/index.html#polychoric-correlations",
    "title": "Key Drivers Analysis",
    "section": "Polychoric Correlations",
    "text": "Polychoric Correlations\nPolychoric correlation is used to estimate the correlation between two theorized normally distributed continuous latent variables from two observed ordinal variables. This type of correlation is particularly useful when dealing with ordinal data, where the variables are measured on an ordinal scale.\nThe polychoric correlation () can be estimated using: \\[\n\\rho = \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{m} (o_{ij} - e_{ij})^2 / e_{ij}}{\\sqrt{\\sum_{i=1}^{n} (o_{i+} - e_{i+})^2 / e_{i+} \\sum_{j=1}^{m} (o_{+j} - e_{+j})^2 / e_{+j}}}\n\\]\nWhere:\n\n\\({o_{ij}}\\) is the observed frequency for the cell in row \\({i}\\) and column \\({j}\\).\n\\({e_{ij}}\\) is the expected frequency under the assumption of independence.\n\\({o_{i+}}\\) and \\({o_{+j}}\\) are the marginal totals for row \\({i}\\) and column \\({j}\\).\n\\({e_{i+}}\\) and \\({e_{+j}}\\) are the expected marginal totals for row \\({i}\\) and column \\({j}\\).\n\nTo enhance the process, we use polycor library from R then transform them to python code:\n\n# Activate the pandas2ri conversion\npandas2ri.activate()\n\n# Initialize a list to store the results\ncorrelation = []\n\n# Define the R code for the polychoric_corr function\nr_code = \"\"\"\nlibrary(polycor)\n\npolychoric_corr &lt;- function(x, y) {\n  result &lt;- polychor(x, y)\n  return(result)\n}\n\"\"\"\n\n# Run the R code\nrobjects.r(r_code)\n\n# Get the polychoric_corr function\npolychoric_corr = robjects.globalenv['polychoric_corr']\n\nfor col in X_list:\n    r_corr = polychoric_corr(y, data[col])\n    correlation.append(r_corr[0])\n\n# Normalize correlations\ntotal = sum(correlation)\ncorrelation = [value / total for value in correlation]\n\n# Convert correlations to a pandas DataFrame\npoly_corr_df = pd.DataFrame({\n    'Perception': X_list,\n    'Polychoric Correlation': correlation\n})\n\n# Reformat the column\npoly_corr_df['Polychoric Correlation'] = (poly_corr_df['Polychoric Correlation']* 100).round(1).astype(str) + '%'\n\n\npoly_corr_df\n\n\n\n\n\n\n\n\n\nPerception\nPolychoric Correlation\n\n\n\n\n0\ntrust\n12.9%\n\n\n1\nbuild\n9.9%\n\n\n2\ndiffers\n10.0%\n\n\n3\neasy\n10.9%\n\n\n4\nappealing\n10.6%\n\n\n5\nrewarding\n10.1%\n\n\n6\npopular\n8.9%\n\n\n7\nservice\n13.0%\n\n\n8\nimpact\n13.8%"
  },
  {
    "objectID": "projects/variable_importance/index.html#results",
    "href": "projects/variable_importance/index.html#results",
    "title": "Key Drivers Analysis",
    "section": "Results",
    "text": "Results\nNow combine all results together:\n\n# Create dataframe list\ndataframes = [\n    pearson_correlations_df, \n    poly_corr_df, coefficients_df, \n    shap_values_df, \n    johnsons_eps, \n    rf_importances_df, \n    xgb_importances_df] \n\n# Merge all dataframe\ntable = reduce(lambda left,right: pd.merge(left,right,on='Perception'), dataframes)\n\ntable\n\n\n\n\n\n\n\n\n\nPerception\nPearson_Correlation\nPolychoric Correlation\nStandardized Coefficient\nShapley Values\nJohnson's Epsilon\nMean Decrease in Gini Coefficient\nXGBoost\n\n\n\n\n0\ntrust\n13.3%\n12.9%\n24.8%\n26.7%\n19.8%\n10.5%\n15.1%\n\n\n1\nbuild\n10.0%\n9.9%\n4.3%\n4.5%\n6.6%\n11.4%\n9.1%\n\n\n2\ndiffers\n9.6%\n10.0%\n6.3%\n5.6%\n7.0%\n10.8%\n11.0%\n\n\n3\neasy\n11.1%\n10.9%\n4.7%\n5.1%\n8.2%\n10.6%\n9.5%\n\n\n4\nappealing\n10.8%\n10.6%\n7.3%\n7.6%\n8.3%\n11.6%\n9.9%\n\n\n5\nrewarding\n10.1%\n10.1%\n1.1%\n1.1%\n6.0%\n11.9%\n8.8%\n\n\n6\npopular\n8.9%\n8.9%\n3.6%\n3.8%\n5.4%\n12.1%\n10.2%\n\n\n7\nservice\n13.0%\n13.0%\n18.9%\n19.9%\n16.6%\n10.9%\n10.7%\n\n\n8\nimpact\n13.2%\n13.8%\n29.1%\n25.5%\n22.0%\n10.2%\n15.8%\n\n\n\n\n\n\n\n\nInterpretation by Perception\n1. Is offered by a brand I trust:\nHighest importance across multiple metrics: Pearson Correlation (13.3%), Standardized Coefficient (24.8%), Shapley Values (26.7%), Johnson‚Äôs Epsilon (19.8%), XGBoost (15.1%).\nConclusion: This is consistently identified as a key driver of satisfaction across different methods.\n2. Helps build credit quickly:\nModerate importance: Pearson Correlation (10.0%), Standardized Coefficient (4.3%), Shapley Values (4.5%), Johnson‚Äôs Epsilon (6.6%), Mean Decrease in Gini (11.4%).\nConclusion: Important, but less so than ‚Äútrust‚Äù.\n3. Is different from other cards:\nModerate to lower importance: Pearson Correlation (9.6%), Standardized Coefficient (6.3%), Shapley Values (5.6%), Johnson‚Äôs Epsilon (7.0%).\nConclusion: Also important, but not a top driver.\n4. Is easy to use:\nModerate importance: Pearson Correlation (11.1%), Standardized Coefficient (4.7%), Shapley Values (5.1%), Johnson‚Äôs Epsilon (8.2%).\nConclusion: Important, but consistently less so than ‚Äútrust‚Äù.\n5. Has appealing benefits or rewards:\nModerate importance: Pearson Correlation (10.8%), Standardized Coefficient (7.3%), Shapley Values (7.6%), Johnson‚Äôs Epsilon (8.3%).\nConclusion: Consistently important across all measures.\n6. Rewards me for responsible usage:\nLower importance: Pearson Correlation (10.1%), Standardized Coefficient (1.1%), Shapley Values (1.1%), Johnson‚Äôs Epsilon (6.0%).\nConclusion: Generally less important compared to other perceptions.\n7. Is used by a lot of people:\nLower to moderate importance: Pearson Correlation (8.9%), Standardized Coefficient (3.6%), Shapley Values (3.8%), Johnson‚Äôs Epsilon (5.4%), Mean Decrease in Gini (12.1%).\nConclusion: Less critical than other factors.\n8. Provides outstanding customer service:\nHigh importance: Pearson Correlation (13.0%), Standardized Coefficient (18.9%), Shapley Values (19.9%), Johnson‚Äôs Epsilon (16.6%).\nConclusion: Another key driver of satisfaction.\n9. Makes a difference in my life:\nVery high importance: Pearson Correlation (13.2%), Standardized Coefficient (29.1%), Shapley Values (25.5%), Johnson‚Äôs Epsilon (22.0%), XGBoost (15.8%).\nConclusion: Consistently one of the most important factors."
  },
  {
    "objectID": "projects/variable_importance/index.html#introduction",
    "href": "projects/variable_importance/index.html#introduction",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Key Driver Analysis (KDA) is a statistical technique used to determine the factors (or ‚Äúdrivers‚Äù) that most significantly impact a particular outcome or dependent variable. It is commonly used in fields like marketing, customer satisfaction, product development, and human resources to understand what influences key outcomes such as customer satisfaction, employee engagement, or product success.\nThis post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card. This involves calculating pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest."
  },
  {
    "objectID": "projects/variable_importance/index.html#data-overview",
    "href": "projects/variable_importance/index.html#data-overview",
    "title": "Key Drivers Analysis",
    "section": "Data Overview",
    "text": "Data Overview\nFirst of all, let‚Äôs have some data overview:\n\ndata = pd.read_csv('data_for_drivers_analysis.csv')\ndata\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2548\n10\n17800\n5\n1\n1\n0\n1\n0\n1\n1\n1\n1\n\n\n2549\n10\n17808\n3\n1\n0\n0\n1\n0\n1\n1\n1\n0\n\n\n2550\n10\n17893\n5\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2551\n10\n17984\n3\n1\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n2552\n10\n18073\n4\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n2553 rows √ó 12 columns\n\n\n\n\n\n# Calculate summary statistics for the dataset\nsummary_statistics = data.describe()\n\n# Display the summary statistics\nsummary_statistics\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nAs we can see, there are 2,553 rows of data that helps further analysis to answer the questions:\n\nIs offered by a brand I trust\nHelps build credit quickly\nIs different from other cards\nIs easy to use\nHas appealing benefits or rewards\nRewards me for responsible usage\nIs used by a lot of people\nProvides outstanding customer service\nMakes a difference in my life"
  },
  {
    "objectID": "projects/variable_importance/index.html#summary-of-metrics",
    "href": "projects/variable_importance/index.html#summary-of-metrics",
    "title": "Key Drivers Analysis",
    "section": "Summary of Metrics",
    "text": "Summary of Metrics\nTo implement measures of variable importance, interpreted as a key drivers analysis, the process will include metrics:\n\nPearson Correlation: Measures the linear relationship between each perception and satisfaction.\nPolychoric Correlation: Estimates the correlation between two theorized normally distributed continuous latent variables from observed ordinal variables.\nStandardized Coefficient: The coefficients from a linear regression model standardized to have unit variance.\nShapley Values: Measure the contribution of each feature to the model‚Äôs prediction averaged over all possible combinations of features.\nJohnson‚Äôs Epsilon: Reflects the relative importance of predictors adjusted for multicollinearity. = Mean Decrease in Gini Coefficient: Reflects the importance of each feature in reducing impurity in a Random Forest model.\nXGBoost: Feature importance from the XGBoost model, based on how useful each feature is in reducing the objective function‚Äôs error.\n\nFirst of all, let‚Äôs do some preparation for the model:\n\n# Assign X as individual variables in regression\nX_list = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\nX = data[X_list]\n\n# Assign y as dependent variables in regression\ny = data['satisfaction']"
  },
  {
    "objectID": "projects/variable_importance/index.html#summary",
    "href": "projects/variable_importance/index.html#summary",
    "title": "Key Drivers Analysis",
    "section": "Summary",
    "text": "Summary\nTop Drivers of Satisfaction\nTrust in the brand is the most consistently high-rated factor across multiple metrics, indicating that customer trust in the brand has a profound and consistent impact on overall satisfaction. This suggests that fostering and maintaining trust is crucial for any brand aiming to improve customer satisfaction. Outstanding customer service also shows high importance across all metrics, underlining that excellent customer service is vital for keeping customers satisfied. Additionally, the perception that the product or service makes a difference in customers‚Äô lives is consistently one of the top factors, showing very high importance across most metrics. This highlights that customers highly value how the product or service impacts their lives positively.\nModerate Drivers of Satisfaction\nAppealing benefits or rewards hold moderately high importance across various metrics, indicating that attractive benefits and rewards significantly contribute to customer satisfaction. Ease of use is another important factor, particularly in non-linear models, suggesting that while ease of use is significant, it is not the top factor. The perception that the product helps build credit quickly shows moderate importance, especially in tree-based models like Random Forest, making it an important but secondary factor. Differentiation from other cards is moderately important across most metrics, indicating that being different from other cards is valued by customers but is not the most critical factor.\nLower Drivers of Satisfaction\nRewards for responsible usage generally show lower importance in linear models but have some significance in non-linear models. This indicates that while these rewards are valued, they are less critical compared to other factors. The perception that the product is used by a lot of people generally has lower importance but shows higher importance in non-linear models like Random Forest and XGBoost. This suggests that while widespread usage is a less critical factor overall, it still holds some significance in certain contexts."
  },
  {
    "objectID": "projects/Debt_analyzing/index.html",
    "href": "projects/Debt_analyzing/index.html",
    "title": "Analyze International Debt Statistic",
    "section": "",
    "text": "It‚Äôs not that we humans only take debts to manage our necessities. A country may also take debt to manage its economy. For example, infrastructure spending is one costly ingredient required for a country‚Äôs citizens to lead comfortable lives. The World Bank is the organization that provides debt to countries.\nIn this project, I am going to analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD) owed by developing countries across several categories. I am going to find the answers to questions like:\nThe data used in this project is provided by The World Bank. It contains both national and regional debt statistics for several countries across the globe as recorded from 1970 to 2015."
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#the-world-banks-international-debt-data",
    "href": "projects/Debt_analyzing/index.html#the-world-banks-international-debt-data",
    "title": "Analyze International Debt Statistic",
    "section": "1. The World Bank‚Äôs international debt data",
    "text": "1. The World Bank‚Äôs international debt data\n\nIt‚Äôs not that we humans only take debts to manage our necessities. A country may also take debt to manage its economy. For example, infrastructure spending is one costly ingredient required for a country‚Äôs citizens to lead comfortable lives. The World Bank is the organization that provides debt to countries.\n\n\nIn this notebook, we are going to analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD) owed by developing countries across several categories. We are going to find the answers to questions like:\n\n\n\nWhat is the total amount of debt that is owed by the countries listed in the dataset?\n\n\nWhich country owns the maximum amount of debt and what does that amount look like?\n\n\nWhat is the average amount of debt owed by countries across different debt indicators?\n\n\n\n\n\n\nThe first line of code connects us to the international_debt database where the table international_debt is residing. Let‚Äôs first SELECT all of the columns from the international_debt table. Also, we‚Äôll limit the output to the first ten rows to keep the output clean.\n\n\nimport pandas as pd\nimport sqlite3\n\n# Load your CSV data into a DataFrame\ndf = pd.read_csv('international_debt.csv')\n\n# Create a SQLite database\nconn = sqlite3.connect('international_debt.db')\n\n# Write the data to a SQL table\ndf.to_sql('international_debt', conn, if_exists='replace', index=False)\n\n# Close the connection\nconn.close()\n\n# Load the SQL extension\n%load_ext sql\n\n# Connect to SQLite database\n%sql sqlite:///international_debt.db\n\n# Execute a SQL query\n%sql SELECT * FROM international_debt LIMIT 10;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ncountry_name\ncountry_code\nindicator_name\nindicator_code\ndebt\n\n\n\n\nAfghanistan\nAFG\nDisbursements on external debt, long-term (DIS, current US$)\nDT.DIS.DLXF.CD\n72894453.7\n\n\nAfghanistan\nAFG\nInterest payments on external debt, long-term (INT, current US$)\nDT.INT.DLXF.CD\n53239440.1\n\n\nAfghanistan\nAFG\nPPG, bilateral (AMT, current US$)\nDT.AMT.BLAT.CD\n61739336.9\n\n\nAfghanistan\nAFG\nPPG, bilateral (DIS, current US$)\nDT.DIS.BLAT.CD\n49114729.4\n\n\nAfghanistan\nAFG\nPPG, bilateral (INT, current US$)\nDT.INT.BLAT.CD\n39903620.1\n\n\nAfghanistan\nAFG\nPPG, multilateral (AMT, current US$)\nDT.AMT.MLAT.CD\n39107845.0\n\n\nAfghanistan\nAFG\nPPG, multilateral (DIS, current US$)\nDT.DIS.MLAT.CD\n23779724.3\n\n\nAfghanistan\nAFG\nPPG, multilateral (INT, current US$)\nDT.INT.MLAT.CD\n13335820.0\n\n\nAfghanistan\nAFG\nPPG, official creditors (AMT, current US$)\nDT.AMT.OFFT.CD\n100847181.9\n\n\nAfghanistan\nAFG\nPPG, official creditors (DIS, current US$)\nDT.DIS.OFFT.CD\n72894453.7"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#finding-the-number-of-distinct-countries",
    "href": "projects/Debt_analyzing/index.html#finding-the-number-of-distinct-countries",
    "title": "Analyze International Debt Statistic",
    "section": "2. Finding the number of distinct countries",
    "text": "2. Finding the number of distinct countries\n\nFrom the first ten rows, we can see the amount of debt owed by Afghanistan in the different debt indicators. But we do not know the number of different countries we have on the table. There are repetitions in the country names because a country is most likely to have debt in more than one debt indicator.\n\n\nWithout a count of unique countries, we will not be able to perform our statistical analyses holistically. In this section, we are going to extract the number of unique countries present in the table.\n\n\n%%sql\nSELECT \n    COUNT(DISTINCT(country_name)) AS total_distinct_countries\nFROM international_debt;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ntotal_distinct_countries\n\n\n\n\n124"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#finding-out-the-distinct-debt-indicators",
    "href": "projects/Debt_analyzing/index.html#finding-out-the-distinct-debt-indicators",
    "title": "Analyze International Debt Statistic",
    "section": "3. Finding out the distinct debt indicators",
    "text": "3. Finding out the distinct debt indicators\n\nWe can see there are a total of 124 countries present on the table. As we saw in the first section, there is a column called indicator_name that briefly specifies the purpose of taking the debt. Just beside that column, there is another column called indicator_code which symbolizes the category of these debts. Knowing about these various debt indicators will help us to understand the areas in which a country can possibly be indebted to.\n\n\n%%sql\nSELECT \n    DISTINCT indicator_code AS distinct_debt_indicators\nFROM international_debt\nORDER BY distinct_debt_indicators;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ndistinct_debt_indicators\n\n\n\n\nDT.AMT.BLAT.CD\n\n\nDT.AMT.DLXF.CD\n\n\nDT.AMT.DPNG.CD\n\n\nDT.AMT.MLAT.CD\n\n\nDT.AMT.OFFT.CD\n\n\nDT.AMT.PBND.CD\n\n\nDT.AMT.PCBK.CD\n\n\nDT.AMT.PROP.CD\n\n\nDT.AMT.PRVT.CD\n\n\nDT.DIS.BLAT.CD\n\n\nDT.DIS.DLXF.CD\n\n\nDT.DIS.MLAT.CD\n\n\nDT.DIS.OFFT.CD\n\n\nDT.DIS.PCBK.CD\n\n\nDT.DIS.PROP.CD\n\n\nDT.DIS.PRVT.CD\n\n\nDT.INT.BLAT.CD\n\n\nDT.INT.DLXF.CD\n\n\nDT.INT.DPNG.CD\n\n\nDT.INT.MLAT.CD\n\n\nDT.INT.OFFT.CD\n\n\nDT.INT.PBND.CD\n\n\nDT.INT.PCBK.CD\n\n\nDT.INT.PROP.CD\n\n\nDT.INT.PRVT.CD"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#totaling-the-amount-of-debt-owed-by-the-countries",
    "href": "projects/Debt_analyzing/index.html#totaling-the-amount-of-debt-owed-by-the-countries",
    "title": "Analyze International Debt Statistic",
    "section": "4. Totaling the amount of debt owed by the countries",
    "text": "4. Totaling the amount of debt owed by the countries\n\nAs mentioned earlier, the financial debt of a particular country represents its economic state. But if we were to project this on an overall global scale, how will we approach it?\n\n\nLet‚Äôs switch gears from the debt indicators now and find out the total amount of debt (in USD) that is owed by the different countries. This will give us a sense of how the overall economy of the entire world is holding up.\n\n\n%%sql\nSELECT \n    ROUND(SUM(debt)/1000000, 2) AS total_debt\nFROM international_debt;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ntotal_debt\n\n\n\n\n3079734.49"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#country-with-the-highest-debt",
    "href": "projects/Debt_analyzing/index.html#country-with-the-highest-debt",
    "title": "Analyze International Debt Statistic",
    "section": "5. Country with the highest debt",
    "text": "5. Country with the highest debt\n\n‚ÄúHuman beings cannot comprehend very large or very small numbers. It would be useful for us to acknowledge that fact.‚Äù - Daniel Kahneman. That is more than 3 million USD, an amount which is really hard for us to fathom.\n\n\nNow that we have the exact total of the amounts of debt owed by several countries, let‚Äôs now find out the country that owns the highest amount of debt along with the amount. Note that this debt is the sum of different debts owed by a country across several categories. This will help to understand more about the country in terms of its socio-economic scenarios. We can also find out the category in which the country owns its highest debt. But we will leave that for now.\n\n\n%%sql\nSELECT \n    country_name, SUM(debt) AS total_debt\nFROM international_debt\nGROUP BY country_name\nORDER BY total_debt desc\nLIMIT 1;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ncountry_name\ntotal_debt\n\n\n\n\nChina\n285793494734.2"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#average-amount-of-debt-across-indicators",
    "href": "projects/Debt_analyzing/index.html#average-amount-of-debt-across-indicators",
    "title": "Analyze International Debt Statistic",
    "section": "6. Average amount of debt across indicators",
    "text": "6. Average amount of debt across indicators\n\nSo, it was China. A more in-depth breakdown of China‚Äôs debts can be found here.\n\n\nWe now have a brief overview of the dataset and a few of its summary statistics. We already have an idea of the different debt indicators in which the countries owe their debts. We can dig even further to find out on an average how much debt a country owes? This will give us a better sense of the distribution of the amount of debt across different indicators.\n\n\n%%sql\nSELECT \n    indicator_code AS debt_indicator,\n    indicator_name,\n    AVG(debt) AS average_debt\nFROM international_debt\nGROUP BY debt_indicator, indicator_name\nORDER BY average_debt desc\nLIMIT 10;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ndebt_indicator\nindicator_name\naverage_debt\n\n\n\n\nDT.AMT.DLXF.CD\nPrincipal repayments on external debt, long-term (AMT, current US$)\n5904868401.499194\n\n\nDT.AMT.DPNG.CD\nPrincipal repayments on external debt, private nonguaranteed (PNG) (AMT, current US$)\n5161194333.812658\n\n\nDT.DIS.DLXF.CD\nDisbursements on external debt, long-term (DIS, current US$)\n2152041216.890244\n\n\nDT.DIS.OFFT.CD\nPPG, official creditors (DIS, current US$)\n1958983452.859836\n\n\nDT.AMT.PRVT.CD\nPPG, private creditors (AMT, current US$)\n1803694101.9632652\n\n\nDT.INT.DLXF.CD\nInterest payments on external debt, long-term (INT, current US$)\n1644024067.6508067\n\n\nDT.DIS.BLAT.CD\nPPG, bilateral (DIS, current US$)\n1223139290.39823\n\n\nDT.INT.DPNG.CD\nInterest payments on external debt, private nonguaranteed (PNG) (INT, current US$)\n1220410844.421519\n\n\nDT.AMT.OFFT.CD\nPPG, official creditors (AMT, current US$)\n1191187963.0830643\n\n\nDT.AMT.PBND.CD\nPPG, bonds (AMT, current US$)\n1082623947.6536233"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#the-highest-amount-of-principal-repayments",
    "href": "projects/Debt_analyzing/index.html#the-highest-amount-of-principal-repayments",
    "title": "Analyze International Debt Statistic",
    "section": "7. The highest amount of principal repayments",
    "text": "7. The highest amount of principal repayments\n\nWe can see that the indicator DT.AMT.DLXF.CD tops the chart of average debt. This category includes repayment of long term debts. Countries take on long-term debt to acquire immediate capital. More information about this category can be found here.\n\n\nAn interesting observation in the above finding is that there is a huge difference in the amounts of the indicators after the second one. This indicates that the first two indicators might be the most severe categories in which the countries owe their debts.\n\n\nWe can investigate this a bit more so as to find out which country owes the highest amount of debt in the category of long term debts (DT.AMT.DLXF.CD). Since not all the countries suffer from the same kind of economic disturbances, this finding will allow us to understand that particular country‚Äôs economic condition a bit more specifically.\n\n\n%%sql \nSELECT \n    country_name, \n    indicator_name\nFROM international_debt\nWHERE debt = (SELECT \n                MAX(debt)\n             FROM international_debt\n             WHERE indicator_code = 'DT.AMT.DLXF.CD');\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ncountry_name\nindicator_name\n\n\n\n\nChina\nPrincipal repayments on external debt, long-term (AMT, current US$)"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#the-most-common-debt-indicator",
    "href": "projects/Debt_analyzing/index.html#the-most-common-debt-indicator",
    "title": "Analyze International Debt Statistic",
    "section": "8. The most common debt indicator",
    "text": "8. The most common debt indicator\n\nChina has the highest amount of debt in the long-term debt (DT.AMT.DLXF.CD) category. This is verified by The World Bank. It is often a good idea to verify our analyses like this since it validates that our investigations are correct.\n\n\nWe saw that long-term debt is the topmost category when it comes to the average amount of debt. But is it the most common indicator in which the countries owe their debt? Let‚Äôs find that out.\n\n\n%%sql\nSELECT indicator_code, COUNT(indicator_code) AS indicator_count\nFROM international_debt\nGROUP BY indicator_code\nORDER BY indicator_count, indicator_code desc\nLIMIT 20;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\nindicator_code\nindicator_count\n\n\n\n\nDT.DIS.PROP.CD\n19\n\n\nDT.DIS.PCBK.CD\n51\n\n\nDT.DIS.PRVT.CD\n53\n\n\nDT.INT.PROP.CD\n54\n\n\nDT.AMT.PROP.CD\n54\n\n\nDT.INT.PBND.CD\n69\n\n\nDT.AMT.PBND.CD\n69\n\n\nDT.INT.DPNG.CD\n79\n\n\nDT.AMT.DPNG.CD\n79\n\n\nDT.INT.PCBK.CD\n84\n\n\nDT.AMT.PCBK.CD\n84\n\n\nDT.INT.PRVT.CD\n98\n\n\nDT.AMT.PRVT.CD\n98\n\n\nDT.DIS.BLAT.CD\n113\n\n\nDT.DIS.MLAT.CD\n120\n\n\nDT.INT.BLAT.CD\n122\n\n\nDT.DIS.OFFT.CD\n122\n\n\nDT.AMT.BLAT.CD\n122\n\n\nDT.DIS.DLXF.CD\n123\n\n\nDT.INT.OFFT.CD\n124"
  },
  {
    "objectID": "projects/Debt_analyzing/index.html#other-viable-debt-issues-and-conclusion",
    "href": "projects/Debt_analyzing/index.html#other-viable-debt-issues-and-conclusion",
    "title": "Analyze International Debt Statistic",
    "section": "9. Other viable debt issues and conclusion",
    "text": "9. Other viable debt issues and conclusion\n\nThere are a total of six debt indicators in which all the countries listed in our dataset have taken debt. The indicator DT.AMT.DLXF.CD is also there in the list. So, this gives us a clue that all these countries are suffering from a common economic issue. But that is not the end of the story, but just a part of the story.\n\n\nLet‚Äôs change tracks from debt_indicators now and focus on the amount of debt again. Let‚Äôs find out the maximum amount of debt that each country has. With this, we will be in a position to identify the other plausible economic issues a country might be going through.\n\n\nIn this notebook, we took a look at debt owed by countries across the globe. We extracted a few summary statistics from the data and unraveled some interesting facts and figures. We also validated our findings to make sure the investigations are correct.\n\n\n%%sql\nSELECT country_name, MAX(debt) AS maximum_debt\nFROM international_debt\nGROUP BY country_name\nORDER BY maximum_debt\nLIMIT 10;\n\n * sqlite:///international_debt.db\nDone.\n\n\n\n\n\n\ncountry_name\nmaximum_debt\n\n\n\n\nTonga\n10369910.6\n\n\nSao Tome and Principe\n10636848.5\n\n\nComoros\n13460035.4\n\n\nDominica\n21394158.2\n\n\nSamoa\n21715408.4\n\n\nTimor-Leste\n21799986.4\n\n\nTurkmenistan\n29132060.9\n\n\nSolomon Islands\n30749703.9\n\n\nEritrea\n31110077.8\n\n\nSomalia\n32985034.3"
  },
  {
    "objectID": "projects/optimizing_online/final.html",
    "href": "projects/optimizing_online/final.html",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "",
    "text": "Sports clothing is a booming sector!\nIn this notebook, I will use my SQL skills to analyze product data for an online sports retail company.\nI will work with numeric, string, and timestamp data on pricing and revenue, ratings, reviews, descriptions, and website traffic then use techniques such as aggregation, cleaning, labeling, Common Table Expressions, and correlation to produce recommendations on how the company can maximize revenue!"
  },
  {
    "objectID": "projects/optimizing_online/final.html#counting-missing-values",
    "href": "projects/optimizing_online/final.html#counting-missing-values",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "1. Counting missing values",
    "text": "1. Counting missing values\n\nSports clothing and athleisure attire is a huge industry, worth approximately $193 billion in 2021 with a strong growth forecast over the next decade!\n\n\nIn this notebook, we play the role of a product analyst for an online sports clothing company. The company is specifically interested in how it can improve revenue. We will dive into product data such as pricing, reviews, descriptions, and ratings, as well as revenue and website traffic, to produce recommendations for its marketing and sales teams.\n\n\nThe database provided to us, sports, contains five tables, with product_id being the primary key for all of them:\n\n\ninfo\n\n\n\n\n\ncolumn\n\n\ndata type\n\n\ndescription\n\n\n\n\n\n\nproduct_name\n\n\nvarchar\n\n\nName of the product\n\n\n\n\nproduct_id\n\n\nvarchar\n\n\nUnique ID for product\n\n\n\n\ndescription\n\n\nvarchar\n\n\nDescription of the product\n\n\n\n\n\nfinance\n\n\n\n\n\ncolumn\n\n\ndata type\n\n\ndescription\n\n\n\n\n\n\nproduct_id\n\n\nvarchar\n\n\nUnique ID for product\n\n\n\n\nlisting_price\n\n\nfloat\n\n\nListing price for product\n\n\n\n\nsale_price\n\n\nfloat\n\n\nPrice of the product when on sale\n\n\n\n\ndiscount\n\n\nfloat\n\n\nDiscount, as a decimal, applied to the sale price\n\n\n\n\nrevenue\n\n\nfloat\n\n\nAmount of revenue generated by each product, in US dollars\n\n\n\n\n\nreviews\n\n\n\n\n\ncolumn\n\n\ndata type\n\n\ndescription\n\n\n\n\n\n\nproduct_name\n\n\nvarchar\n\n\nName of the product\n\n\n\n\nproduct_id\n\n\nvarchar\n\n\nUnique ID for product\n\n\n\n\nrating\n\n\nfloat\n\n\nProduct rating, scored from 1.0 to 5.0\n\n\n\n\nreviews\n\n\nfloat\n\n\nNumber of reviews for the product\n\n\n\n\n\ntraffic\n\n\n\n\n\ncolumn\n\n\ndata type\n\n\ndescription\n\n\n\n\n\n\nproduct_id\n\n\nvarchar\n\n\nUnique ID for product\n\n\n\n\nlast_visited\n\n\ntimestamp\n\n\nDate and time the product was last viewed on the website\n\n\n\n\n\nbrands\n\n\n\n\n\ncolumn\n\n\ndata type\n\n\ndescription\n\n\n\n\n\n\nproduct_id\n\n\nvarchar\n\n\nUnique ID for product\n\n\n\n\nbrand\n\n\nvarchar\n\n\nBrand of the product\n\n\n\n\n\nWe will be dealing with missing data as well as numeric, string, and timestamp data types to draw insights about the products in the online store. Let‚Äôs start by finding out how complete the data is.\n\n\nimport pandas as pd\nimport sqlite3\n\n# Define the names of your CSV files and corresponding table names\nnames = ['finance', 'info', 'reviews', 'traffic', 'brands']\n\n# Create or open a single SQLite database\nconn = sqlite3.connect('consolidated.db')\n\n# Load each CSV data into a DataFrame and write it to a SQL table in the single database\nfor name in names:\n    df = pd.read_csv(f'{name}.csv')\n    df.to_sql(name, conn, if_exists='replace', index=False)\n\n# Close the database connection\nconn.close()\n\n\n%load_ext sql\n%sql sqlite:///consolidated.db\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n%%sql\nSELECT \n    COUNT(*) AS total_rows,\n    SUM(CASE WHEN i.description IS NULL THEN 0 ELSE 1 END) AS count_description,\n    SUM(CASE WHEN f.listing_price IS NULL THEN 0 ELSE 1 END) AS count_listing_price,\n    SUM(CASE WHEN t.last_visited IS NULL THEN 0 ELSE 1 END) AS count_last_visited\nFROM info i\nJOIN finance f ON i.product_id = f.product_id\nJOIN traffic t ON f.product_id = t.product_id;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\ntotal_rows\ncount_description\ncount_listing_price\ncount_last_visited\n\n\n\n\n3179\n3117\n3120\n2928"
  },
  {
    "objectID": "projects/optimizing_online/final.html#nike-vs-adidas-pricing",
    "href": "projects/optimizing_online/final.html#nike-vs-adidas-pricing",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "2. Nike vs Adidas pricing",
    "text": "2. Nike vs Adidas pricing\n\nWe can see the database contains 3,179 products in total. Of the columns we previewed, only one ‚Äî last_visited ‚Äî is missing more than five percent of its values. Now let‚Äôs turn our attention to pricing.\n\n\nHow do the price points of Nike and Adidas products differ? Answering this question can help us build a picture of the company‚Äôs stock range and customer market. We will run a query to produce a distribution of the listing_price and the count for each price, grouped by brand.\n\n\n%%sql\n\nSELECT \n    b.brand,\n    CAST(f.listing_price AS INTEGER),\n    COUNT(*)\nFROM finance AS f\nJOIN brands AS b\nON b.product_id = f.product_id\nWHERE f.listing_price &gt; 0\nAND (b.brand = 'Adidas' OR b.brand = 'Nike')\nGROUP BY b.brand, f.listing_price\nORDER BY f.listing_price DESC;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nbrand\nCAST(f.listing_price AS INTEGER)\nCOUNT(*)\n\n\n\n\nAdidas\n299\n2\n\n\nAdidas\n279\n4\n\n\nAdidas\n239\n5\n\n\nAdidas\n229\n8\n\n\nAdidas\n219\n11\n\n\nAdidas\n199\n8\n\n\nNike\n199\n1\n\n\nAdidas\n189\n7\n\n\nNike\n189\n2\n\n\nAdidas\n179\n34\n\n\nNike\n179\n4\n\n\nAdidas\n169\n27\n\n\nNike\n169\n14\n\n\nAdidas\n159\n28\n\n\nNike\n159\n31\n\n\nAdidas\n149\n41\n\n\nNike\n149\n6\n\n\nAdidas\n139\n36\n\n\nNike\n139\n12\n\n\nAdidas\n129\n96\n\n\nNike\n129\n12\n\n\nAdidas\n119\n115\n\n\nNike\n119\n16\n\n\nAdidas\n109\n91\n\n\nNike\n109\n17\n\n\nAdidas\n99\n72\n\n\nNike\n99\n14\n\n\nAdidas\n95\n2\n\n\nNike\n94\n1\n\n\nAdidas\n89\n89\n\n\nNike\n89\n13\n\n\nAdidas\n85\n7\n\n\nAdidas\n84\n1\n\n\nNike\n84\n5\n\n\nAdidas\n79\n322\n\n\nNike\n79\n16\n\n\nNike\n78\n1\n\n\nAdidas\n75\n149\n\n\nAdidas\n74\n1\n\n\nNike\n74\n7\n\n\nAdidas\n69\n87\n\n\nNike\n69\n4\n\n\nAdidas\n65\n102\n\n\nNike\n64\n1\n\n\nAdidas\n62\n1\n\n\nAdidas\n59\n211\n\n\nNike\n59\n2\n\n\nAdidas\n55\n174\n\n\nAdidas\n54\n2\n\n\nAdidas\n52\n43\n\n\nAdidas\n49\n183\n\n\nNike\n49\n5\n\n\nAdidas\n47\n42\n\n\nNike\n47\n1\n\n\nAdidas\n45\n163\n\n\nAdidas\n44\n1\n\n\nNike\n44\n3\n\n\nAdidas\n42\n51\n\n\nAdidas\n39\n81\n\n\nNike\n39\n1\n\n\nAdidas\n37\n24\n\n\nAdidas\n35\n25\n\n\nAdidas\n32\n24\n\n\nAdidas\n29\n37\n\n\nNike\n29\n2\n\n\nAdidas\n27\n38\n\n\nAdidas\n26\n18\n\n\nAdidas\n24\n28\n\n\nAdidas\n22\n1\n\n\nAdidas\n19\n8\n\n\nAdidas\n17\n4\n\n\nAdidas\n15\n4\n\n\nAdidas\n14\n27\n\n\nAdidas\n12\n27\n\n\nAdidas\n11\n1\n\n\nAdidas\n9\n11\n\n\nAdidas\n8\n1"
  },
  {
    "objectID": "projects/optimizing_online/final.html#labeling-price-ranges",
    "href": "projects/optimizing_online/final.html#labeling-price-ranges",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "3. Labeling price ranges",
    "text": "3. Labeling price ranges\n\nIt turns out there are 77 unique prices for the products in our database, which makes the output of our last query quite difficult to analyze.\n\n\nLet‚Äôs build on our previous query by assigning labels to different price ranges, grouping by brand and label. We will also include the total revenue for each price range and brand.\n\n\n%%sql\nSELECT \n    b.brand,\n    COUNT(*),\n    SUM(f.revenue) AS total_revenue,\n    CASE \n        WHEN f.listing_price &lt; 42 THEN 'Budget'\n        WHEN f.listing_price &gt;= 42 AND f.listing_price &lt; 74 THEN 'Average'\n        WHEN f.listing_price &gt;= 74 AND f.listing_price &lt; 129 THEN 'Expensive'\n        ELSE 'Elite'\n    END AS price_category\nFROM finance AS f\nJOIN brands AS b \nON b.product_id = f.product_id\nWHERE b.brand IS NOT NULL\nGROUP BY b.brand, price_category\nORDER BY total_revenue DESC;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nbrand\nCOUNT(*)\ntotal_revenue\nprice_category\n\n\n\n\nAdidas\n849\n4626980.07\nExpensive\n\n\nAdidas\n1060\n3233661.06\nAverage\n\n\nAdidas\n307\n3014316.83\nElite\n\n\nAdidas\n359\n651661.12\nBudget\n\n\nNike\n357\n595341.02\nBudget\n\n\nNike\n82\n128475.59\nElite\n\n\nNike\n90\n71843.15\nExpensive\n\n\nNike\n16\n6623.5\nAverage"
  },
  {
    "objectID": "projects/optimizing_online/final.html#average-discount-by-brand",
    "href": "projects/optimizing_online/final.html#average-discount-by-brand",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "4. Average discount by brand",
    "text": "4. Average discount by brand\n\nInterestingly, grouping products by brand and price range allows us to see that Adidas items generate more total revenue regardless of price category! Specifically, ‚ÄúElite‚Äù Adidas products priced $129 or more typically generate the highest revenue, so the company can potentially increase revenue by shifting their stock to have a larger proportion of these products!\n\n\nNote we have been looking at listing_price so far. The listing_price may not be the price that the product is ultimately sold for. To understand revenue better, let‚Äôs take a look at the discount, which is the percent reduction in the listing_price when the product is actually sold. We would like to know whether there is a difference in the amount of discount offered between brands, as this could be influencing revenue.\n\n\n%%sql\nSELECT \n    b.brand,\n    AVG(f.discount) * 100 AS average_discount\nFROM finance AS f\nINNER JOIN brands AS b \nON b.product_id = f.product_id\nGROUP BY brand\nHAVING brand IS NOT NULL;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nbrand\naverage_discount\n\n\n\n\nAdidas\n33.45242718446602\n\n\nNike\n0.0"
  },
  {
    "objectID": "projects/optimizing_online/final.html#correlation-between-revenue-and-reviews",
    "href": "projects/optimizing_online/final.html#correlation-between-revenue-and-reviews",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "5. Correlation between revenue and reviews",
    "text": "5. Correlation between revenue and reviews\n\nStrangely, no discount is offered on Nike products! In comparison, not only do Adidas products generate the most revenue, but these products are also heavily discounted!\n\n\nTo improve revenue further, the company could try to reduce the amount of discount offered on Adidas products, and monitor sales volume to see if it remains stable. Alternatively, it could try offering a small discount on Nike products. This would reduce average revenue for these products, but may increase revenue overall if there is an increase in the volume of Nike products sold.\n\n\nNow explore whether relationships exist between the columns in our database. We will check the strength and direction of a correlation between revenue and reviews.\n\n\n# Connect to the SQLite database\nconn = sqlite3.connect('consolidated.db')\n\n# Fetch data\nquery = \"\"\"\nSELECT r.reviews, f.revenue\nFROM finance AS f\nJOIN reviews AS r ON r.product_id = f.product_id\n\"\"\"\ndf = pd.read_sql(query, conn)\n\n# Calculate correlation using pandas\ncorrelation = df['reviews'].corr(df['revenue'])\nprint(\"Correlation between reviews and revenue:\", correlation)\n\n# Close the database connection\nconn.close()\n\nCorrelation between reviews and revenue: 0.6518512283481297"
  },
  {
    "objectID": "projects/optimizing_online/final.html#ratings-and-reviews-by-product-description-length",
    "href": "projects/optimizing_online/final.html#ratings-and-reviews-by-product-description-length",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "6. Ratings and reviews by product description length",
    "text": "6. Ratings and reviews by product description length\n\nInterestingly, there is a strong positive correlation between revenue and reviews. This means, potentially, if we can get more reviews on the company‚Äôs website, it may increase sales of those items with a larger number of reviews.\n\n\nPerhaps the length of a product‚Äôs description might influence a product‚Äôs rating and reviews ‚Äî if so, the company can produce content guidelines for listing products on their website and test if this influences revenue. Let‚Äôs check this out!\n\n\n%%sql\nSELECT\n    (LENGTH(i.description) / 100) * 100 AS description_length,  -- Truncate to nearest 100\n    ROUND(AVG(r.rating), 2) AS average_rating  -- Calculate average and round\nFROM reviews r\nJOIN info i ON r.product_id = i.product_id\nWHERE i.description IS NOT NULL\nGROUP BY description_length\nORDER BY description_length;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\ndescription_length\naverage_rating\n\n\n\n\n0\n1.87\n\n\n100\n3.21\n\n\n200\n3.27\n\n\n300\n3.29\n\n\n400\n3.32\n\n\n500\n3.12\n\n\n600\n3.65"
  },
  {
    "objectID": "projects/optimizing_online/final.html#reviews-by-month-and-brand",
    "href": "projects/optimizing_online/final.html#reviews-by-month-and-brand",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "7. Reviews by month and brand",
    "text": "7. Reviews by month and brand\n\nUnfortunately, there doesn‚Äôt appear to be a clear pattern between the length of a product‚Äôs description and its rating.\n\n\nAs we know a correlation exists between reviews and revenue, one approach the company could take is to run experiments with different sales processes encouraging more reviews from customers about their purchases, such as by offering a small discount on future purchases.\n\n\nLet‚Äôs take a look at the volume of reviews by month to see if there are any trends or gaps we can look to exploit.\n\n\n%%sql\n\nSELECT \n    b.brand,\n    strftime('%m', t.last_visited) AS month,  -- Extracts the month as 'MM'\n    COUNT(r.product_id) AS num_reviews\nFROM reviews AS r\nJOIN traffic AS t ON r.product_id = t.product_id\nJOIN brands AS b ON r.product_id = b.product_id\nWHERE b.brand IS NOT NULL AND t.last_visited IS NOT NULL\nGROUP BY b.brand, strftime('%m', t.last_visited)\nORDER BY b.brand, month;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nbrand\nmonth\nnum_reviews\n\n\n\n\nAdidas\n01\n253\n\n\nAdidas\n02\n272\n\n\nAdidas\n03\n269\n\n\nAdidas\n04\n180\n\n\nAdidas\n05\n172\n\n\nAdidas\n06\n159\n\n\nAdidas\n07\n170\n\n\nAdidas\n08\n189\n\n\nAdidas\n09\n181\n\n\nAdidas\n10\n192\n\n\nAdidas\n11\n150\n\n\nAdidas\n12\n190\n\n\nNike\n01\n52\n\n\nNike\n02\n52\n\n\nNike\n03\n55\n\n\nNike\n04\n42\n\n\nNike\n05\n41\n\n\nNike\n06\n43\n\n\nNike\n07\n37\n\n\nNike\n08\n29\n\n\nNike\n09\n28\n\n\nNike\n10\n47\n\n\nNike\n11\n38\n\n\nNike\n12\n35"
  },
  {
    "objectID": "projects/optimizing_online/final.html#footwear-product-performance",
    "href": "projects/optimizing_online/final.html#footwear-product-performance",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "8. Footwear product performance",
    "text": "8. Footwear product performance\n\nLooks like product reviews are highest in the first quarter of the calendar year, so there is scope to run experiments aiming to increase the volume of reviews in the other nine months!\n\n\nSo far, we have been primarily analyzing Adidas vs Nike products. Now, let‚Äôs switch our attention to the type of products being sold. As there are no labels for product type, we will create a Common Table Expression (CTE) that filters description for keywords, then use the results to find out how much of the company‚Äôs stock consists of footwear products and the median revenue generated by these items.\n\n\n%%sql\nWITH footwear AS (\n    SELECT \n        i.description, \n        f.revenue\n    FROM info AS i\n    INNER JOIN finance AS f ON i.product_id = f.product_id\n    WHERE (LOWER(i.description) LIKE '%shoe%' OR \n           LOWER(i.description) LIKE '%trainer%' OR \n           LOWER(i.description) LIKE '%foot%') \n          AND i.description IS NOT NULL\n)\n\nSELECT\n    COUNT(*) AS num_footwear_products,\n    (SELECT f.revenue\n     FROM footwear f\n     ORDER BY f.revenue\n     LIMIT 1\n     OFFSET (SELECT (COUNT(*) - 1) / 2 FROM footwear)) AS median_footwear_revenue\nFROM footwear;\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nnum_footwear_products\nmedian_footwear_revenue\n\n\n\n\n2700\n3118.36"
  },
  {
    "objectID": "projects/optimizing_online/final.html#clothing-product-performance",
    "href": "projects/optimizing_online/final.html#clothing-product-performance",
    "title": "Optimizing Online Sport Retail Revenue",
    "section": "9. Clothing product performance",
    "text": "9. Clothing product performance\n\nRecall from the first task that we found there are 3,117 products without missing values for description. Of those, 2,700 are footwear products, which accounts for around 85% of the company‚Äôs stock. They also generate a median revenue of over $3000 dollars!\n\n\nThis is interesting, but we have no point of reference for whether footwear‚Äôs median_revenue is good or bad compared to other products. So, for our final task, let‚Äôs examine how this differs to clothing products. We will re-use footwear, adding a filter afterward to count the number of products and median_revenue of products that are not in footwear.\n\n\n%%sql\nWITH footwear AS (\n    SELECT \n        i.product_id\n    FROM info AS i\n    WHERE i.description IS NOT NULL\n      AND (LOWER(i.description) LIKE '%shoe%' OR \n           LOWER(i.description) LIKE '%trainer%' OR \n           LOWER(i.description) LIKE '%foot%')\n)\n\nSELECT \n    COUNT(*) AS num_clothing_products,\n    (SELECT f.revenue\n     FROM finance AS f\n     JOIN info AS i ON i.product_id = f.product_id\n     WHERE i.product_id NOT IN (SELECT product_id FROM footwear)\n     ORDER BY f.revenue\n     LIMIT 1\n     OFFSET (SELECT COUNT(*) / 2 FROM finance AS f\n             JOIN info AS i ON i.product_id = f.product_id\n             WHERE i.product_id NOT IN (SELECT product_id FROM footwear))) AS median_clothing_revenue\nFROM info AS i\nWHERE i.product_id NOT IN (SELECT product_id FROM footwear);\n\n * sqlite:///consolidated.db\nDone.\n\n\n\n\n\n\nnum_clothing_products\nmedian_clothing_revenue\n\n\n\n\n479\n388.37"
  },
  {
    "objectID": "projects/cc_approval/index.html",
    "href": "projects/cc_approval/index.html",
    "title": "Credit card applications",
    "section": "",
    "text": "Commercial banks receive a lot of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual‚Äôs credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like the real banks do.\nWe‚Äôll use the Credit Card Approval dataset from the UCI Machine Learning Repository. The structure of this notebook is as follows:\nFirst, loading and viewing the dataset. We find that since this data is confidential, the contributor of the dataset has anonymized the feature names.\n# Import pandas\nimport pandas as pd\n\n# Load dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header = None)\n\n# Inspect data\nprint(cc_apps.head())\n\n  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
  },
  {
    "objectID": "projects/cc_approval/index.html#inspecting-the-applications",
    "href": "projects/cc_approval/index.html#inspecting-the-applications",
    "title": "Credit card applications",
    "section": "Inspecting the applications",
    "text": "Inspecting the applications\n\nThe output may appear a bit confusing at its first sight, but let‚Äôs try to figure out the most important features of a credit card application. The features of this dataset have been anonymized to protect the privacy, but this blog gives us a pretty good overview of the probable features. The probable features in a typical credit card application are Gender, Age, Debt, Married, BankCustomer, EducationLevel, Ethnicity, YearsEmployed, PriorDefault, Employed, CreditScore, DriversLicense, Citizen, ZipCode, Income and finally the ApprovalStatus. This gives us a pretty good starting point, and we can map these features with respect to the columns in the output.\n\n\nAs we can see from our first glance at the data, the dataset has a mixture of numerical and non-numerical features. This can be fixed with some preprocessing, but before we do that, let‚Äôs learn about the dataset a bit more to see if there are other dataset issues that need to be fixed.\n\n\n# Print summary statistics\ncc_apps_description = cc_apps.describe()\nprint(cc_apps_description)\n\nprint('\\n')\n\n# Print DataFrame information\ncc_apps_info = cc_apps.info()\nprint(cc_apps_info)\n\nprint('\\n')\n\n# Inspect missing values in the dataset\nprint(cc_apps.tail(17))\n\n               2           7          10             14\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 690 entries, 0 to 689\nData columns (total 16 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    object \n 13  13      690 non-null    object \n 14  14      690 non-null    int64  \n 15  15      690 non-null    object \ndtypes: float64(2), int64(2), object(12)\nmemory usage: 86.4+ KB\nNone\n\n\n    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -"
  },
  {
    "objectID": "projects/cc_approval/index.html#splitting-the-dataset-into-train-and-test-sets",
    "href": "projects/cc_approval/index.html#splitting-the-dataset-into-train-and-test-sets",
    "title": "Credit card applications",
    "section": "Splitting the dataset into train and test sets",
    "text": "Splitting the dataset into train and test sets\n\nNow, we will split our data into train set and test set to prepare our data for two different phases of machine learning modeling: training and testing. Ideally, no information from the test data should be used to preprocess the training data or should be used to direct the training process of a machine learning model. Hence, we first split the data and then preprocess it.\n\n\nAlso, features like DriversLicense and ZipCode are not as important as the other features in the dataset for predicting credit card approvals. To get a better sense, we can measure their statistical correlation to the labels of the dataset. But this is out of scope for this project. We should drop them to design our machine learning model with the best set of features. In Data Science literature, this is often referred to as feature selection.\n\n\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Drop the features 11 and 13\ncc_apps = cc_apps.drop([11, 13], axis=1)\n\n# Split into train and test sets\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)"
  },
  {
    "objectID": "projects/cc_approval/index.html#handling-the-missing-values-part-i",
    "href": "projects/cc_approval/index.html#handling-the-missing-values-part-i",
    "title": "Credit card applications",
    "section": "Handling the missing values (part i)",
    "text": "Handling the missing values (part i)\n\nNow we‚Äôve split our data, we can handle some of the issues we identified when inspecting the DataFrame, including:\n\n\n\nOur dataset contains both numeric and non-numeric data (specifically data that are of float64, int64 and object types). Specifically, the features 2, 7, 10 and 14 contain numeric values (of types float64, float64, int64 and int64 respectively) and all the other features contain non-numeric values.\n\n\nThe dataset also contains values from several ranges. Some features have a value range of 0 - 28, some have a range of 2 - 67, and some have a range of 1017 - 100000. Apart from these, we can get useful statistical information (like mean, max, and min) about the features that have numerical values.\n\n\nFinally, the dataset has missing values, which we‚Äôll take care of in this task. The missing values in the dataset are labeled with ‚Äò?‚Äô, which can be seen in the last cell‚Äôs output of the second task.\n\n\n\nNow, let‚Äôs temporarily replace these missing value question marks with NaN.\n\n\n# Import numpy\nimport numpy as np\n\n# Replace the '?'s with NaN in the train and test sets\ncc_apps_train = cc_apps_train.replace('?', np.NaN)\ncc_apps_test = cc_apps_test.replace('?', np.NaN)"
  },
  {
    "objectID": "projects/cc_approval/index.html#handling-the-missing-values-part-ii",
    "href": "projects/cc_approval/index.html#handling-the-missing-values-part-ii",
    "title": "Credit card applications",
    "section": "Handling the missing values (part ii)",
    "text": "Handling the missing values (part ii)\n\nWe replaced all the question marks with NaNs. This is going to help us in the next missing value treatment that we are going to perform.\n\n\nAn important question that gets raised here is why are we giving so much importance to missing values? Can‚Äôt they be just ignored? Ignoring missing values can affect the performance of a machine learning model heavily. While ignoring the missing values our machine learning model may miss out on information about the dataset that may be useful for its training. Then, there are many models which cannot handle missing values implicitly such as Linear Discriminant Analysis (LDA).\n\n\nSo, to avoid this problem, we are going to impute the missing values with a strategy called mean imputation.\n\n\n# Impute the missing values with mean imputation\n# Identify numeric columns in the DataFrame\nnumeric_columns = cc_apps_train.select_dtypes(include=[np.number]).columns\n\n# Impute the missing values with mean imputation for numeric columns only\ncc_apps_train[numeric_columns] = cc_apps_train[numeric_columns].fillna(cc_apps_train[numeric_columns].mean())\ncc_apps_test[numeric_columns] = cc_apps_test[numeric_columns].fillna(cc_apps_test[numeric_columns].mean())\n\n# Count the number of NaNs in the datasets and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())\n\n0     8\n1     5\n2     0\n3     6\n4     6\n5     7\n6     7\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64\n0     4\n1     7\n2     0\n3     0\n4     0\n5     2\n6     2\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64"
  },
  {
    "objectID": "projects/cc_approval/index.html#handling-the-missing-values-part-iii",
    "href": "projects/cc_approval/index.html#handling-the-missing-values-part-iii",
    "title": "Credit card applications",
    "section": "Handling the missing values (part iii)",
    "text": "Handling the missing values (part iii)\n\nWe have successfully taken care of the missing values present in the numeric columns. There are still some missing values to be imputed for columns 0, 1, 3, 4, 5, 6 and 13. All of these columns contain non-numeric data and this is why the mean imputation strategy would not work here. This needs a different treatment.\n\n\nWe are going to impute these missing values with the most frequent values as present in the respective columns. This is good practice when it comes to imputing missing values for categorical data in general.\n\n\n# Iterate over each column of cc_apps_train\nfor col in cc_apps_train.columns:\n    # Check if the column is of object type\n    if cc_apps_train[col].dtypes == 'object':\n        # Impute with the most frequent value\n        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n\n# Count the number of NaNs in the dataset and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())\n\n0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64\n0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64"
  },
  {
    "objectID": "projects/cc_approval/index.html#preprocessing-the-data-part-i",
    "href": "projects/cc_approval/index.html#preprocessing-the-data-part-i",
    "title": "Credit card applications",
    "section": "Preprocessing the data (part i)",
    "text": "Preprocessing the data (part i)\n\nThe missing values are now successfully handled.\n\n\nThere is still some minor but essential data preprocessing needed before we proceed towards building our machine learning model. We are going to divide these remaining preprocessing steps into two main tasks:\n\n\n\nConvert the non-numeric data into numeric.\n\n\nScale the feature values to a uniform range.\n\n\n\nFirst, we will be converting all the non-numeric values into numeric ones. We do this because not only it results in a faster computation but also many machine learning models (like XGBoost) (and especially the ones developed using scikit-learn) require the data to be in a strictly numeric format. We will do this by using the get_dummies() method from pandas.\n\n\n# Convert the categorical features in the train and test sets independently\ncc_apps_train = pd.get_dummies(cc_apps_train)\ncc_apps_test = pd.get_dummies(cc_apps_test)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test = cc_apps_test.reindex(columns= cc_apps_train.columns, fill_value= 0)"
  },
  {
    "objectID": "projects/cc_approval/index.html#preprocessing-the-data-part-ii",
    "href": "projects/cc_approval/index.html#preprocessing-the-data-part-ii",
    "title": "Credit card applications",
    "section": "Preprocessing the data (part ii)",
    "text": "Preprocessing the data (part ii)\n\nNow, we are only left with one final preprocessing step of scaling before we can fit a machine learning model to the data.\n\n\nNow, let‚Äôs try to understand what these scaled values mean in the real world. Let‚Äôs use CreditScore as an example. The credit score of a person is their creditworthiness based on their credit history. The higher this number, the more financially trustworthy a person is considered to be. So, a CreditScore of 1 is the highest since we‚Äôre rescaling all the values to the range of 0-1.\n\n\n# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Segregate features and labels into separate variables\nX_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\nX_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "projects/cc_approval/index.html#fitting-a-logistic-regression-model-to-the-train-set",
    "href": "projects/cc_approval/index.html#fitting-a-logistic-regression-model-to-the-train-set",
    "title": "Credit card applications",
    "section": "Fitting a logistic regression model to the train set",
    "text": "Fitting a logistic regression model to the train set\n\nEssentially, predicting if a credit card application will be approved or not is a classification task. According to UCI, our dataset contains more instances that correspond to ‚ÄúDenied‚Äù status than instances corresponding to ‚ÄúApproved‚Äù status. Specifically, out of 690 instances, there are 383 (55.5%) applications that got denied and 307 (44.5%) applications that got approved.\n\n\nThis gives us a benchmark. A good machine learning model should be able to accurately predict the status of the applications with respect to these statistics.\n\n\nWhich model should we pick? A question to ask is: are the features that affect the credit card approval decision process correlated with each other? Although we can measure correlation, that is outside the scope of this notebook, so we‚Äôll rely on our intuition that they indeed are correlated for now. Because of this correlation, we‚Äôll take advantage of the fact that generalized linear models perform well in these cases. Let‚Äôs start our machine learning modeling with a Logistic Regression model (a generalized linear model).\n\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()"
  },
  {
    "objectID": "projects/cc_approval/index.html#making-predictions-and-evaluating-performance",
    "href": "projects/cc_approval/index.html#making-predictions-and-evaluating-performance",
    "title": "Credit card applications",
    "section": "Making predictions and evaluating performance",
    "text": "Making predictions and evaluating performance\n\nBut how well does our model perform?\n\n\nWe will now evaluate our model on the test set with respect to classification accuracy. But we will also take a look the model‚Äôs confusion matrix. In the case of predicting credit card applications, it is important to see if our machine learning model is equally capable of predicting approved and denied status, in line with the frequency of these labels in our original dataset. If our model is not performing well in this aspect, then it might end up approving the application that should have been approved. The confusion matrix helps us to view our model‚Äôs performance from these aspects.\n\n\n# Import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))\n\n# Print the confusion matrix of the logreg model\nconfusion_matrix(y_test,y_pred)\n\nAccuracy of logistic regression classifier:  1.0\n\n\narray([[103,   0],\n       [  0, 125]])"
  },
  {
    "objectID": "projects/cc_approval/index.html#grid-searching-and-making-the-model-perform-better",
    "href": "projects/cc_approval/index.html#grid-searching-and-making-the-model-perform-better",
    "title": "Credit card applications",
    "section": "Grid searching and making the model perform better",
    "text": "Grid searching and making the model perform better\n\nOur model was pretty good! In fact it was able to yield an accuracy score of 100%.\n\n\nFor the confusion matrix, the first element of the of the first row of the confusion matrix denotes the true negatives meaning the number of negative instances (denied applications) predicted by the model correctly. And the last element of the second row of the confusion matrix denotes the true positives meaning the number of positive instances (approved applications) predicted by the model correctly.\n\n\nBut if we hadn‚Äôt got a perfect score what‚Äôs to be done?. We can perform a grid search of the model parameters to improve the model‚Äôs ability to predict credit card approvals.\n\n\nscikit-learn‚Äôs implementation of logistic regression consists of different hyperparameters but we will grid search over the following two:\n\n\n\ntol\n\n\nmax_iter\n\n\n\n# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001 ,0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)"
  },
  {
    "objectID": "projects/cc_approval/index.html#finding-the-best-performing-model",
    "href": "projects/cc_approval/index.html#finding-the-best-performing-model",
    "title": "Credit card applications",
    "section": "Finding the best performing model",
    "text": "Finding the best performing model\n\nWe have defined the grid of hyperparameter values and converted them into a single dictionary format which GridSearchCV() expects as one of its parameters. Now, we will begin the grid search to see which values perform best.\n\n\nWe will instantiate GridSearchCV() with our earlier logreg model with all the data we have. We will also instruct GridSearchCV() to perform a cross-validation of five folds.\n\n\nWe‚Äôll end the notebook by storing the best-achieved score and the respective best parameters.\n\n\nWhile building this credit card predictor, we tackled some of the most widely-known preprocessing steps such as scaling, label encoding, and missing value imputation. We finished with some machine learning to predict if a person‚Äôs application for a credit card would get approved or not given some information about that person.\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test,y_test))\n\nBest: 1.000000 using {'max_iter': 100, 'tol': 0.001}\nAccuracy of logistic regression classifier:  1.0"
  },
  {
    "objectID": "projects/Segmentation/index.html",
    "href": "projects/Segmentation/index.html",
    "title": "Segmentation Methods",
    "section": "",
    "text": "Market segmentation is the process of dividing a broad consumer or business market, normally consisting of existing and potential customers, into sub-groups of consumers (known as segments) based on some type of shared characteristics. The objective is to design marketing strategies that target specific groups more effectively, leading to better customer satisfaction and increased business success.\nCluster analytics, also known as cluster analysis or clustering, is a technique used in data analysis and machine learning to group a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). The primary goal of cluster analytics is to find natural groupings in data based on inherent similarities or patterns.\nCluster analytics provides a powerful, data-driven approach to market segmentation, uncovering natural groupings within the data that might not be visible through traditional methods. By leveraging cluster analytics, businesses can achieve more accurate, dynamic, and actionable market segments, leading to enhanced marketing effectiveness and improved customer satisfaction."
  },
  {
    "objectID": "projects/Segmentation/index.html#introduction",
    "href": "projects/Segmentation/index.html#introduction",
    "title": "Segmentation Methods",
    "section": "",
    "text": "Market segmentation is the process of dividing a broad consumer or business market, normally consisting of existing and potential customers, into sub-groups of consumers (known as segments) based on some type of shared characteristics. The objective is to design marketing strategies that target specific groups more effectively, leading to better customer satisfaction and increased business success.\nCluster analytics, also known as cluster analysis or clustering, is a technique used in data analysis and machine learning to group a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). The primary goal of cluster analytics is to find natural groupings in data based on inherent similarities or patterns.\nCluster analytics provides a powerful, data-driven approach to market segmentation, uncovering natural groupings within the data that might not be visible through traditional methods. By leveraging cluster analytics, businesses can achieve more accurate, dynamic, and actionable market segments, leading to enhanced marketing effectiveness and improved customer satisfaction."
  },
  {
    "objectID": "projects/Segmentation/index.html#data-overview",
    "href": "projects/Segmentation/index.html#data-overview",
    "title": "Segmentation Methods",
    "section": "Data overview",
    "text": "Data overview\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\n# Calculate summary statistics for the dataset\nsummary_statistics = iris.describe()\n\n# Display the summary statistics\nsummary_statistics\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000"
  },
  {
    "objectID": "projects/Segmentation/index.html#k-means",
    "href": "projects/Segmentation/index.html#k-means",
    "title": "Segmentation Methods",
    "section": "K-Means",
    "text": "K-Means\nTo implement the k-means algorithm and visualize its steps, I will follow these steps:\n\nInitial of Centroids\nCompute distances\nAssign clusters based on initial centroids\nUpdate Centroids\nImplement the k-means algorithm.\nVisualize steps of the algorithm.\n\nLet‚Äôs start by normalizing the dataset and implementing the k-means algorithm. We‚Äôll use only the numerical columns for clustering\n\nInitial of Centroids\nThe initialization of centroids is a crucial step in the k-means algorithm. The process involves selecting \\({k}\\) initial points in the dataset that will serve as the starting centroids for the clustering process. Proper initialization can significantly affect the convergence speed and the quality of the final clusters. Here‚Äôs a detailed explanation of the centroid initialization process:\n\n# Step 1: Randomly select initial centroids\ndef initial_centroids(X, k):\n    np.random.seed(42)\n    random_indices = np.random.choice(X.index, size=k, replace=False)\n    return X.loc[random_indices]\n\n# Number of clusters\nk = 3\n\ncentroids = initial_centroids(iris, k)\ninitial_centroids_numeric = centroids.select_dtypes(include=[np.number])\n\n# Function to plot the initial centroid selection\ndef plot_initial_centroids(X, centroids):\n    X_values = X.values\n    centroids_values = centroids.values\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X_values[:, 0], X_values[:, 1], s=30, cmap='viridis')\n    plt.scatter(centroids_values[:, 0], centroids_values[:, 1], s=300, c='red', marker='X')\n    plt.title('Initial Centroid Selection')\n    plt.xlabel('Sepal Length (normalized)')\n    plt.ylabel('Sepal Width (normalized)')\n    plt.show()\n\n# Plot initial centroids\nplot_initial_centroids(iris, centroids)\n\n\n\n\n\n\n\n\nIn this step, we randomly select \\({k}\\) data points from the dataset. The initial centroids act as the starting points for the algorithm.\nThese points are chosen to represent the center of the initial clusters. Since they are selected randomly, different runs of the algorithm might start with different centroids, leading to different clustering results.\n\n\nCompute distances\n\n# Step 2: Compute distances\ndef compute_distances(X, centroids):\n    X_numeric = X.select_dtypes(include=[np.number])\n    X_values = X_numeric.values\n    centroids_values = centroids.values\n    distances = np.linalg.norm(X_values[:, np.newaxis] - centroids_values, axis=2)\n    return distances\n\n# Compute distances\ndistances = compute_distances(iris, initial_centroids_numeric)\n\n# Assign clusters based on initial centroids\ninitial_labels = np.argmin(distances, axis=1)\n\n\n# Extract the first data point from the dataset\nfirst_data_point = iris.select_dtypes(include=[np.number]).iloc[0].values\n\n# Compute distances from the first data point to each centroid\ndistances_first_point = np.linalg.norm(first_data_point - initial_centroids_numeric.values, axis=1)\n\n# Coordinates of the first data point and initial centroids\nfirst_data_point_coords = first_data_point[:2]  # Only take the first two dimensions\ncentroid_coords = initial_centroids_numeric.values[:, :2]  # Only take the first two dimensions\n\n# Plot the first data point and centroids with distances\nplt.figure(figsize=(8, 6))\nplt.scatter(first_data_point_coords[0], first_data_point_coords[1], color='green', s=100, label='Data Point 1')\nplt.scatter(centroid_coords[:, 0], centroid_coords[:, 1], color='red', s=100, label='Centroids')\n\n# Draw lines and annotate distances\nfor i, (x, y) in enumerate(centroid_coords):\n    plt.plot([first_data_point_coords[0], x], [first_data_point_coords[1], y], 'k-')\n    distance = np.linalg.norm(first_data_point_coords - np.array([x, y]))\n    mid_x, mid_y = (first_data_point_coords[0] + x) / 2, (first_data_point_coords[1] + y) / 2\n    plt.text(mid_x, mid_y, f'{distance:.2f}', fontsize=12)\n\n# Annotate plot\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Distances from First Data Point to Initial Centroids')\nplt.show()\n\n\n\n\n\n\n\n\nThe plot visualizes the distances from the first data point in Iris dataset to three initial centroids:\nDistance to Centroid 1:\n\nThe distance is approximately 0.67 units.\nThis centroid is the closest to Data Point 1.\n\nDistance to Centroid 2:\n\nThe distance is approximately 1.22 units.\nThis centroid is further away compared to Centroid 1.\n\nDistance to Centroid 3:\n\nThe distance is approximately 2.75 units.\nThis centroid is the farthest from Data Point 1.\n\nIterpretation:\n\nCluster Assignment: Based on the Euclidean distances, Data Point 1 would be assigned to the cluster of Centroid 1, as it is the nearest.\nCluster Formation: Initial clusters will be formed by assigning each data point in the dataset to the nearest centroid. Over several iterations, the centroids will move to minimize the within-cluster variance.\n\n\n\nAssign clusters based on initial centroids\n\n# Step 3: Assign cluster\n# Function to plot the initial assignment step\ndef plot_initial_assignment(X, centroids, labels):\n    X_numeric = X.select_dtypes(include=[np.number])\n    X_values = X_numeric.values\n    centroids_values = centroids.values\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X_values[:, 0], X_values[:, 1], c=labels, s=30, cmap='viridis')\n    plt.scatter(centroids_values[:, 0], centroids_values[:, 1], s=300, c='red', marker='X')\n    plt.title('Initial Assignment Step')\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.show()\n\n# Plot initial assignment step\nplot_initial_assignment(iris, initial_centroids_numeric, initial_labels)\n\n\n\n\n\n\n\n\n\n\nUpdate Centroids\nAfter assigning each data point to the nearest centroid (forming clusters), the algorithm updates the position of each centroid based on the mean of the data points assigned to that cluster. This step ensures that the centroids move closer to the center of their respective clusters, improving the overall clustering result.\n\n# Step 3: Update centroids\ndef update_centroids(X, labels, k):\n    X_numeric = X.select_dtypes(include=[np.number])\n    new_centroids = np.array([X_numeric.values[labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\nWhy This Step is Important:\n\nAccuracy: Updating the centroids ensures that they accurately represent the current cluster composition.\nConvergence: The k-means algorithm converges when the centroids stabilize, meaning they no longer change significantly between iterations.\nCluster Quality: Properly updated centroids lead to more compact and well-defined clusters, improving the overall quality of the clustering.\n\n\n\nImplement k-means algorithm\n\n# Step 4: Implement k-means algorithm\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    centroids = initial_centroids(X, k).select_dtypes(include=[np.number]).values\n\n    for i in range(max_iters):\n        distances = compute_distances(X, pd.DataFrame(centroids))\n        labels = np.argmin(distances, axis=1)\n        new_centroids = update_centroids(X, labels, k)\n        \n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n        \n        centroids = new_centroids\n\n    return centroids, labels\n\n# Run the k-means algorithm\nfinal_centroids, final_labels = kmeans(iris, k)\n\n# Plot the final clusters\ndef plot_final_clusters(X, centroids, labels, message):\n    X_numeric = X.select_dtypes(include=[np.number])\n    X_values = X_numeric.values\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X_values[:, 0], X_values[:, 1], c=labels, s=30, cmap='viridis')\n    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')\n    plt.title(f'K-means using {message}')\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.show()\n\n# Plot final clusters\nplot_final_clusters(iris, final_centroids, final_labels, message = \"Self-Build Fuction\")\n\n\n\n\n\n\n\n\n\n# Plot the final clusters\ndef plot_final_clusters(X, centroids, labels, message):\n    X_numeric = X.select_dtypes(include=[np.number])\n    X_values = X_numeric.values\n    plt.figure(figsize=(8, 6))\n    for i, cluster in enumerate(X_values):\n        plt.scatter(X_values[:, 0], X_values[:, 1], c=labels, s=30, cmap='viridis')\n    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')\n    plt.title(f'K-means using {message}')\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.show()\n\n# Plot final clusters\nplot_final_clusters(iris, final_centroids, final_labels, message = \"Self-Build Fuction\")\n\n\n\n\n\n\n\n\n\n\nCompare to the built-in Python package\n\n# Using the built-in KMeans function from scikit-learn\niris_numeric = iris.select_dtypes(include=[np.number])\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42)\nkmeans_sklearn.fit(iris_numeric)\n\n# Extracting the centroids and labels from the sklearn KMeans\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\nsklearn_labels = kmeans_sklearn.labels_\n\n# Align labels using the Hungarian algorithm. \n# This function to making sure label of both approaches are the same colors \n# without change the clusters themselves\ndef align_labels(final_labels, sklearn_labels):\n    D = np.zeros((k, k))\n    for i in range(k):\n        for j in range(k):\n            D[i, j] = np.sum((sklearn_labels == i) != (final_labels == j))\n    row_ind, col_ind = linear_sum_assignment(D)\n    aligned_labels = np.zeros_like(sklearn_labels)\n    for i, j in zip(row_ind, col_ind):\n        aligned_labels[sklearn_labels == i] = j\n    return aligned_labels\naligned_labels = align_labels(final_labels, sklearn_labels)\n\nplot_final_clusters(iris, centroids_sklearn, aligned_labels, message = \"Sklearn Function\")\n\n\n\n\n\n\n\n\nAs we can see, both plot presents the same cluster characteristic. Here are the detailed intepretation:\nCluster 1 (Green):\n\nLocated primarily on the left side of the plot.\nContains data points with a range of Sepal Lengths and Sepal Widths.\nThe centroid is positioned where the density of green points is highest.\n\nCluster 2 (Purple):\n\nPositioned in the middle of the plot.\nComprises data points with intermediate Sepal Lengths and Sepal Widths.\nThe centroid is at the center of the dense region of purple points.\n\nCluster 3 (Yellow):\n\nFound on the right side of the plot.\nIncludes data points with larger Sepal Lengths and a range of Sepal Widths.\nThe centroid is placed where the yellow points are densest.\n\nCluster Separation and Compactness:\n\nCompactness: Each cluster appears to be relatively compact, with data points grouped closely around their centroids.\nSeparation: There is clear separation between the clusters, as indicated by distinct regions of colors. This suggests that the clusters are well-defined and the algorithm has successfully partitioned the data.\n\nCluster Size:\n\nCluster 1 (Green) appears to have the largest number of data points.\nCluster 3 (Yellow) has fewer data points compared to Cluster 1.\nCluster 2 (Purple) has the smallest number of data points.\n\n\nComparison to Initial Centroid Selection\nInitial Centroids:\n\nThe initial centroids were randomly placed and did not represent the actual data distribution well.\nThe data points were not assigned to any clusters, so no clear clustering was visible.\n\nMovement of Centroids:\n\nThe centroids moved significantly from their initial positions to their final positions.\nThis movement indicates that the algorithm iteratively adjusted the centroids to better represent the data points assigned to each cluster.\n\nClustering Quality:\n\nInitial State: No clear clusters, with centroids positioned arbitrarily.\nFinal State: Well-defined clusters with centroids accurately representing the center of each cluster.\n\nSummary\n\nThe final clusters plot demonstrates the effectiveness of the k-means algorithm in partitioning the data into three distinct clusters.\nThe clusters are well-separated and compact, indicating that the centroids have been optimized to minimize within-cluster variance.\nThe clear separation and grouping of data points into distinct clusters highlight the successful convergence of the k-means algorithm.\nBy analyzing the final clusters, we can conclude that the k-means algorithm has effectively identified meaningful patterns in the data, resulting in well-defined clusters\n\n\n\n\nNumber of Clusters\n\nWithin Cluster Sum of Squares\nThe Within-Cluster Sum of Squares (WCSS), also known as inertia, is a measure of the total variance within clusters. It quantifies the compactness of the clusters formed by a clustering algorithm like k-means. The goal of k-means clustering is to minimize this WCSS value.\nFor a given set of clusters \\({C}\\) = { \\({C_1}\\), \\({C_2}\\), \\(\\ldots\\), \\({C_k}\\) }, where \\({C_i}\\) represents the \\({i}\\)-th cluster, the WCSS is defined as follows:\n\\[\n\\text{WCSS} = \\sum_{i=1}^k \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 \\]\nwhere:\n\n\\({x}\\) is a data point.\n\\(\\mu_i\\) is the centroid of the \\({i}\\)-th cluster.\n\\({\\|x - \\mu_i\\|}\\) represents the Euclidean distance between a data point \\({x}\\) and the centroid \\(\\mu_i\\).\n\n\n# Define range of clusters\ncluster_range = range(2, 8)\nwcss = []\n\nfor k in cluster_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(iris_numeric)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(cluster_range, wcss, marker='o')\nplt.title('Within-Cluster Sum of Squares (WCSS)')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\nThe WCSS plot shows the sum of squared distances between each point and the centroid of its assigned cluster for different numbers of clusters (K). The goal is to minimize the WCSS, indicating compact clusters where data points are close to their centroids.\nK=2: The WCSS is very high (~145), suggesting that having only two clusters does not effectively capture the structure of the data.\nK=3: There is a significant drop in WCSS to around 85, indicating that adding a third cluster greatly improves the compactness of the clusters.\nK=4 to K=7: The decrease in WCSS continues but at a much slower rate. The changes are more gradual, indicating diminishing returns as more clusters are added.\nInterpretation\n\nElbow Point: The ‚Äúelbow‚Äù point is typically where the WCSS starts to decrease more slowly. In this plot, the elbow is at K=3.\nDiminishing Returns: After K=3, adding more clusters results in smaller reductions in WCSS, suggesting that the additional clusters do not significantly improve the clustering quality.\n\nThe WCSS plot suggests that K=3 is a good choice for the number of clusters because it is the elbow point where the rate of decrease in WCSS slows down. This indicates a balance between having a sufficient number of clusters and maintaining compact clusters.\n\n\nSilhouette Score\nThe Silhouette Score is a metric used to evaluate the quality of a clustering. It measures how similar each point is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a higher score indicates that the clusters are well-separated and well-defined.\nThe silhouette score for a data point \\({i}\\) is defined as:\n\\[ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} \\]\nwhere:\n\n\\({a(i)}\\) is the average distance between the data point \\({i}\\) and all other points in the same cluster.\n\\({b(i)}\\) is the minimum average distance between the data point \\({i}\\) and points in the nearest cluster that \\({i}\\) is not a part of.\n\nThe overall silhouette score for a clustering is the mean silhouette score of all the data points.\n\nsilhouette_scores = []\nfor k in cluster_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(iris_numeric)\n    silhouette_scores.append(silhouette_score(iris_numeric, kmeans.labels_))\nplt.plot(cluster_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\n\n\n\n\n\nThe Silhouette Score plot shows the average silhouette score for different numbers of clusters (K). The silhouette score measures how similar a point is to its own cluster compared to other clusters, with values ranging from -1 to 1.\nK=2: The silhouette score is the highest (~0.67), suggesting that two clusters provide the best separation and well-defined clusters.\nK=3: The silhouette score decreases to about 0.55, indicating that the separation between clusters is not as good as when K=2.\nK=4 to K=7: The silhouette score continues to decrease, indicating that adding more clusters reduces the quality of the separation between clusters.\nInterpretation\nPeak Score: The highest silhouette score is at K=2, suggesting that two clusters provide the best-defined and well-separated clusters.\nDecreasing Trend: As K increases, the silhouette score decreases, indicating that adding more clusters reduces the quality of separation.\nThe Silhouette Score plot suggests that K=2 is the best choice for the number of clusters because it has the highest score, indicating the best separation between clusters."
  },
  {
    "objectID": "projects/Segmentation/index.html#latent-class-mnl",
    "href": "projects/Segmentation/index.html#latent-class-mnl",
    "title": "Segmentation Methods",
    "section": "Latent-Class MNL",
    "text": "Latent-Class MNL\ntodo: Use the Yogurt dataset from HW3 to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989), which you may want to read or reference. Compare the results to the standard (aggregate) MNL model from HW3. What are the differences in the parameter estimates?\ntodo: Fit the latent-class MNL model with 2, 3, ‚Ä¶, K classes. How many classes are suggested by the BIC? The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate ‚Äì akin to the adjusted R-squared for the linear regression model. Note, however, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model."
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html",
    "href": "projects/Unusual_properties/NY_OG.html",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "",
    "text": "This project has good example algorithms to do a forensic-type analysis, looking for anomalies in a dataset. We first do some data cleaning (exclusions, imputation, don‚Äôt remove outliers - that‚Äôs what we‚Äôre looking for), then build variables that are designed to look for the kinds of anomalies we are interested in, in this case, unusual property valuations.\nAfter we build the variables we know we have lots of correlations and too high dimensionality so we need to remove correlations and reduce dimensionality. Since we don‚Äôt have a dependent variable the easiest useful thing to do is PCA. We z scale (always z scale before a PCA), do PCA, keep the top few PCs, then z scale again in order to make each retained PC equally important (optional step; only do this if you keep just a few PCs.).\nWe use two different anomaly detection (fraud) algorithms. The first just looks for outliers in the final scaled PC space using a Minkowski distance from the origin. The second method makes a simple autoencoder and the fraud score is then the reproduction error. It‚Äôs important to note that each/either of these two methods would be a fine fraud score by itself.\nSince we have two score and we don‚Äôt really know which one is better we just average the two scores. To do this we replace the score with its rank order and then average the rank-ordered scores for our final score.\nFinally we sort all the records by this final score and explore the top n records. To help the investigation we show which of the variables are driving these top scoring records with a heat map of the variable zscores, which can point the investigators to what‚Äôs making the high score for these top scoring records.\nThis problem is an invented problem to demonstrate the process of building unsupervised fraud models. The data set is real and the invented problem is realistic. What‚Äôs lacking the most is the ability to interact with domain experts in order to do proper exclusions and design good/appropriate variables.\nThe data can be found here: https://data.cityofnewyork.us/Housing-Development/Property-Valuation-and-Assessment-Data/rgy2-tti8\nDownload PDF file.\ndata.head()\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nBLDFRONT\nBLDDEPTH\nAVLAND2\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\n\n\n\n\n0\n1\n1000010101\n1\n1\n101\nNaN\nU S GOVT LAND & BLDGS\nP7\n4\n500\n...\n0\n0\n3775500.0\n8613000.0\n3775500.0\n8613000.0\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n1\n2\n1000010201\n1\n1\n201\nNaN\nU S GOVT LAND & BLDGS\nZ9\n4\n27\n...\n0\n0\n11111400.0\n80690400.0\n11111400.0\n80690400.0\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n2\n3\n1000020001\n1\n2\n1\nNaN\nDEPT OF GENERAL SERVI\nY7\n4\n709\n...\n709\n564\n32321790.0\n40179510.0\n32321790.0\n40179510.0\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n3\n4\n1000020023\n1\n2\n23\nNaN\nDEPARTMENT OF BUSINES\nT2\n4\n793\n...\n85\n551\n13644000.0\n15750000.0\n13644000.0\n15750000.0\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n4\n5\n1000030001\n1\n3\n1\nNaN\nPARKS AND RECREATION\nQ1\n4\n323\n...\n89\n57\n106348680.0\n107758350.0\n106348680.0\n107758350.0\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n\n\n5 rows √ó 32 columns"
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html#remove-some-properties-that-we-arent-interested-in",
    "href": "projects/Unusual_properties/NY_OG.html#remove-some-properties-that-we-arent-interested-in",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "Remove some properties that we aren‚Äôt interested in",
    "text": "Remove some properties that we aren‚Äôt interested in\n\nnumrecords_orig = len(data)\nnumrecords = numrecords_orig\nnumrecords\n\n1070994\n\n\n\n#remove the records with easement type as goverment \ndata = data[data[\"EASEMENT\"] != \"U\"].reset_index(drop=True)\nnumremoved = numrecords - len(data)\nprint('# records removed:', numremoved)\n\n# records removed: 1\n\n\n\n# create some words for the owner name that might be goverment or a cemetery\ngov_list = ['DEPT ', 'DEPARTMENT', 'UNITED STATES','GOVERNMENT',' GOVT ', 'CEMETERY']\n\n# owner = list(set(data['OWNER'].to_list()))\n# owner.pop(0) #remove the nan\n\nowner1 = list(set(data['OWNER'].to_list()))\nowner = [item for item in owner1 if str(item) != 'nan'] # remove any nan's\n\nremove_list = []\nprint(\"Total owner number before removing is \", len(owner))\n\nfor i in owner:\n   for g in gov_list:\n    if g in i and 'STORES' not in i:\n        remove_list.append(i)\n\nTotal owner number before removing is  863347\n\n\n\n# Look at the most frequent owners. This might show some other properties we aren't interested in.\nremove_list2 = data['OWNER'].value_counts().head(20).index.tolist()\nremove_list2\n\n['PARKCHESTER PRESERVAT',\n 'PARKS AND RECREATION',\n 'DCAS',\n 'HOUSING PRESERVATION',\n 'CITY OF NEW YORK',\n 'DEPT OF ENVIRONMENTAL',\n 'BOARD OF EDUCATION',\n 'NEW YORK CITY HOUSING',\n 'CNY/NYCTA',\n 'NYC HOUSING PARTNERSH',\n 'YORKVILLE TOWERS ASSO',\n 'DEPARTMENT OF BUSINES',\n 'DEPT OF TRANSPORTATIO',\n 'MTA/LIRR',\n 'PARCKHESTER PRESERVAT',\n 'MH RESIDENTIAL 1, LLC',\n '434 M LLC',\n 'LINCOLN PLAZA ASSOCIA',\n 'DEUTSCHE BANK NATIONA',\n '561 11TH AVENUE TMG L']\n\n\n\n# add some others to also be removed\nremove_list2.append('THE CITY OF NEW YORK')\nremove_list2.append('NYS URBAN DEVELOPMENT')\nremove_list2.append('CULTURAL AFFAIRS')\nremove_list2.append('NY STATE PUBLIC WORKS')\nremove_list2.append(\"NYC DEP'T OF HIGHWAYS\")\nremove_list2.append('CITY WIDE ADMINISTRAT')\nremove_list2.append('NEW YORK CITY')\nremove_list2.append('THE PORT OFNY & NJ')\nremove_list2.append('NEW YORK STATE DEPART')\nremove_list2.append('CITY AND NON-CITY OWN')\nremove_list2.append('SANITATION')\nremove_list2.append('NYS DOT')\nremove_list2.append('NEW YORK CITY TRANSIT')\nremove_list2.append('PORT AUTHORITY OF NY')\nremove_list2.append('NEW YORK STATE OWNED')\nremove_list2.append('NYC PARK DEPT')\nremove_list2.append('PORT OF NEW YORK AUTH')\nremove_list2.append('NYC PARK DEPT')\nremove_list2.append('LIRR')\nremove_list2.append('NY STATE PUBLIC SERV')\nremove_list2.append('STATE OF NEW YORK')\nremove_list2.append('NYC HIGHWAY DEPT')\n\n\nfor i in remove_list2:\n    if i not in remove_list:\n        remove_list.append(i)\n    else:\n        print(i)\n\nDEPT OF ENVIRONMENTAL\nDEPARTMENT OF BUSINES\nDEPT OF TRANSPORTATIO\nNYC PARK DEPT\n\n\n\n# rremove some of the removes...\nremove_list.remove('YORKVILLE TOWERS ASSO')\nremove_list.remove('434 M LLC')\nremove_list.remove('DEUTSCHE BANK NATIONA')\nremove_list.remove('561 11TH AVENUE TMG L')\nremove_list.remove('MH RESIDENTIAL 1, LLC')\n\n\nlen(remove_list)\n\n263\n\n\n\nnumrecords = len(data)\nremoved = data[data['OWNER'].isin(remove_list)].reset_index(drop=True)\ndata = data[~data['OWNER'].isin(remove_list)].reset_index(drop=True)\nnumremoved = numrecords - len(data)\nprint('# records removed:', numremoved)\n\n# records removed: 26500\n\n\n\nremoved.shape\n\n(26500, 32)\n\n\n\n# any on this list that we shouldn't remove?\n# plt.rcParams.update({'figure.figsize':(6,14)})\nplt.figure(figsize=(6,14))\nplt.xscale('log')\nremoved['OWNER'].value_counts().head(50).sort_values().plot(kind='barh')\n\n\n\n\n\n\n\n\n\ndata.shape\n\n(1044493, 32)\n\n\n\n# this is how many records we removed\nnumrecords_orig - len(data)\n\n26501\n\n\n\ndata.head(10)\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nBLDFRONT\nBLDDEPTH\nAVLAND2\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\n\n\n\n\n0\n9\n1000041001\n1\n4\n1001\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n636093.0\n2049290.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n1\n10\n1000041002\n1\n4\n1002\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n919276.0\n2961617.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n2\n11\n1000041003\n1\n4\n1003\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n967500.0\n5483912.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n3\n12\n1000041004\n1\n4\n1004\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n163174.0\n525692.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n4\n13\n1000041005\n1\n4\n1005\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n373783.0\n1204211.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n5\n14\n1000041006\n1\n4\n1006\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n353383.0\n1138493.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n6\n15\n1000041007\n1\n4\n1007\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n1246572.0\n4016063.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n7\n16\n1000041008\n1\n4\n1008\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n1213369.0\n3909089.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n8\n17\n1000041009\n1\n4\n1009\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n1213369.0\n3909089.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n9\n18\n1000041010\n1\n4\n1010\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n...\n0\n0\n1213369.0\n3909089.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n\n\n10 rows √ó 32 columns"
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html#fill-in-missing-zip",
    "href": "projects/Unusual_properties/NY_OG.html#fill-in-missing-zip",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "Fill in missing ZIP",
    "text": "Fill in missing ZIP\n\n# How many zips are missing? Replace NAN with 0 and count them.\nmissing_zips = np.where(pd.isnull(data['ZIP']))[0]\nnum_missing_zips_orig = len(missing_zips)\nnum_missing_zips_orig\n\n20431\n\n\n\nsum(data['BORO'].isna())\n\n0\n\n\n\nsum(data['STADDR'].isna())\n\n364\n\n\n\n# concatenate the 'staddr' and 'boro' columns into a new 'staddr_boro' column \ndata['staddr_boro'] = data[data['STADDR'].notnull()]['STADDR'] + '_' + data[data['BORO'].notnull()]['BORO'].astype(str)\ndata['staddr_boro']\n\n0              1 WATER STREET_1\n1              1 WATER STREET_1\n2              1 WATER STREET_1\n3              1 WATER STREET_1\n4              1 WATER STREET_1\n                   ...         \n1044488    142 BENTLEY STREET_5\n1044489    146 BENTLEY STREET_5\n1044490    150 BENTLEY STREET_5\n1044491    156 BENTLEY STREET_5\n1044492    162 BENTLEY STREET_5\nName: staddr_boro, Length: 1044493, dtype: object\n\n\n\nstaddr_boro_zip = {}\nfor index, staddrboro in data['staddr_boro'].items():\n    if staddrboro not in staddr_boro_zip :\n        staddr_boro_zip [staddrboro] = data.loc[index, 'ZIP']\n        \n        \n# fill in by mapping with street addrees boroughs\ndata['ZIP'] = data['ZIP'].fillna(data['staddr_boro'].map(staddr_boro_zip))\n\n\n# how many missing zips did we fill in with this last step?\nnum_filled_in = num_missing_zips_orig - len(np.where(pd.isnull(data['ZIP']))[0])\nnum_filled_in\n\n2832\n\n\n\n# How many are still left to fill in?\nmissing_zips = np.where(pd.isnull(data['ZIP']))[0]\nlen(missing_zips)\n\n17599\n\n\n\n# Assume the data is already sorted by zip. If a zip is missing, \n# and the before and after zips are the same, fill in the zip with that value\nfor i in range(len(missing_zips)):\n    if(data.loc[missing_zips[i]+1,'ZIP'] == data.loc[missing_zips[i]-1,'ZIP']):\n        data.loc[missing_zips[i],'ZIP'] = data.loc[missing_zips[i]-1,'ZIP']\n\n\n# how many mnissing zips did we fill in with this last step?\nnum_filled_in = len(missing_zips) - len(np.where(pd.isnull(data['ZIP']))[0])\nnum_filled_in\n\n9491\n\n\n\n# How many are still left to fill in?\nmissing_zips = np.where(pd.isnull(data['ZIP']))[0]\nlen(missing_zips)\n\n8108\n\n\n\n%%time\n# For the remaining missing zips, just fill in with the previous record's zip.\n# another slow loop that should be improved...\nfor i in range(len(missing_zips)):\n    data.loc[missing_zips[i],'ZIP'] = data.loc[missing_zips[i]-1,'ZIP']\n\nCPU times: user 484 ms, sys: 3.4 ms, total: 487 ms\nWall time: 492 ms\n\n\n\nmissing_zips = np.where(pd.isnull(data['ZIP']))[0]\nlen(missing_zips)\n\n0\n\n\n\ndata = data.drop('staddr_boro', axis=1)\n\n\nFULLVAL, AVLAND, AVTOT\n\nFULLVAL\n\nlen(data[data['FULLVAL']==0])\n\n10025\n\n\n\ndata['FULLVAL'].isnull().sum()\n\n0\n\n\n\ndata['FULLVAL'].replace(0, np.nan, inplace=True)\ndata['FULLVAL'].isnull().sum()\n\n10025\n\n\n\ndata[\"FULLVAL\"] = data.\\\n                        groupby(['TAXCLASS','BORO','BLDGCL'])['FULLVAL'].transform(lambda x: x.fillna(x.mean()))\ndata['FULLVAL'].isnull().sum()\n\n7307\n\n\n\ndata[\"FULLVAL\"] = data.\\\n                        groupby(['TAXCLASS','BORO'])['FULLVAL'].transform(lambda x: x.fillna(x.mean()))\ndata['FULLVAL'].isnull().sum()\n\n386\n\n\n\ndata[\"FULLVAL\"] = data.\\\n                        groupby(['TAXCLASS'])['FULLVAL'].transform(lambda x: x.fillna(x.mean()))\ndata['FULLVAL'].isnull().sum()\n\n0\n\n\n\n\nAVLAND\n\nlen(data[data['AVLAND']==0])\n\n10027\n\n\n\ndata['AVLAND'].isnull().sum()\n\n0\n\n\n\ndata['AVLAND'].replace(0, np.nan, inplace=True)\ndata['AVLAND'].isnull().sum()\n\n10027\n\n\n\ndata[\"AVLAND\"] = data.\\\n                        groupby(['TAXCLASS','BORO','BLDGCL'])['AVLAND'].transform(lambda x: x.fillna(x.mean()))\ndata['AVLAND'].isnull().sum()\n\n7307\n\n\n\ndata[\"AVLAND\"] = data.\\\n                        groupby(['TAXCLASS','BORO'])['AVLAND'].transform(lambda x: x.fillna(x.mean()))\ndata['AVLAND'].isnull().sum()\n\n386\n\n\n\ndata[\"AVLAND\"] = data.\\\n                        groupby(['TAXCLASS'])['AVLAND'].transform(lambda x: x.fillna(x.mean()))\ndata['AVLAND'].isnull().sum()\n\n0\n\n\n\n\nAVTOT\n\nlen(data[data['AVTOT']==0])\n\n10025\n\n\n\ndata['AVTOT'].isnull().sum()\n\n0\n\n\n\ndata['AVTOT'].replace(0, np.nan, inplace=True)\ndata['AVTOT'].isnull().sum()\n\n10025\n\n\n\ndata[\"AVTOT\"] = data.\\\n                        groupby(['TAXCLASS','BORO','BLDGCL'])['AVTOT'].transform(lambda x: x.fillna(x.mean()))\ndata['AVTOT'].isnull().sum()\n\n7307\n\n\n\ndata[\"AVTOT\"] = data.\\\n                        groupby(['TAXCLASS','BORO'])['AVTOT'].transform(lambda x: x.fillna(x.mean()))\ndata['AVTOT'].isnull().sum()\n\n386\n\n\n\ndata[\"AVTOT\"] = data.\\\n                        groupby(['TAXCLASS'])['AVTOT'].transform(lambda x: x.fillna(x.mean()))\ndata['AVTOT'].isnull().sum()\n\n0\n\n\n\ndata.head().transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nRECORD\n9\n10\n11\n12\n13\n\n\nBBLE\n1000041001\n1000041002\n1000041003\n1000041004\n1000041005\n\n\nBORO\n1\n1\n1\n1\n1\n\n\nBLOCK\n4\n4\n4\n4\n4\n\n\nLOT\n1001\n1002\n1003\n1004\n1005\n\n\nEASEMENT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOWNER\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\n\n\nBLDGCL\nR5\nR5\nR5\nR5\nR5\n\n\nTAXCLASS\n4\n4\n4\n4\n4\n\n\nLTFRONT\n0\n0\n0\n0\n0\n\n\nLTDEPTH\n0\n0\n0\n0\n0\n\n\nEXT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSTORIES\n50.0\n50.0\n50.0\n50.0\n50.0\n\n\nFULLVAL\n3944762.0\n5700930.0\n10600000.0\n1011928.0\n2318026.0\n\n\nAVLAND\n636093.0\n919276.0\n967500.0\n163174.0\n373783.0\n\n\nAVTOT\n1775143.0\n2565419.0\n4770000.0\n455368.0\n1043112.0\n\n\nEXLAND\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nEXTOT\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nEXCD1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSTADDR\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n\n\nZIP\n10004.0\n10004.0\n10004.0\n10004.0\n10004.0\n\n\nEXMPTCL\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nBLDFRONT\n0\n0\n0\n0\n0\n\n\nBLDDEPTH\n0\n0\n0\n0\n0\n\n\nAVLAND2\n636093.0\n919276.0\n967500.0\n163174.0\n373783.0\n\n\nAVTOT2\n2049290.0\n2961617.0\n5483912.0\n525692.0\n1204211.0\n\n\nEXLAND2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nEXTOT2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nEXCD2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nPERIOD\nFINAL\nFINAL\nFINAL\nFINAL\nFINAL\n\n\nYEAR\n2010/11\n2010/11\n2010/11\n2010/11\n2010/11\n\n\nVALTYPE\nAC-TR\nAC-TR\nAC-TR\nAC-TR\nAC-TR"
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html#fill-in-the-missing-stories",
    "href": "projects/Unusual_properties/NY_OG.html#fill-in-the-missing-stories",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "Fill in the missing STORIES",
    "text": "Fill in the missing STORIES\n\ndata['STORIES'].isnull().sum()\n\n42030\n\n\n\nmodes = data.groupby(['BORO', 'BLDGCL'])['STORIES'] \\\n         .transform(lambda x: x.mode(dropna=False).iloc[0])\ndata['STORIES'] = data['STORIES'].fillna(modes)\n\n\ndata['STORIES'].isnull().sum()\n\n37922\n\n\n\ndata[\"STORIES\"] = data.\\\n                        groupby(['TAXCLASS'])['STORIES'].transform(lambda x: x.fillna(x.mean()))\n\n\ndata['STORIES'].isnull().sum()\n\n0\n\n\n\ndata.head().transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nRECORD\n9\n10\n11\n12\n13\n\n\nBBLE\n1000041001\n1000041002\n1000041003\n1000041004\n1000041005\n\n\nBORO\n1\n1\n1\n1\n1\n\n\nBLOCK\n4\n4\n4\n4\n4\n\n\nLOT\n1001\n1002\n1003\n1004\n1005\n\n\nEASEMENT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOWNER\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\nTRZ HOLDINGS, LLC\n\n\nBLDGCL\nR5\nR5\nR5\nR5\nR5\n\n\nTAXCLASS\n4\n4\n4\n4\n4\n\n\nLTFRONT\n0\n0\n0\n0\n0\n\n\nLTDEPTH\n0\n0\n0\n0\n0\n\n\nEXT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSTORIES\n50.0\n50.0\n50.0\n50.0\n50.0\n\n\nFULLVAL\n3944762.0\n5700930.0\n10600000.0\n1011928.0\n2318026.0\n\n\nAVLAND\n636093.0\n919276.0\n967500.0\n163174.0\n373783.0\n\n\nAVTOT\n1775143.0\n2565419.0\n4770000.0\n455368.0\n1043112.0\n\n\nEXLAND\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nEXTOT\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nEXCD1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSTADDR\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n1 WATER STREET\n\n\nZIP\n10004.0\n10004.0\n10004.0\n10004.0\n10004.0\n\n\nEXMPTCL\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nBLDFRONT\n0\n0\n0\n0\n0\n\n\nBLDDEPTH\n0\n0\n0\n0\n0\n\n\nAVLAND2\n636093.0\n919276.0\n967500.0\n163174.0\n373783.0\n\n\nAVTOT2\n2049290.0\n2961617.0\n5483912.0\n525692.0\n1204211.0\n\n\nEXLAND2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nEXTOT2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nEXCD2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nPERIOD\nFINAL\nFINAL\nFINAL\nFINAL\nFINAL\n\n\nYEAR\n2010/11\n2010/11\n2010/11\n2010/11\n2010/11\n\n\nVALTYPE\nAC-TR\nAC-TR\nAC-TR\nAC-TR\nAC-TR"
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html#fill-in-ltfront-ltdepth-blddepth-bldfront-with-averages-by-taxclass",
    "href": "projects/Unusual_properties/NY_OG.html#fill-in-ltfront-ltdepth-blddepth-bldfront-with-averages-by-taxclass",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "Fill in LTFRONT, LTDEPTH, BLDDEPTH, BLDFRONT with averages by TAXCLASS",
    "text": "Fill in LTFRONT, LTDEPTH, BLDDEPTH, BLDFRONT with averages by TAXCLASS\n\n# Because these 4 fields do not have NAs, we just need to replace 0s. \n# We think zero and 1 are invalid values for these fields, so replace them with NA.\n# Calculate groupwise average. Replace 0 and 1's by NAs so they are not counted in calculating mean.\ndata.loc[data['LTFRONT']==0,'LTFRONT']=np.nan\ndata.loc[data['LTDEPTH']==0,'LTDEPTH']=np.nan\ndata.loc[data['BLDFRONT']==0,'BLDFRONT']=np.nan\ndata.loc[data['BLDDEPTH']==0,'BLDDEPTH']=np.nan\ndata.loc[data['LTFRONT']==1,'LTFRONT']=np.nan\ndata.loc[data['LTDEPTH']==1,'LTDEPTH']=np.nan\ndata.loc[data['BLDFRONT']==1,'BLDFRONT']=np.nan\ndata.loc[data['BLDDEPTH']==1,'BLDDEPTH']=np.nan\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nBLDFRONT\nBLDDEPTH\nAVLAND2\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\n\n\n\n\n0\n9\n1000041001\n1\n4\n1001\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\nNaN\n...\nNaN\nNaN\n636093.0\n2049290.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n1\n10\n1000041002\n1\n4\n1002\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\nNaN\n...\nNaN\nNaN\n919276.0\n2961617.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n2\n11\n1000041003\n1\n4\n1003\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\nNaN\n...\nNaN\nNaN\n967500.0\n5483912.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n3\n12\n1000041004\n1\n4\n1004\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\nNaN\n...\nNaN\nNaN\n163174.0\n525692.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n4\n13\n1000041005\n1\n4\n1005\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\nNaN\n...\nNaN\nNaN\n373783.0\n1204211.0\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n\n\n5 rows √ó 32 columns\n\n\n\n\n\nLTFRONT\n\ndata['LTFRONT'].isnull().sum()\n\n161133\n\n\n\ndata[\"LTFRONT\"] = data.\\\n                        groupby(['TAXCLASS','BORO'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\ndata[data['LTFRONT'].isnull()]\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nBLDFRONT\nBLDDEPTH\nAVLAND2\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\n\n\n\n\n126002\n127752\n1018259034\n1\n1825\n9034\nNaN\nNaN\nV0\n1B\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n126003\n127753\n1018259036\n1\n1825\n9036\nNaN\nNaN\nV0\n1B\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n\n\n2 rows √ó 32 columns\n\n\n\n\n\ndata[\"LTFRONT\"] = data.\\\n                        groupby(['TAXCLASS'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\ndata['LTFRONT'].isnull().sum()\n\n0\n\n\n\n\nLTDEPTH\n\ndata['LTDEPTH'].isnull().sum()\n\n161715\n\n\n\ndata[\"LTDEPTH\"] = data.\\\n                        groupby(['TAXCLASS','BORO'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\ndata[data['LTDEPTH'].isnull()]\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nBLDFRONT\nBLDDEPTH\nAVLAND2\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\n\n\n\n\n126002\n127752\n1018259034\n1\n1825\n9034\nNaN\nNaN\nV0\n1B\n45.465048\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n126003\n127753\n1018259036\n1\n1825\n9036\nNaN\nNaN\nV0\n1B\n45.465048\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n\n\n\n\n2 rows √ó 32 columns\n\n\n\n\n\ndata[\"LTDEPTH\"] = data.\\\n                        groupby(['TAXCLASS'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\ndata['LTDEPTH'].isnull().sum()\n\n0\n\n\n\n\nBLDFRONT\n\ndata['BLDFRONT'].isnull().sum()\n\n206926\n\n\n\ndata['BLDFRONT'] = data.\\\n                        groupby(['TAXCLASS','BORO','BLDGCL'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDFRONT'].isnull().sum()\n\n18672\n\n\n\ndata['BLDFRONT'] = data.\\\n                        groupby(['TAXCLASS','BORO'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDFRONT'].isnull().sum()\n\n15718\n\n\n\ndata['BLDFRONT'] = data.\\\n                        groupby(['TAXCLASS'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDFRONT'].isnull().sum()\n\n0\n\n\n\n\nBLDEPTH\n\ndata['BLDDEPTH'].isnull().sum()\n\n206944\n\n\n\ndata['BLDDEPTH'] = data.\\\n                        groupby(['TAXCLASS','BORO','BLDGCL'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDDEPTH'].isnull().sum()\n\n15804\n\n\n\ndata['BLDDEPTH'] = data.\\\n                        groupby(['TAXCLASS','BORO'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDDEPTH'].isnull().sum()\n\n12830\n\n\n\ndata['BLDDEPTH'] = data.\\\n                        groupby(['TAXCLASS'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\ndata['BLDDEPTH'].isnull().sum()\n\n0\n\n\n\ndata.dtypes\n\nRECORD        int64\nBBLE         object\nBORO          int64\nBLOCK         int64\nLOT           int64\nEASEMENT     object\nOWNER        object\nBLDGCL       object\nTAXCLASS     object\nLTFRONT     float64\nLTDEPTH     float64\nEXT          object\nSTORIES     float64\nFULLVAL     float64\nAVLAND      float64\nAVTOT       float64\nEXLAND      float64\nEXTOT       float64\nEXCD1       float64\nSTADDR       object\nZIP         float64\nEXMPTCL      object\nBLDFRONT    float64\nBLDDEPTH    float64\nAVLAND2     float64\nAVTOT2      float64\nEXLAND2     float64\nEXTOT2      float64\nEXCD2       float64\nPERIOD       object\nYEAR         object\nVALTYPE      object\ndtype: object\n\n\n\n# convert ZIP to a string rather than a float\n# We call the first three digits of the zip zip3\ndata['ZIP'] = data['ZIP'].astype(str)\ndata['zip3'] = data['ZIP'].str[:3]\n\n\ncols = data.columns\nprint(cols)\n\nIndex(['RECORD', 'BBLE', 'BORO', 'BLOCK', 'LOT', 'EASEMENT', 'OWNER', 'BLDGCL',\n       'TAXCLASS', 'LTFRONT', 'LTDEPTH', 'EXT', 'STORIES', 'FULLVAL', 'AVLAND',\n       'AVTOT', 'EXLAND', 'EXTOT', 'EXCD1', 'STADDR', 'ZIP', 'EXMPTCL',\n       'BLDFRONT', 'BLDDEPTH', 'AVLAND2', 'AVTOT2', 'EXLAND2', 'EXTOT2',\n       'EXCD2', 'PERIOD', 'YEAR', 'VALTYPE', 'zip3'],\n      dtype='object')\n\n\n\n\nNow build variables that try to find properties that are unusual in ways we‚Äôre interested in\n\n# epsilon is an arbitrary small number to make sure we don't divide by zero\nepsilon = .0001\ndata['ltsize'] = data['LTFRONT'] * data['LTDEPTH'] + epsilon\ndata['bldsize'] = data['BLDFRONT'] * data['BLDDEPTH'] + epsilon\ndata['bldvol'] = data['bldsize'] * data['STORIES'] + epsilon\n\n\ndata['r1'] = data['FULLVAL'] / data['ltsize']\ndata['r2'] = data['FULLVAL'] / data['bldsize']\ndata['r3'] = data['FULLVAL'] / data['bldvol']\ndata['r4'] = data['AVLAND'] / data['ltsize']\ndata['r5'] = data['AVLAND'] / data['bldsize']\ndata['r6'] = data['AVLAND'] / data['bldvol']\ndata['r7'] = data['AVTOT'] / data['ltsize']\ndata['r8'] = data['AVTOT'] / data['bldsize']\ndata['r9'] = data['AVTOT'] / data['bldvol']\n\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nRECORD\nBORO\nBLOCK\nLOT\nLTFRONT\nLTDEPTH\nSTORIES\nFULLVAL\nAVLAND\nAVTOT\n...\nbldvol\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n...\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n5.368069e+05\n3.220281e+00\n4.756777e+03\n3.509013e+02\n5.045457e+01\n1.073812e+02\n4.969850e+00\n8.166002e+05\n6.654802e+04\n1.999379e+05\n...\n6.143487e+04\n2.107194e+02\n5.350157e+02\n2.451426e+02\n9.965350e+00\n2.336084e+01\n1.025281e+01\n2.534315e+01\n4.970893e+01\n1.930086e+01\n\n\nstd\n3.080025e+05\n1.199074e+00\n3.677416e+03\n8.267095e+02\n5.999584e+01\n5.153451e+01\n8.225039e+00\n6.399805e+06\n2.012308e+06\n5.392440e+06\n...\n2.325184e+06\n4.571358e+02\n9.277953e+02\n4.564148e+02\n6.515434e+01\n3.670539e+02\n1.592434e+02\n1.593337e+02\n9.083669e+02\n3.132694e+02\n\n\nmin\n9.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n2.000000e+00\n2.000000e+00\n1.000000e+00\n4.000000e+00\n1.000000e+00\n1.000000e+00\n...\n8.000200e+00\n3.614764e-04\n8.696149e-04\n6.269316e-05\n8.333333e-06\n9.662388e-05\n7.432606e-06\n8.333333e-06\n2.000000e-04\n3.716303e-05\n\n\n25%\n2.729100e+05\n3.000000e+00\n1.542000e+03\n2.300000e+01\n2.100000e+01\n1.000000e+02\n2.000000e+00\n3.181550e+05\n9.679000e+03\n1.892600e+04\n...\n1.408000e+03\n7.566667e+01\n2.064777e+02\n8.013468e+01\n2.370750e+00\n5.980342e+00\n2.170500e+00\n5.378469e+00\n1.736622e+01\n7.240779e+00\n\n\n50%\n5.387720e+05\n3.000000e+00\n4.078000e+03\n4.900000e+01\n3.000000e+01\n1.000000e+02\n2.000000e+00\n4.540000e+05\n1.387800e+04\n2.579100e+04\n...\n2.210000e+03\n1.533719e+02\n5.000000e+02\n2.391826e+02\n4.590250e+00\n1.504083e+01\n7.180850e+00\n8.619412e+00\n2.700291e+01\n1.272639e+01\n\n\n75%\n8.022750e+05\n4.000000e+00\n6.920000e+03\n1.400000e+02\n6.000000e+01\n1.120598e+02\n4.000000e+00\n6.240000e+05\n1.998000e+04\n4.724400e+04\n...\n7.035000e+03\n2.430000e+02\n6.853332e+02\n3.364317e+02\n7.215789e+00\n2.035416e+01\n1.006597e+01\n1.352434e+01\n3.602222e+01\n1.759953e+01\n\n\nmax\n1.070994e+06\n5.000000e+00\n1.635000e+04\n9.450000e+03\n9.999000e+03\n9.619000e+03\n1.190000e+02\n1.663775e+09\n1.792809e+09\n4.668309e+09\n...\n2.205711e+09\n8.586666e+04\n2.855351e+05\n2.275000e+05\n3.728371e+04\n3.331136e+05\n1.110379e+05\n3.864000e+04\n8.673969e+05\n2.891323e+05\n\n\n\n\n8 rows √ó 32 columns\n\n\n\n\nI want outliers in these 9 variables, either very high or very low. Very high is easy to find but very low might be close to zero and probably not many standard deviations below the average. A simple way to look for outliers that are very low is to also include 1/over these variables, which will be very large outliers when the variables are very low. First I scale them all to have reasonable average.\n\nvars9 = ['r1','r2','r3','r4','r5','r6','r7','r8','r9']\nfor vars in vars9:\n    data[vars] = data[vars]/data[vars].median()\n    \ndata.describe()\n\n\n\n\n\n\n\n\n\nRECORD\nBORO\nBLOCK\nLOT\nLTFRONT\nLTDEPTH\nSTORIES\nFULLVAL\nAVLAND\nAVTOT\n...\nbldvol\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n...\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n5.368069e+05\n3.220281e+00\n4.756777e+03\n3.509013e+02\n5.045457e+01\n1.073812e+02\n4.969850e+00\n8.166002e+05\n6.654802e+04\n1.999379e+05\n...\n6.143487e+04\n1.373911e+00\n1.070031e+00\n1.024918e+00\n2.170982e+00\n1.553161e+00\n1.427799e+00\n2.940242e+00\n1.840873e+00\n1.516602e+00\n\n\nstd\n3.080025e+05\n1.199074e+00\n3.677416e+03\n8.267095e+02\n5.999584e+01\n5.153451e+01\n8.225039e+00\n6.399805e+06\n2.012308e+06\n5.392440e+06\n...\n2.325184e+06\n2.980571e+00\n1.855591e+00\n1.908227e+00\n1.419407e+01\n2.440382e+01\n2.217612e+01\n1.848544e+01\n3.363960e+01\n2.461574e+01\n\n\nmin\n9.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n2.000000e+00\n2.000000e+00\n1.000000e+00\n4.000000e+00\n1.000000e+00\n1.000000e+00\n...\n8.000200e+00\n2.356863e-06\n1.739230e-06\n2.621142e-07\n1.815442e-06\n6.424104e-06\n1.035059e-06\n9.668100e-07\n7.406610e-06\n2.920156e-06\n\n\n25%\n2.729100e+05\n3.000000e+00\n1.542000e+03\n2.300000e+01\n2.100000e+01\n1.000000e+02\n2.000000e+00\n3.181550e+05\n9.679000e+03\n1.892600e+04\n...\n1.408000e+03\n4.933542e-01\n4.129555e-01\n3.350355e-01\n5.164751e-01\n3.976071e-01\n3.022622e-01\n6.239949e-01\n6.431241e-01\n5.689580e-01\n\n\n50%\n5.387720e+05\n3.000000e+00\n4.078000e+03\n4.900000e+01\n3.000000e+01\n1.000000e+02\n2.000000e+00\n4.540000e+05\n1.387800e+04\n2.579100e+04\n...\n2.210000e+03\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\n75%\n8.022750e+05\n4.000000e+00\n6.920000e+03\n1.400000e+02\n6.000000e+01\n1.120598e+02\n4.000000e+00\n6.240000e+05\n1.998000e+04\n4.724400e+04\n...\n7.035000e+03\n1.584384e+00\n1.370666e+00\n1.406589e+00\n1.571982e+00\n1.353260e+00\n1.401780e+00\n1.569056e+00\n1.334013e+00\n1.382917e+00\n\n\nmax\n1.070994e+06\n5.000000e+00\n1.635000e+04\n9.450000e+03\n9.999000e+03\n9.619000e+03\n1.190000e+02\n1.663775e+09\n1.792809e+09\n4.668309e+09\n...\n2.205711e+09\n5.598592e+02\n5.710702e+02\n9.511558e+02\n8.122370e+03\n2.214728e+04\n1.546305e+04\n4.482904e+03\n3.212235e+04\n2.271912e+04\n\n\n\n\n8 rows √ó 32 columns\n\n\n\n\n\n# add in the inverse of all the 9 primary variables.\nfor vars in vars9:\n    data[vars+'inv'] = 1/(data[vars] + epsilon)\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nr9\nr1inv\nr2inv\nr3inv\nr4inv\nr5inv\nr6inv\nr7inv\nr8inv\nr9inv\n\n\n\n\n0\n9\n1000041001\n1\n4\n1001\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n70.813856\n...\n0.188672\n0.319454\n1.873784\n44.626014\n0.059294\n0.349613\n8.339012\n0.039897\n0.224915\n5.297406\n\n\n1\n10\n1000041002\n1\n4\n1002\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n70.813856\n...\n0.272666\n0.221048\n1.296641\n30.921506\n0.041028\n0.241917\n5.771662\n0.027607\n0.155631\n3.666141\n\n\n2\n11\n1000041003\n1\n4\n1003\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n70.813856\n...\n0.506981\n0.118886\n0.697406\n16.654116\n0.038983\n0.229860\n5.484137\n0.014848\n0.083703\n1.972072\n\n\n3\n12\n1000041004\n1\n4\n1004\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n70.813856\n...\n0.048399\n1.245200\n7.300537\n171.742678\n0.231138\n1.362741\n32.429174\n0.155527\n0.876721\n20.619011\n\n\n4\n13\n1000041005\n1\n4\n1005\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n70.813856\n...\n0.110867\n0.543627\n3.188341\n75.706405\n0.100904\n0.594947\n14.182787\n0.067895\n0.382749\n9.011649\n\n\n\n\n5 rows √ó 54 columns\n\n\n\n\nNow I want the large outliers where the variables are either very low or very high, so I‚Äôll keep only one of the two, r or rinv, depending on which is largest. This allows me to find both the very low and high outliers.\n\nfor vars in vars9:\n    data[vars] = data[[vars,vars+'inv']].max(axis=1)\n\nNow I can remove the inverse columns since I have the 9 variables that I need\n\nfor vars in vars9:\n    data.drop(columns=(vars+'inv'),inplace=True)\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nRECORD\nBORO\nBLOCK\nLOT\nLTFRONT\nLTDEPTH\nSTORIES\nFULLVAL\nAVLAND\nAVTOT\n...\nbldvol\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n...\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n5.368069e+05\n3.220281e+00\n4.756777e+03\n3.509013e+02\n5.045457e+01\n1.073812e+02\n4.969850e+00\n8.166002e+05\n6.654802e+04\n1.999379e+05\n...\n6.143487e+04\n1.294444e+01\n2.499720e+01\n1.127279e+02\n8.519128e+00\n1.485009e+01\n5.837383e+01\n5.746233e+00\n9.022489e+00\n2.764315e+01\n\n\nstd\n3.080025e+05\n1.199074e+00\n3.677416e+03\n8.267095e+02\n5.999584e+01\n5.153451e+01\n8.225039e+00\n6.399805e+06\n2.012308e+06\n5.392440e+06\n...\n2.325184e+06\n1.088717e+02\n1.815853e+02\n5.091489e+02\n7.580573e+01\n1.304015e+02\n3.641357e+02\n5.249273e+01\n1.047840e+02\n2.144214e+02\n\n\nmin\n9.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n2.000000e+00\n2.000000e+00\n1.000000e+00\n4.000000e+00\n1.000000e+00\n1.000000e+00\n...\n8.000200e+00\n9.999570e-01\n9.999517e-01\n9.999559e-01\n9.999539e-01\n9.999507e-01\n9.999511e-01\n9.999514e-01\n9.999527e-01\n9.999501e-01\n\n\n25%\n2.729100e+05\n3.000000e+00\n1.542000e+03\n2.300000e+01\n2.100000e+01\n1.000000e+02\n2.000000e+00\n3.181550e+05\n9.679000e+03\n1.892600e+04\n...\n1.408000e+03\n1.269474e+00\n1.192974e+00\n1.210624e+00\n1.262922e+00\n1.191561e+00\n1.215178e+00\n1.233189e+00\n1.164291e+00\n1.178231e+00\n\n\n50%\n5.387720e+05\n3.000000e+00\n4.078000e+03\n4.900000e+01\n3.000000e+01\n1.000000e+02\n2.000000e+00\n4.540000e+05\n1.387800e+04\n2.579100e+04\n...\n2.210000e+03\n1.701746e+00\n1.488500e+00\n1.560963e+00\n1.682360e+00\n1.474922e+00\n1.577504e+00\n1.585997e+00\n1.403326e+00\n1.470000e+00\n\n\n75%\n8.022750e+05\n4.000000e+00\n6.920000e+03\n1.400000e+02\n6.000000e+01\n1.120598e+02\n4.000000e+00\n6.240000e+05\n1.998000e+04\n4.724400e+04\n...\n7.035000e+03\n3.207889e+00\n3.313753e+00\n3.685617e+00\n3.260447e+00\n4.274797e+00\n4.964164e+00\n2.710417e+00\n2.769308e+00\n3.372197e+00\n\n\nmax\n1.070994e+06\n5.000000e+00\n1.635000e+04\n9.450000e+03\n9.999000e+03\n9.619000e+03\n1.190000e+02\n1.663775e+09\n1.792809e+09\n4.668309e+09\n...\n2.205711e+09\n9.769741e+03\n9.829050e+03\n9.973857e+03\n9.821693e+03\n2.214728e+04\n1.546305e+04\n9.904245e+03\n3.212235e+04\n2.271912e+04\n\n\n\n\n8 rows √ó 32 columns\n\n\n\n\nNow I add more variables where I standardize each of these 9 basic variables by a few logical groupings. For example, is a property‚Äôs value of r1 typical for that zip code? for that taxclass?\n\n# Standardized variables by appropriate logical group\nzip5_mean = data.groupby('ZIP')[vars9].mean()\ntaxclass_mean = data.groupby('TAXCLASS')[vars9].mean()\ndata = data.join(zip5_mean, on='ZIP', rsuffix='_zip5')\ndata = data.join(taxclass_mean, on='TAXCLASS', rsuffix='_taxclass')\nrsuffix = ['_zip5', '_taxclass']\nfor var in vars9:\n    for r in rsuffix:\n        data[str(var)+r] = data[var] / data[str(var)+r]\n\n\n# include two more possibly interesting variables\ndata['value_ratio'] = data['FULLVAL']/(data['AVLAND']+data['AVTOT'])\ndata['value_ratio'] = data['value_ratio']/data['value_ratio'].mean()\n# again, use 1/variable if that's larger, in order to find the low outliers\ndata['value_ratio'] = np.where(data['value_ratio'] &lt; 1, 1/(data['value_ratio']+epsilon), data['value_ratio'])\ndata['size_ratio'] = data['bldsize'] / (data['ltsize']+1)\n\n\ndata.head().transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nRECORD\n9\n10\n11\n12\n13\n\n\nBBLE\n1000041001\n1000041002\n1000041003\n1000041004\n1000041005\n\n\nBORO\n1\n1\n1\n1\n1\n\n\nBLOCK\n4\n4\n4\n4\n4\n\n\nLOT\n1001\n1002\n1003\n1004\n1005\n\n\n...\n...\n...\n...\n...\n...\n\n\nr7_taxclass\n1.288115\n1.861571\n3.461304\n0.330433\n0.756924\n\n\nr8_taxclass\n0.177858\n0.257039\n0.477925\n0.045625\n0.104513\n\n\nr9_taxclass\n0.056876\n0.039362\n0.021173\n0.221379\n0.096755\n\n\nvalue_ratio\n8.056705\n8.056707\n7.135024\n8.056717\n8.056713\n\n\nsize_ratio\n1.799293\n1.799293\n1.799293\n1.799293\n1.799293\n\n\n\n\n65 rows √ó 5 columns\n\n\n\n\n\ndata.columns\n\nIndex(['RECORD', 'BBLE', 'BORO', 'BLOCK', 'LOT', 'EASEMENT', 'OWNER', 'BLDGCL',\n       'TAXCLASS', 'LTFRONT', 'LTDEPTH', 'EXT', 'STORIES', 'FULLVAL', 'AVLAND',\n       'AVTOT', 'EXLAND', 'EXTOT', 'EXCD1', 'STADDR', 'ZIP', 'EXMPTCL',\n       'BLDFRONT', 'BLDDEPTH', 'AVLAND2', 'AVTOT2', 'EXLAND2', 'EXTOT2',\n       'EXCD2', 'PERIOD', 'YEAR', 'VALTYPE', 'zip3', 'ltsize', 'bldsize',\n       'bldvol', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8', 'r9',\n       'r1_zip5', 'r2_zip5', 'r3_zip5', 'r4_zip5', 'r5_zip5', 'r6_zip5',\n       'r7_zip5', 'r8_zip5', 'r9_zip5', 'r1_taxclass', 'r2_taxclass',\n       'r3_taxclass', 'r4_taxclass', 'r5_taxclass', 'r6_taxclass',\n       'r7_taxclass', 'r8_taxclass', 'r9_taxclass', 'value_ratio',\n       'size_ratio'],\n      dtype='object')\n\n\n\nsave_record = data['RECORD']\nsave_record.head()\n\n0     9\n1    10\n2    11\n3    12\n4    13\nName: RECORD, dtype: int64\n\n\n\ndata['lot_bldsize'] = data['ltsize'] / data['bldsize']\n# Flags and Indicators\nhigh_market_value_threshold = data['FULLVAL'].quantile(0.90)\ndata['high_market_value_indicator'] = data['FULLVAL'] &gt; high_market_value_threshold\n\nhigh_building_density_threshold = 0.70\ndata['high_market_value_indicator'] = data['size_ratio'] &gt; high_building_density_threshold\n\n# Interaction Terms\ndata['lot_building_interaction'] = data['ltsize'] * data['bldsize']\ndata['market_assessed_value_interaction'] = data['FULLVAL'] * data['AVTOT']\n\n\ndropcols = ['RECORD','BBLE', 'BORO', 'BLOCK', 'LOT', 'EASEMENT',\n       'OWNER', 'BLDGCL', 'TAXCLASS', 'LTFRONT', 'LTDEPTH', 'EXT', 'STORIES',\n       'FULLVAL', 'AVLAND', 'AVTOT', 'EXLAND', 'EXTOT', 'EXCD1', 'STADDR',\n       'ZIP', 'EXMPTCL', 'BLDFRONT', 'BLDDEPTH', 'AVLAND2', 'AVTOT2',\n       'EXLAND2', 'EXTOT2', 'EXCD2', 'PERIOD', 'YEAR', 'VALTYPE', 'zip3','ltsize','bldsize','bldvol']\ndata = data.drop(columns = dropcols)\ndata.shape\n\n(1044493, 33)\n\n\n\n# this dataframe is now just the variables for our unsupervised fraud models\ndata.head().transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nr1\n3.130243\n4.523796\n8.411301\n1.2452\n1.839398\n\n\nr2\n1.873784\n1.296641\n1.433786\n7.300537\n3.188341\n\n\nr3\n44.626014\n30.921506\n16.654116\n171.742678\n75.706405\n\n\nr4\n16.865039\n24.373205\n25.651791\n4.32631\n9.910288\n\n\nr5\n2.860204\n4.133542\n4.350382\n1.362741\n1.680722\n\n\nr6\n8.339012\n5.771662\n5.484137\n32.429174\n14.182787\n\n\nr7\n25.064485\n36.222944\n67.350964\n6.429659\n14.728427\n\n\nr8\n4.446018\n6.42534\n11.946927\n1.140513\n2.612575\n\n\nr9\n5.297406\n3.666141\n1.972072\n20.619011\n9.011649\n\n\nr1_zip5\n0.344811\n0.498317\n0.926544\n0.137165\n0.202618\n\n\nr2_zip5\n0.068016\n0.047066\n0.052044\n0.264999\n0.115732\n\n\nr3_zip5\n0.111838\n0.077493\n0.041737\n0.430409\n0.18973\n\n\nr4_zip5\n1.040068\n1.503097\n1.581947\n0.266804\n0.611168\n\n\nr5_zip5\n0.140338\n0.202815\n0.213455\n0.066864\n0.082466\n\n\nr6_zip5\n0.027176\n0.01881\n0.017872\n0.105685\n0.046221\n\n\nr7_zip5\n1.368515\n1.977764\n3.677347\n0.351058\n0.804169\n\n\nr8_zip5\n0.536412\n0.775217\n1.441397\n0.137603\n0.315207\n\n\nr9_zip5\n0.104537\n0.072347\n0.038916\n0.406889\n0.177833\n\n\nr1_taxclass\n0.058539\n0.0846\n0.1573\n0.023286\n0.034399\n\n\nr2_taxclass\n0.016154\n0.011178\n0.012361\n0.062937\n0.027486\n\n\nr3_taxclass\n0.132343\n0.091701\n0.04939\n0.509322\n0.224516\n\n\nr4_taxclass\n0.545548\n0.788422\n0.829781\n0.139947\n0.320577\n\n\nr5_taxclass\n0.054123\n0.078218\n0.082321\n0.025787\n0.031804\n\n\nr6_taxclass\n0.044412\n0.030738\n0.029207\n0.17271\n0.075534\n\n\nr7_taxclass\n1.288115\n1.861571\n3.461304\n0.330433\n0.756924\n\n\nr8_taxclass\n0.177858\n0.257039\n0.477925\n0.045625\n0.104513\n\n\nr9_taxclass\n0.056876\n0.039362\n0.021173\n0.221379\n0.096755\n\n\nvalue_ratio\n8.056705\n8.056707\n7.135024\n8.056717\n8.056713\n\n\nsize_ratio\n1.799293\n1.799293\n1.799293\n1.799293\n1.799293\n\n\nlot_bldsize\n0.555706\n0.555706\n0.555706\n0.555706\n0.555706\n\n\nhigh_market_value_indicator\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nlot_building_interaction\n121492255.305798\n121492255.305798\n121492255.305798\n121492255.305798\n121492255.305798\n\n\nmarket_assessed_value_interaction\n7002516650966.0\n14625274139670.0\n50562000000000.0\n460799629504.0\n2417960736912.0\n\n\n\n\n\n\n\n\n\n# Calculate and write the basic statistics of all the variables to check if everything looks OK\nstats = data.describe().transpose()\nstats\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nr1\n1044493.0\n1.294444e+01\n1.088717e+02\n0.999957\n1.269474e+00\n1.701746e+00\n3.207889e+00\n9.769741e+03\n\n\nr2\n1044493.0\n2.499720e+01\n1.815853e+02\n0.999952\n1.192974e+00\n1.488500e+00\n3.313753e+00\n9.829050e+03\n\n\nr3\n1044493.0\n1.127279e+02\n5.091489e+02\n0.999956\n1.210624e+00\n1.560963e+00\n3.685617e+00\n9.973857e+03\n\n\nr4\n1044493.0\n8.519128e+00\n7.580573e+01\n0.999954\n1.262922e+00\n1.682360e+00\n3.260447e+00\n9.821693e+03\n\n\nr5\n1044493.0\n1.485009e+01\n1.304015e+02\n0.999951\n1.191561e+00\n1.474922e+00\n4.274797e+00\n2.214728e+04\n\n\nr6\n1044493.0\n5.837383e+01\n3.641357e+02\n0.999951\n1.215178e+00\n1.577504e+00\n4.964164e+00\n1.546305e+04\n\n\nr7\n1044493.0\n5.746233e+00\n5.249273e+01\n0.999951\n1.233189e+00\n1.585997e+00\n2.710417e+00\n9.904245e+03\n\n\nr8\n1044493.0\n9.022489e+00\n1.047840e+02\n0.999953\n1.164291e+00\n1.403326e+00\n2.769308e+00\n3.212235e+04\n\n\nr9\n1044493.0\n2.764315e+01\n2.144214e+02\n0.999950\n1.178231e+00\n1.470000e+00\n3.372197e+00\n2.271912e+04\n\n\nr1_zip5\n1044493.0\n1.000000e+00\n8.217521e+00\n0.005040\n1.619956e-01\n3.215630e-01\n6.642092e-01\n2.284920e+03\n\n\nr2_zip5\n1044493.0\n1.000000e+00\n8.869314e+00\n0.001689\n1.020206e-01\n2.270835e-01\n5.228364e-01\n1.365401e+03\n\n\nr3_zip5\n1044493.0\n1.000000e+00\n9.074389e+00\n0.000491\n5.121857e-02\n1.354225e-01\n4.224967e-01\n1.454222e+03\n\n\nr4_zip5\n1044493.0\n1.000000e+00\n6.586892e+00\n0.007198\n2.092768e-01\n3.997583e-01\n7.210783e-01\n2.092640e+03\n\n\nr5_zip5\n1044493.0\n1.000000e+00\n9.721280e+00\n0.003136\n1.464844e-01\n2.348185e-01\n4.425110e-01\n2.998695e+03\n\n\nr6_zip5\n1044493.0\n1.000000e+00\n9.493043e+00\n0.000569\n8.694206e-02\n1.559555e-01\n3.436608e-01\n1.838606e+03\n\n\nr7_zip5\n1044493.0\n1.000000e+00\n7.881081e+00\n0.009727\n2.953191e-01\n4.733899e-01\n7.430448e-01\n2.296383e+03\n\n\nr8_zip5\n1044493.0\n1.000000e+00\n1.234179e+01\n0.006126\n1.632185e-01\n2.475734e-01\n4.167635e-01\n3.005149e+03\n\n\nr9_zip5\n1044493.0\n1.000000e+00\n1.152124e+01\n0.001153\n9.615886e-02\n1.617427e-01\n2.970505e-01\n2.177199e+03\n\n\nr1_taxclass\n1044493.0\n1.000000e+00\n3.494666e+00\n0.007712\n5.608901e-01\n7.193174e-01\n9.924283e-01\n1.013376e+03\n\n\nr2_taxclass\n1044493.0\n1.000000e+00\n5.572151e+00\n0.008621\n6.723179e-01\n8.026218e-01\n1.011107e+00\n3.755317e+03\n\n\nr3_taxclass\n1044493.0\n1.000000e+00\n6.074469e+00\n0.002193\n6.655659e-01\n8.122948e-01\n1.064785e+00\n4.260404e+03\n\n\nr4_taxclass\n1044493.0\n1.000000e+00\n4.296360e+00\n0.026635\n5.986215e-01\n7.583626e-01\n1.017097e+00\n1.303541e+03\n\n\nr5_taxclass\n1044493.0\n1.000000e+00\n5.350972e+00\n0.008257\n5.863012e-01\n7.962156e-01\n9.913575e-01\n3.133052e+03\n\n\nr6_taxclass\n1044493.0\n1.000000e+00\n5.316970e+00\n0.004834\n6.154172e-01\n7.804270e-01\n1.024000e+00\n3.567660e+03\n\n\nr7_taxclass\n1044493.0\n1.000000e+00\n4.675335e+00\n0.023186\n6.099768e-01\n7.581443e-01\n1.000533e+00\n2.082944e+03\n\n\nr8_taxclass\n1044493.0\n1.000000e+00\n6.165441e+00\n0.005117\n7.017349e-01\n8.178881e-01\n9.923429e-01\n3.790923e+03\n\n\nr9_taxclass\n1044493.0\n1.000000e+00\n5.973456e+00\n0.003115\n6.826764e-01\n8.073178e-01\n1.023067e+00\n4.239516e+03\n\n\nvalue_ratio\n1044493.0\n3.255193e+00\n1.823361e+01\n0.999901\n1.119627e+00\n1.284058e+00\n6.390237e+00\n1.000271e+04\n\n\nsize_ratio\n1044493.0\n6.551120e-01\n1.261914e+01\n0.000027\n2.541492e-01\n3.811814e-01\n6.245891e-01\n1.019954e+04\n\n\nlot_bldsize\n1044493.0\n4.244950e+00\n6.185213e+01\n0.000098\n1.600000e+00\n2.622768e+00\n3.933566e+00\n3.705596e+04\n\n\nlot_building_interaction\n1044493.0\n1.066826e+08\n1.306322e+10\n3600.012000\n1.716000e+06\n3.130380e+06\n1.085600e+07\n1.031513e+13\n\n\nmarket_assessed_value_interaction\n1044493.0\n2.004535e+13\n2.637493e+15\n40.000000\n6.506798e+09\n1.159500e+10\n2.365120e+10\n1.746040e+18\n\n\n\n\n\n\n\n\n\ndata.isna().sum().sum()\n\n0\n\n\n\n# zscale all the variables\ndata_zs = (data - data.mean()) / data.std()\ndata_zs_save = data_zs.copy()\ndata_zs.describe().transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nr1\n1044493.0\n4.530633e-18\n1.0\n-0.109712\n-0.107236\n-0.103265\n-0.089431\n89.617375\n\n\nr2\n1044493.0\n-9.066709e-17\n1.0\n-0.132154\n-0.131091\n-0.129464\n-0.119412\n53.991444\n\n\nr3\n1044493.0\n-2.068037e-17\n1.0\n-0.219441\n-0.219027\n-0.218339\n-0.214166\n19.367869\n\n\nr4\n1044493.0\n1.768716e-18\n1.0\n-0.099190\n-0.095721\n-0.090188\n-0.069370\n129.451612\n\n\nr5\n1044493.0\n-5.904789e-18\n1.0\n-0.106211\n-0.104742\n-0.102569\n-0.081098\n169.725243\n\n\nr6\n1044493.0\n-2.737428e-17\n1.0\n-0.157562\n-0.156971\n-0.155976\n-0.146675\n42.304772\n\n\nr7\n1044493.0\n2.552393e-17\n1.0\n-0.090418\n-0.085975\n-0.079254\n-0.057833\n188.568939\n\n\nr8\n1044493.0\n-1.129257e-17\n1.0\n-0.076563\n-0.074994\n-0.072713\n-0.059677\n306.471822\n\n\nr9\n1044493.0\n1.455789e-18\n1.0\n-0.124256\n-0.123425\n-0.122064\n-0.113193\n105.826575\n\n\nr1_zip5\n1044493.0\n1.171434e-17\n1.0\n-0.121078\n-0.101978\n-0.082560\n-0.040863\n277.932913\n\n\nr2_zip5\n1044493.0\n-4.839478e-17\n1.0\n-0.112558\n-0.101246\n-0.087145\n-0.053799\n153.833918\n\n\nr3_zip5\n1044493.0\n-2.387086e-17\n1.0\n-0.110146\n-0.104556\n-0.095277\n-0.063641\n160.145446\n\n\nr4_zip5\n1044493.0\n4.955125e-17\n1.0\n-0.150724\n-0.120045\n-0.091127\n-0.042345\n317.545865\n\n\nr5_zip5\n1044493.0\n-4.581654e-17\n1.0\n-0.102545\n-0.087799\n-0.078712\n-0.057347\n308.364256\n\n\nr6_zip5\n1044493.0\n1.185720e-17\n1.0\n-0.105280\n-0.096182\n-0.088912\n-0.069139\n193.573973\n\n\nr7_zip5\n1044493.0\n3.945596e-19\n1.0\n-0.125652\n-0.089414\n-0.066820\n-0.032604\n291.252317\n\n\nr8_zip5\n1044493.0\n-3.524846e-17\n1.0\n-0.080529\n-0.067801\n-0.060966\n-0.047257\n243.412712\n\n\nr9_zip5\n1044493.0\n-5.727918e-17\n1.0\n-0.086696\n-0.078450\n-0.072758\n-0.061013\n188.885835\n\n\nr1_taxclass\n1044493.0\n-7.925207e-18\n1.0\n-0.283944\n-0.125651\n-0.080317\n-0.002167\n289.691763\n\n\nr2_taxclass\n1044493.0\n-7.393231e-17\n1.0\n-0.177917\n-0.058807\n-0.035422\n0.001993\n673.764395\n\n\nr3_taxclass\n1044493.0\n-3.470084e-17\n1.0\n-0.164262\n-0.055056\n-0.030901\n0.010665\n701.197810\n\n\nr4_taxclass\n1044493.0\n2.733346e-17\n1.0\n-0.226556\n-0.093423\n-0.056242\n0.003979\n303.173221\n\n\nr5_taxclass\n1044493.0\n1.123815e-17\n1.0\n-0.185339\n-0.077313\n-0.038084\n-0.001615\n585.323933\n\n\nr6_taxclass\n1044493.0\n-9.251743e-19\n1.0\n-0.187168\n-0.072331\n-0.041297\n0.004514\n670.807012\n\n\nr7_taxclass\n1044493.0\n4.419068e-17\n1.0\n-0.208929\n-0.083421\n-0.051730\n0.000114\n445.303652\n\n\nr8_taxclass\n1044493.0\n-2.258514e-18\n1.0\n-0.161364\n-0.048377\n-0.029538\n-0.001242\n614.704237\n\n\nr9_taxclass\n1044493.0\n3.466002e-18\n1.0\n-0.166886\n-0.053122\n-0.032256\n0.003862\n709.558482\n\n\nvalue_ratio\n1044493.0\n-3.327907e-17\n1.0\n-0.123689\n-0.117123\n-0.108105\n0.171938\n548.408183\n\n\nsize_ratio\n1044493.0\n2.881646e-17\n1.0\n-0.051912\n-0.031774\n-0.021708\n-0.002419\n808.207306\n\n\nlot_bldsize\n1044493.0\n-3.417703e-17\n1.0\n-0.068629\n-0.042762\n-0.026227\n-0.005034\n599.037066\n\n\nhigh_market_value_indicator\n1044493.0\n7.662620e-17\n1.0\n-0.500292\n-0.500292\n-0.500292\n-0.500292\n1.998831\n\n\nlot_building_interaction\n1044493.0\n-2.948993e-18\n1.0\n-0.008166\n-0.008035\n-0.007927\n-0.007336\n789.623089\n\n\nmarket_assessed_value_interaction\n1044493.0\n-3.387771e-18\n1.0\n-0.007600\n-0.007598\n-0.007596\n-0.007591\n662.000028\n\n\n\n\n\n\n\n\n\n# do a complete PCA and look at the scree and cumulative variance plots\npca = PCA(n_components = .99, svd_solver = 'full')\npca.fit(data_zs)\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel('Number of components minus 1')\nplt.ylabel('PC variance')\nplt.xticks(np.arange(0, 36, step=2))\nplt.axvline(x=4, linestyle='--')\n\n\n\n\n\n\n\n\n\n# do a complete PCA and look at the scree and cumulative variance plots\npca = PCA(n_components = .99, svd_solver = 'full')\npca.fit(data_zs)\nplt.axvline(x=4, linestyle='--')\nplt.bar(np.arange(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\nplt.plot(np.arange(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_, color='black')\nplt.xlabel('Number of components minus 1')\nplt.ylabel('PC variance')\nplt.xticks(np.arange(0, 10, step=1))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.xlabel('Number of components minus 1')\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.ylabel('PC cumulative variance')\nplt.yticks(np.arange(0.05, 1.1, step=.1))\nplt.xticks(np.arange(0, 36, step=2))\nplt.axvline(x=4, linestyle='--')\nplt.ylim(0,1)\n\n\n\n\n\n\n\n\n\n%%time\n# now redo the PCA but just keep the top few PCs\ndata_zs = data_zs_save.copy()\npca = PCA(n_components = 5, svd_solver = 'full')\nprinc_comps = pca.fit_transform(data_zs)\npca.n_components_\n\nCPU times: user 8.3 s, sys: 732 ms, total: 9.03 s\nWall time: 2.41 s\n\n\n5\n\n\n\nprint(np.cumsum(pca.explained_variance_ratio_))\n\n[0.3194038  0.46603662 0.57642861 0.64734677 0.70235809]\n\n\n\ndata_pca = pd.DataFrame(princ_comps, columns = ['PC' + str(i) for i in range(1, pca.n_components_+1)])\ndata_pca.shape\n\n(1044493, 5)\n\n\n\ndata_pca.head(5)\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\n0\n-0.287502\n-0.312171\n0.021055\n-0.171276\n0.190024\n\n\n1\n-0.167137\n-0.399473\n0.172508\n-0.057031\n0.245359\n\n\n2\n0.069719\n-0.575129\n0.451087\n0.203608\n0.298239\n\n\n3\n-0.387076\n-0.137734\n-0.249001\n-0.485465\n0.059842\n\n\n4\n-0.381933\n-0.226063\n-0.122581\n-0.298735\n0.132512\n\n\n\n\n\n\n\n\n\ndata_pca.describe()\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n4.468048e-17\n7.164659e-17\n-3.877569e-17\n4.680294e-17\n2.031302e-17\n\n\nstd\n3.246587e+00\n2.199746e+00\n1.908648e+00\n1.529804e+00\n1.347358e+00\n\n\nmin\n-7.175446e-01\n-2.136625e+02\n-3.459215e+02\n-3.316447e+01\n-1.712929e+02\n\n\n25%\n-4.594581e-01\n-3.344164e-02\n-6.328371e-02\n1.412898e-01\n-4.141070e-03\n\n\n50%\n-4.062916e-01\n8.377637e-02\n-1.950388e-03\n2.171742e-01\n4.098485e-02\n\n\n75%\n-2.631572e-01\n1.474075e-01\n6.211204e-02\n2.489887e-01\n6.809073e-02\n\n\nmax\n7.937746e+02\n1.384273e+03\n3.553147e+02\n3.896609e+02\n3.495221e+02\n\n\n\n\n\n\n\n\n\n# zscale the pcs.\n# I do this (make all the retained PCs equally important) if I only keep a small number of PCs.\n# Alternatively you can keep maybe up to 6 to 8 or so, and don't do this second z scale\n# I prefer to keep a somewhat small number of PCs and then make them all equally important via zscaling.\n# This second zscale step makes the later Minkowski distance to be similar to a Mahalanobis distance.\n# Many people don't do this second zscaling, but I like to do it.\n\ndata_pca_zs = (data_pca - data_pca.mean()) / data_pca.std()\ndata_pca_zs.describe()\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n-3.068041e-17\n-4.081651e-20\n-2.598651e-18\n-7.564661e-18\n3.959202e-18\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\nmin\n-2.210151e-01\n-9.713052e+01\n-1.812391e+02\n-2.167891e+01\n-1.271324e+02\n\n\n25%\n-1.415203e-01\n-1.520250e-02\n-3.315631e-02\n9.235811e-02\n-3.073474e-03\n\n\n50%\n-1.251442e-01\n3.808456e-02\n-1.021869e-03\n1.419621e-01\n3.041868e-02\n\n\n75%\n-8.105657e-02\n6.701115e-02\n3.254243e-02\n1.627586e-01\n5.053648e-02\n\n\nmax\n2.444951e+02\n6.292876e+02\n1.861604e+02\n2.547130e+02\n2.594129e+02\n\n\n\n\n\n\n\n\n\ndata_pca_zs.shape\n\n(1044493, 5)\n\n\n\ndata_pca_zs.head(5)\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\n0\n-0.088555\n-0.141912\n0.011031\n-0.111960\n0.141034\n\n\n1\n-0.051481\n-0.181599\n0.090382\n-0.037280\n0.182104\n\n\n2\n0.021475\n-0.261452\n0.236338\n0.133094\n0.221351\n\n\n3\n-0.119225\n-0.062614\n-0.130459\n-0.317338\n0.044414\n\n\n4\n-0.117641\n-0.102768\n-0.064224\n-0.195277\n0.098350"
  },
  {
    "objectID": "projects/Unusual_properties/NY_OG.html#now-calculate-two-unsupervised-fraud-scores",
    "href": "projects/Unusual_properties/NY_OG.html#now-calculate-two-unsupervised-fraud-scores",
    "title": "Unsupervised anomaly detection (fraud) algorithm",
    "section": "Now calculate two unsupervised fraud scores",
    "text": "Now calculate two unsupervised fraud scores\n\n# Set the powers for the two Minkowski distances. The final results are relatively insensitive to these choices. \n# Good choices are anywhere from 1 to about 4.\np1 = 2\np2 = 2\nntop = 10000\n\n\nCalculate score 1\n\noop1 = 1/p1\nscore1 = (((data_pca_zs).abs()**p1).sum(axis=1))**oop1\nscore1.head(10)\n\n0    0.246025\n1    0.279909\n2    0.437475\n3    0.371256\n4    0.276281\n5    0.281636\n6    0.371264\n7    0.360560\n8    0.360560\n9    0.360560\ndtype: float64\n\n\n\ndata_pca_zs.head(10)\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\n0\n-0.088555\n-0.141912\n0.011031\n-0.111960\n0.141034\n\n\n1\n-0.051481\n-0.181599\n0.090382\n-0.037280\n0.182104\n\n\n2\n0.021475\n-0.261452\n0.236338\n0.133094\n0.221351\n\n\n3\n-0.119225\n-0.062614\n-0.130459\n-0.317338\n0.044414\n\n\n4\n-0.117641\n-0.102768\n-0.064224\n-0.195277\n0.098350\n\n\n5\n-0.119354\n-0.099474\n-0.070257\n-0.203236\n0.094537\n\n\n6\n-0.006353\n-0.226469\n0.181305\n0.043025\n0.227564\n\n\n7\n-0.011020\n-0.221945\n0.172113\n0.035079\n0.223089\n\n\n8\n-0.011020\n-0.221945\n0.172113\n0.035079\n0.223089\n\n\n9\n-0.011020\n-0.221945\n0.172113\n0.035079\n0.223089\n\n\n\n\n\n\n\n\n\nscore1.max()\n\n726.0134522130637\n\n\n\n\nAutoencoder for score 2\n\nNNmodel = MLPRegressor(hidden_layer_sizes=(3),activation='logistic',max_iter=50,random_state=1)\nNNmodel.fit(data_pca_zs,data_pca_zs)\n\nMLPRegressor(activation='logistic', hidden_layer_sizes=3, max_iter=50,\n             random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†MLPRegressor?Documentation for MLPRegressoriFittedMLPRegressor(activation='logistic', hidden_layer_sizes=3, max_iter=50,\n             random_state=1) \n\n\n\n# calculate score 2 as the error of an autoencoder\npca_out = NNmodel.predict(data_pca_zs)\nerror = pca_out - data_pca_zs\noop2 = 1/p2\nscore2 = ((error.abs()**p2).sum(axis=1))**oop2\n\n\nscores = pd.DataFrame(score1)\nscores.columns=['score1']\nscores['score2'] = score2\nscores['RECORD'] = save_record\nscores.head(10)\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\n\n\n\n\n0\n0.246025\n0.222947\n9\n\n\n1\n0.279909\n0.258002\n10\n\n\n2\n0.437475\n0.373022\n11\n\n\n3\n0.371256\n0.265296\n12\n\n\n4\n0.276281\n0.227088\n13\n\n\n5\n0.281636\n0.229178\n14\n\n\n6\n0.371264\n0.329990\n15\n\n\n7\n0.360560\n0.321740\n16\n\n\n8\n0.360560\n0.321740\n17\n\n\n9\n0.360560\n0.321740\n18\n\n\n\n\n\n\n\n\n\nscores['score1 rank'] = scores['score1'].rank()\nscores['score2 rank'] = scores['score2'].rank()\nscores.head(20)\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\nscore1 rank\nscore2 rank\n\n\n\n\n0\n0.246025\n0.222947\n9\n678868.0\n808100.0\n\n\n1\n0.279909\n0.258002\n10\n765499.0\n856994.0\n\n\n2\n0.437475\n0.373022\n11\n875948.0\n963320.0\n\n\n3\n0.371256\n0.265296\n12\n850129.0\n870794.0\n\n\n4\n0.276281\n0.227088\n13\n759084.0\n813206.0\n\n\n5\n0.281636\n0.229178\n14\n768573.0\n815829.0\n\n\n6\n0.371264\n0.329990\n15\n850134.0\n937470.0\n\n\n7\n0.360560\n0.321740\n16\n844765.0\n931056.0\n\n\n8\n0.360560\n0.321740\n17\n844764.0\n931058.0\n\n\n9\n0.360560\n0.321740\n18\n844769.0\n931061.0\n\n\n10\n0.360560\n0.321740\n19\n844770.0\n931062.0\n\n\n11\n0.360560\n0.321740\n20\n844767.0\n931060.0\n\n\n12\n0.360560\n0.321740\n21\n844768.0\n931057.0\n\n\n13\n0.360560\n0.321740\n22\n844766.0\n931059.0\n\n\n14\n0.360409\n0.321624\n23\n844681.0\n930980.0\n\n\n15\n0.355454\n0.317796\n24\n841943.0\n928028.0\n\n\n16\n0.355454\n0.317796\n25\n841942.0\n928027.0\n\n\n17\n0.367873\n0.327379\n26\n848537.0\n935575.0\n\n\n18\n0.367873\n0.327379\n27\n848535.0\n935573.0\n\n\n19\n0.367873\n0.327379\n28\n848534.0\n935572.0\n\n\n\n\n\n\n\n\n\n# calculate the final score as the average of the two scores\nweight = .5\nscores['final'] = (weight*scores['score1 rank'] + (1-weight)*scores['score2 rank'])\nscores_sorted = scores.sort_values(by='final', ascending=False)\nscores_sorted.head(20)\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\nscore1 rank\nscore2 rank\nfinal\n\n\n\n\n934505\n726.013452\n724.824555\n956520\n1044493.0\n1044493.0\n1044493.0\n\n\n641107\n658.286442\n657.201701\n658933\n1044492.0\n1044492.0\n1044492.0\n\n\n897398\n358.381867\n329.739441\n917942\n1044491.0\n1044491.0\n1044491.0\n\n\n632014\n251.986026\n215.525578\n649717\n1044490.0\n1044490.0\n1044490.0\n\n\n960230\n226.307024\n213.092588\n982930\n1044488.0\n1044489.0\n1044488.5\n\n\n443552\n241.769839\n201.444558\n459429\n1044489.0\n1044487.0\n1044488.0\n\n\n957706\n224.150706\n211.014789\n980276\n1044487.0\n1044488.0\n1044487.5\n\n\n973751\n206.165107\n164.724296\n996722\n1044486.0\n1044486.0\n1044486.0\n\n\n320488\n159.593777\n160.141393\n333412\n1044485.0\n1044485.0\n1044485.0\n\n\n230836\n159.122510\n131.377231\n241946\n1044484.0\n1044484.0\n1044484.0\n\n\n110100\n132.861721\n130.441746\n111420\n1044482.0\n1044483.0\n1044482.5\n\n\n1028942\n141.101680\n115.443716\n1053859\n1044483.0\n1044481.0\n1044482.0\n\n\n956476\n131.662481\n122.488266\n979038\n1044481.0\n1044482.0\n1044481.5\n\n\n1024406\n126.321406\n102.640900\n1048771\n1044476.0\n1044480.0\n1044478.0\n\n\n1024115\n126.171272\n101.988095\n1048475\n1044475.0\n1044479.0\n1044477.0\n\n\n212609\n130.105159\n95.853362\n223485\n1044480.0\n1044466.0\n1044473.0\n\n\n678408\n128.442574\n94.071228\n696562\n1044479.0\n1044464.0\n1044471.5\n\n\n1020880\n120.657288\n95.991879\n1045012\n1044472.0\n1044467.0\n1044469.5\n\n\n7005\n117.979749\n101.199214\n7040\n1044465.0\n1044473.0\n1044469.0\n\n\n7006\n117.979749\n101.199214\n7041\n1044465.0\n1044473.0\n1044469.0\n\n\n\n\n\n\n\n\n\nscores_sorted.tail(10)\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\nscore1 rank\nscore2 rank\nfinal\n\n\n\n\n443730\n0.092243\n0.022357\n459607\n130.0\n477.0\n303.5\n\n\n408048\n0.104202\n0.020191\n423613\n292.0\n233.0\n262.5\n\n\n428809\n0.092621\n0.020661\n444598\n134.0\n273.0\n203.5\n\n\n437408\n0.102844\n0.016975\n453273\n266.0\n64.0\n165.0\n\n\n511325\n0.100810\n0.017665\n527820\n232.0\n80.0\n156.0\n\n\n408873\n0.102089\n0.012103\n424441\n256.0\n13.0\n134.5\n\n\n408049\n0.101055\n0.012903\n423614\n236.0\n17.0\n126.5\n\n\n445557\n0.094642\n0.018453\n461446\n146.0\n104.0\n125.0\n\n\n308515\n0.082448\n0.019058\n321161\n80.0\n138.0\n109.0\n\n\n182515\n0.081563\n0.015904\n186718\n75.0\n40.0\n57.5\n\n\n\n\n\n\n\n\n\nscores.describe()\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\nscore1 rank\nscore2 rank\nfinal\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n4.861766e-01\n2.176407e-01\n5.368069e+05\n5.222470e+05\n5.222470e+05\n5.222470e+05\n\n\nstd\n2.182575e+00\n1.563808e+00\n3.080025e+05\n3.015193e+05\n3.015193e+05\n2.926665e+05\n\n\nmin\n4.918587e-02\n8.101042e-03\n9.000000e+00\n1.000000e+00\n1.000000e+00\n5.750000e+01\n\n\n25%\n2.089560e-01\n6.090604e-02\n2.729100e+05\n2.611240e+05\n2.611240e+05\n2.638430e+05\n\n\n50%\n2.204428e-01\n9.609887e-02\n5.387720e+05\n5.222470e+05\n5.222470e+05\n5.045235e+05\n\n\n75%\n2.907654e-01\n1.986070e-01\n8.022750e+05\n7.833700e+05\n7.833700e+05\n7.812585e+05\n\n\nmax\n7.260135e+02\n7.248246e+02\n1.070994e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\n\n\n\n\n\n\n\nscores_sorted.describe()\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nRECORD\nscore1 rank\nscore2 rank\nfinal\n\n\n\n\ncount\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\nmean\n4.861766e-01\n2.176407e-01\n5.368069e+05\n5.222470e+05\n5.222470e+05\n5.222470e+05\n\n\nstd\n2.182575e+00\n1.563808e+00\n3.080025e+05\n3.015193e+05\n3.015193e+05\n2.926665e+05\n\n\nmin\n4.918587e-02\n8.101042e-03\n9.000000e+00\n1.000000e+00\n1.000000e+00\n5.750000e+01\n\n\n25%\n2.089560e-01\n6.090604e-02\n2.729100e+05\n2.611240e+05\n2.611240e+05\n2.638430e+05\n\n\n50%\n2.204428e-01\n9.609887e-02\n5.387720e+05\n5.222470e+05\n5.222470e+05\n5.045235e+05\n\n\n75%\n2.907654e-01\n1.986070e-01\n8.022750e+05\n7.833700e+05\n7.833700e+05\n7.812585e+05\n\n\nmax\n7.260135e+02\n7.248246e+02\n1.070994e+06\n1.044493e+06\n1.044493e+06\n1.044493e+06\n\n\n\n\n\n\n\n\n\nscores_sorted.set_index('RECORD', drop=True, inplace=True)\nscores_sorted.head(10)\n\n\n\n\n\n\n\n\n\nscore1\nscore2\nscore1 rank\nscore2 rank\nfinal\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n956520\n726.013452\n724.824555\n1044493.0\n1044493.0\n1044493.0\n\n\n658933\n658.286442\n657.201701\n1044492.0\n1044492.0\n1044492.0\n\n\n917942\n358.381867\n329.739441\n1044491.0\n1044491.0\n1044491.0\n\n\n649717\n251.986026\n215.525578\n1044490.0\n1044490.0\n1044490.0\n\n\n982930\n226.307024\n213.092588\n1044488.0\n1044489.0\n1044488.5\n\n\n459429\n241.769839\n201.444558\n1044489.0\n1044487.0\n1044488.0\n\n\n980276\n224.150706\n211.014789\n1044487.0\n1044488.0\n1044487.5\n\n\n996722\n206.165107\n164.724296\n1044486.0\n1044486.0\n1044486.0\n\n\n333412\n159.593777\n160.141393\n1044485.0\n1044485.0\n1044485.0\n\n\n241946\n159.122510\n131.377231\n1044484.0\n1044484.0\n1044484.0\n\n\n\n\n\n\n\n\n\nsc1max = int(score1.max())\nplt.hist(score1, bins =100, range=(0,sc1max+1))\nplt.yscale('log')\nplt.ylim(ymin=.1)\n\n\n\n\n\n\n\n\n\nsc2max = int(score2.max())\nsc2max\n\n724\n\n\n\nsc2max = int(score2.max())\nprint(sc2max)\nplt.hist(score2, bins =100, range=(0,sc2max+1))\nplt.yscale('log')\nplt.ylim(ymin=.1)\n\n724\n\n\n\n\n\n\n\n\n\nThe flatter the next plot, the more similar are the two scores. If the two scores are very similar then the rank order hardly changes and the plot is flat.\n\nsns.displot(scores['final'])\n\n\n\n\n\n\n\n\n\ntop_records = scores_sorted.head(ntop).index\nprint(top_records)\n\nIndex([ 956520,  658933,  917942,  649717,  982930,  459429,  980276,  996722,\n        333412,  241946,\n       ...\n         98965, 1059396,  956149,  779437, 1008025,  161344,   51125,  929750,\n         58145,  944706],\n      dtype='int64', name='RECORD', length=10000)\n\n\n\ndata_zs['RECORD'] = save_record\ndata_zs.set_index('RECORD', inplace=True, drop=True)\ndata_zs.head()\n\n\n\n\n\n\n\n\n\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr1_zip5\n...\nr6_taxclass\nr7_taxclass\nr8_taxclass\nr9_taxclass\nvalue_ratio\nsize_ratio\nlot_bldsize\nhigh_market_value_indicator\nlot_building_interaction\nmarket_assessed_value_interaction\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n-0.090145\n-0.127342\n-0.133756\n0.110096\n-0.091946\n-0.137407\n0.368018\n-0.043675\n-0.104214\n-0.079731\n...\n-0.179724\n0.061624\n-0.133347\n-0.157886\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.004945\n\n\n10\n-0.077345\n-0.130520\n-0.160673\n0.209141\n-0.082181\n-0.144458\n0.580589\n-0.024786\n-0.111822\n-0.061050\n...\n-0.182296\n0.184280\n-0.120504\n-0.160818\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.002055\n\n\n11\n-0.041637\n-0.129765\n-0.188695\n0.226007\n-0.080518\n-0.145247\n1.173586\n0.027909\n-0.119723\n-0.008939\n...\n-0.182584\n0.526444\n-0.084678\n-0.163863\n0.212785\n0.09067\n-0.059646\n1.998831\n0.001134\n0.011570\n\n\n12\n-0.107459\n-0.097456\n0.115909\n-0.055310\n-0.103429\n-0.071250\n0.013019\n-0.075221\n-0.032759\n-0.104999\n...\n-0.155594\n-0.143213\n-0.154794\n-0.130347\n0.263334\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.007425\n\n\n13\n-0.102001\n-0.120103\n-0.072713\n0.018352\n-0.100991\n-0.121359\n0.171113\n-0.061173\n-0.086892\n-0.097034\n...\n-0.173871\n-0.051991\n-0.145243\n-0.151210\n0.263334\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.006683\n\n\n\n\n5 rows √ó 33 columns\n\n\n\n\n\nscores.set_index('RECORD',inplace=True)\nscores.drop(columns=['score1','score2'],inplace=True)\nscores.head(30)\n\n\n\n\n\n\n\n\n\nscore1 rank\nscore2 rank\nfinal\n\n\nRECORD\n\n\n\n\n\n\n\n9\n678868.0\n808100.0\n743484.0\n\n\n10\n765499.0\n856994.0\n811246.5\n\n\n11\n875948.0\n963320.0\n919634.0\n\n\n12\n850129.0\n870794.0\n860461.5\n\n\n13\n759084.0\n813206.0\n786145.0\n\n\n14\n768573.0\n815829.0\n792201.0\n\n\n15\n850134.0\n937470.0\n893802.0\n\n\n16\n844765.0\n931056.0\n887910.5\n\n\n17\n844764.0\n931058.0\n887911.0\n\n\n18\n844769.0\n931061.0\n887915.0\n\n\n19\n844770.0\n931062.0\n887916.0\n\n\n20\n844767.0\n931060.0\n887913.5\n\n\n21\n844768.0\n931057.0\n887912.5\n\n\n22\n844766.0\n931059.0\n887912.5\n\n\n23\n844681.0\n930980.0\n887830.5\n\n\n24\n841943.0\n928028.0\n884985.5\n\n\n25\n841942.0\n928027.0\n884984.5\n\n\n26\n848537.0\n935575.0\n892056.0\n\n\n27\n848535.0\n935573.0\n892054.0\n\n\n28\n848534.0\n935572.0\n892053.0\n\n\n29\n848536.0\n935574.0\n892055.0\n\n\n30\n848072.0\n935039.0\n891555.5\n\n\n31\n719090.0\n828360.0\n773725.0\n\n\n32\n719092.0\n828358.0\n773725.0\n\n\n33\n719096.0\n828366.0\n773731.0\n\n\n34\n719087.0\n828357.0\n773722.0\n\n\n35\n719089.0\n828359.0\n773724.0\n\n\n36\n719095.0\n828365.0\n773730.0\n\n\n37\n719091.0\n828363.0\n773727.0\n\n\n38\n719094.0\n828364.0\n773729.0\n\n\n\n\n\n\n\n\n\nscores.tail(30)\n\n\n\n\n\n\n\n\n\nscore1 rank\nscore2 rank\nfinal\n\n\nRECORD\n\n\n\n\n\n\n\n1070965\n522201.0\n502692.0\n512446.5\n\n\n1070966\n836834.0\n898565.0\n867699.5\n\n\n1070967\n824253.0\n848050.0\n836151.5\n\n\n1070968\n891054.0\n980773.0\n935913.5\n\n\n1070969\n830636.0\n862357.0\n846496.5\n\n\n1070970\n743884.0\n767988.0\n755936.0\n\n\n1070971\n820033.0\n838925.0\n829479.0\n\n\n1070972\n736017.0\n764968.0\n750492.5\n\n\n1070973\n761012.0\n777574.0\n769293.0\n\n\n1070974\n757279.0\n776538.0\n766908.5\n\n\n1070975\n559461.0\n576011.0\n567736.0\n\n\n1070976\n791591.0\n801969.0\n796780.0\n\n\n1070977\n739251.0\n769153.0\n754202.0\n\n\n1070978\n579983.0\n558380.0\n569181.5\n\n\n1070979\n432258.0\n447401.0\n439829.5\n\n\n1070980\n1037809.0\n1041573.0\n1039691.0\n\n\n1070981\n594367.0\n592528.0\n593447.5\n\n\n1070982\n623164.0\n646634.0\n634899.0\n\n\n1070983\n693762.0\n722095.0\n707928.5\n\n\n1070984\n806874.0\n818237.0\n812555.5\n\n\n1070985\n657713.0\n695697.0\n676705.0\n\n\n1070986\n726138.0\n755905.0\n741021.5\n\n\n1070987\n391803.0\n435637.0\n413720.0\n\n\n1070988\n220064.0\n216836.0\n218450.0\n\n\n1070989\n864500.0\n934193.0\n899346.5\n\n\n1070990\n633654.0\n663809.0\n648731.5\n\n\n1070991\n906686.0\n991818.0\n949252.0\n\n\n1070992\n861146.0\n938972.0\n900059.0\n\n\n1070993\n658402.0\n684300.0\n671351.0\n\n\n1070994\n310196.0\n257939.0\n284067.5\n\n\n\n\n\n\n\n\n\nNY_data_with_scores = NY_data_orig.join(scores, on='RECORD')\nNY_data_with_scores['final'].fillna(1,inplace=True)\nNY_data_with_scores\n\n\n\n\n\n\n\n\n\nRECORD\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\n...\nAVTOT2\nEXLAND2\nEXTOT2\nEXCD2\nPERIOD\nYEAR\nVALTYPE\nscore1 rank\nscore2 rank\nfinal\n\n\n\n\n0\n1\n1000010101\n1\n1\n101\nNaN\nU S GOVT LAND & BLDGS\nP7\n4\n500\n...\n8613000.0\n3775500.0\n8613000.0\nNaN\nFINAL\n2010/11\nAC-TR\nNaN\nNaN\n1.0\n\n\n1\n2\n1000010201\n1\n1\n201\nNaN\nU S GOVT LAND & BLDGS\nZ9\n4\n27\n...\n80690400.0\n11111400.0\n80690400.0\nNaN\nFINAL\n2010/11\nAC-TR\nNaN\nNaN\n1.0\n\n\n2\n3\n1000020001\n1\n2\n1\nNaN\nDEPT OF GENERAL SERVI\nY7\n4\n709\n...\n40179510.0\n32321790.0\n40179510.0\nNaN\nFINAL\n2010/11\nAC-TR\nNaN\nNaN\n1.0\n\n\n3\n4\n1000020023\n1\n2\n23\nNaN\nDEPARTMENT OF BUSINES\nT2\n4\n793\n...\n15750000.0\n13644000.0\n15750000.0\nNaN\nFINAL\n2010/11\nAC-TR\nNaN\nNaN\n1.0\n\n\n4\n5\n1000030001\n1\n3\n1\nNaN\nPARKS AND RECREATION\nQ1\n4\n323\n...\n107758350.0\n106348680.0\n107758350.0\nNaN\nFINAL\n2010/11\nAC-TR\nNaN\nNaN\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1070989\n1070990\n5080500083\n5\n8050\n83\nNaN\nTOBIN, GALE\nA1\n1\n60\n...\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n633654.0\n663809.0\n648731.5\n\n\n1070990\n1070991\n5080500086\n5\n8050\n86\nNaN\nSHERRI MILINAZZO\nA1\n1\n62\n...\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n906686.0\n991818.0\n949252.0\n\n\n1070991\n1070992\n5080500089\n5\n8050\n89\nNaN\nJOHN GERVASI\nA1\n1\n53\n...\nNaN\nNaN\nNaN\n1017.0\nFINAL\n2010/11\nAC-TR\n861146.0\n938972.0\n900059.0\n\n\n1070992\n1070993\n5080500092\n5\n8050\n92\nNaN\nRITA M MOOG\nA1\n1\n52\n...\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n658402.0\n684300.0\n671351.0\n\n\n1070993\n1070994\n5080500094\n5\n8050\n94\nNaN\nEDWARD DONOHUE\nA1\n1\n50\n...\nNaN\nNaN\nNaN\nNaN\nFINAL\n2010/11\nAC-TR\n310196.0\n257939.0\n284067.5\n\n\n\n\n1070994 rows √ó 35 columns\n\n\n\n\n\nNY_data_scored_and_sorted = NY_data_with_scores.sort_values(by=['final','RECORD'], ascending = [False,True])\nNY_data_scored_zs = NY_data_with_scores.join(data_zs, on='RECORD')\nNY_data_scored_zs.set_index('RECORD',inplace=True)\nNY_data_scored_zs.head(20)\n\n\n\n\n\n\n\n\n\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\nLTDEPTH\n...\nr6_taxclass\nr7_taxclass\nr8_taxclass\nr9_taxclass\nvalue_ratio\nsize_ratio\nlot_bldsize\nhigh_market_value_indicator\nlot_building_interaction\nmarket_assessed_value_interaction\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1000010101\n1\n1\n101\nNaN\nU S GOVT LAND & BLDGS\nP7\n4\n500\n1046\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1000010201\n1\n1\n201\nNaN\nU S GOVT LAND & BLDGS\nZ9\n4\n27\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1000020001\n1\n2\n1\nNaN\nDEPT OF GENERAL SERVI\nY7\n4\n709\n564\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1000020023\n1\n2\n23\nNaN\nDEPARTMENT OF BUSINES\nT2\n4\n793\n551\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n1000030001\n1\n3\n1\nNaN\nPARKS AND RECREATION\nQ1\n4\n323\n1260\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n1000030002\n1\n3\n2\nNaN\nPARKS AND RECREATION\nQ1\n4\n496\n76\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n1000030003\n1\n3\n3\nNaN\nPARKS AND RECREATION\nQ1\n4\n180\n370\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\n1000030010\n1\n3\n10\nNaN\nDEPT RE-CITY OF NY\nZ9\n4\n362\n177\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\n1000041001\n1\n4\n1001\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.179724\n0.061624\n-0.133347\n-0.157886\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.004945\n\n\n10\n1000041002\n1\n4\n1002\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.182296\n0.184280\n-0.120504\n-0.160818\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.002055\n\n\n11\n1000041003\n1\n4\n1003\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.182584\n0.526444\n-0.084678\n-0.163863\n0.212785\n0.09067\n-0.059646\n1.998831\n0.001134\n0.011570\n\n\n12\n1000041004\n1\n4\n1004\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.155594\n-0.143213\n-0.154794\n-0.130347\n0.263334\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.007425\n\n\n13\n1000041005\n1\n4\n1005\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.173871\n-0.051991\n-0.145243\n-0.151210\n0.263334\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.006683\n\n\n14\n1000041006\n1\n4\n1006\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.173052\n-0.060826\n-0.146168\n-0.150276\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n-0.006781\n\n\n15\n1000041007\n1\n4\n1007\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183813\n0.326043\n-0.105661\n-0.162547\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002596\n\n\n16\n1000041008\n1\n4\n1008\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183696\n0.311661\n-0.107167\n-0.162414\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002060\n\n\n17\n1000041009\n1\n4\n1009\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183696\n0.311661\n-0.107167\n-0.162414\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002060\n\n\n18\n1000041010\n1\n4\n1010\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183696\n0.311661\n-0.107167\n-0.162414\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002060\n\n\n19\n1000041011\n1\n4\n1011\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183696\n0.311661\n-0.107167\n-0.162414\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002060\n\n\n20\n1000041012\n1\n4\n1012\nNaN\nTRZ HOLDINGS, LLC\nR5\n4\n0\n0\n...\n-0.183696\n0.311661\n-0.107167\n-0.162414\n0.263333\n0.09067\n-0.059646\n1.998831\n0.001134\n0.002060\n\n\n\n\n20 rows √ó 67 columns\n\n\n\n\n\nNY_data_scored_zs_sorted = NY_data_scored_zs.sort_values(by=['final','RECORD'], ascending = [False,True])\nNY_data_top_n = NY_data_scored_zs_sorted.head(ntop)\nNY_data_top_n\n\n\n\n\n\n\n\n\n\nBBLE\nBORO\nBLOCK\nLOT\nEASEMENT\nOWNER\nBLDGCL\nTAXCLASS\nLTFRONT\nLTDEPTH\n...\nr6_taxclass\nr7_taxclass\nr8_taxclass\nr9_taxclass\nvalue_ratio\nsize_ratio\nlot_bldsize\nhigh_market_value_indicator\nlot_building_interaction\nmarket_assessed_value_interaction\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n956520\n5006590012\n5\n659\n12\nNaN\nTROMPETA RIZALINA\nA1\n1\n25\n91\n...\n670.807012\n-0.075673\n614.704237\n709.558482\n-0.102717\n316.656659\n-0.068627\n1.998831\n1.575971\n-0.007597\n\n\n658933\n4029060054\n4\n2906\n54\nNaN\nWAN CHIU CHEUNG\nC0\n1\n25\n100\n...\n636.374724\n0.066416\n509.715716\n607.332283\n-0.110012\n443.540911\n-0.068628\n1.998831\n2.671111\n-0.007586\n\n\n917942\n4142600001\n4\n14260\n1\nNaN\nLOGAN PROPERTY, INC.\nT1\n4\n4910\n0\n...\n15.300514\n9.478741\n208.261239\n40.667816\n12.040692\n-0.051220\n1.776500\n-0.500292\n0.244888\n662.000028\n\n\n649717\n4025270002\n4\n2527\n2\nNaN\n57-43 LLC\nV1\n4\n51\n940\n...\n2.396351\n88.289486\n12.295657\n6.686020\n0.478378\n-0.050460\n0.812136\n-0.500292\n-0.004937\n-0.007600\n\n\n982930\n5020900016\n5\n2090\n16\nNaN\nFOREST VIEW HOMEOWNER\nZ0\n1\n371\n211\n...\n11.027945\n445.303652\n18.567554\n18.446932\n-0.085063\n-0.051105\n1.514320\n-0.500292\n-0.003375\n-0.007600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n161344\n2028830060F\n2\n2883\n60\nF\nNaN\nV1\n4\n308\n414\n...\n-0.175918\n0.119849\n-0.108352\n-0.128775\n0.471817\n-0.045016\n0.117096\n-0.500292\n0.100182\n-0.007599\n\n\n51125\n1009130040\n1\n913\n40\nNaN\nRICHARD A CIRILLO\nB9\n1\n25\n98\n...\n0.931872\n2.670813\n1.486738\n0.706372\n-0.113786\n-0.013116\n-0.035622\n-0.500292\n-0.007942\n-0.005410\n\n\n929750\n4161220056\n4\n16122\n56\nNaN\nGRADY, DIANA L\nC2\n2A\n40\n97\n...\n-0.139221\n0.433725\n0.341563\n0.139585\n0.455862\n-0.037621\n0.020984\n-0.500292\n-0.007959\n-0.007600\n\n\n58145\n1010114004\n1\n1011\n4004\nNaN\nCENTRAL PARK REALTY H\nR4\n2\n0\n0\n...\n1.577151\n0.393144\n1.562417\n4.051715\n0.314487\n0.065690\n-0.057737\n1.998831\n0.013932\n-0.007600\n\n\n944706\n5002580005\n5\n258\n5\nNaN\nNaN\nV0\n1B\n47\n840\n...\n-0.093423\n1.306094\n-0.081544\n-0.072015\n7.276806\n-0.044005\n0.093359\n-0.500292\n0.003742\n-0.007599\n\n\n\n\n10000 rows √ó 67 columns\n\n\n\n\n\nNY_data_top_n['OWNER'].head(40)\n\nRECORD\n956520         TROMPETA RIZALINA\n658933           WAN CHIU CHEUNG\n917942      LOGAN PROPERTY, INC.\n649717                 57-43 LLC\n982930     FOREST VIEW HOMEOWNER\n459429                       NaN\n980276     WOODMONT WEST HOA INC\n996722     IMPERIAL COURT HOMEOW\n333412            SPOONER ALSTON\n241946       RUFFALO ENTERPRISES\n111420     BOXWOOD FLTD PARNTERS\n1053859                      NaN\n979038           CITY WEST H.O.A\n1048771     HUGUENOT VILLAGE H O\n1048475    PARK VILLAGE RESIDENT\n223485             ASSET HLDG CP\n696562                       NaN\n1045012           LINDA VITALONE\n7033                         NaN\n7034              HSIA, JONATHAN\n7035                NOVARO, HUGO\n7036             LECH, JOHN PAUL\n7037                 HANLON, AMY\n7038               OWEN, MICHAEL\n7039              LAMBERT, PETER\n7040            BILINKAS, EDWARD\n7041                         NaN\n7042           SWENDSRUD, MONICA\n7043                CHOI, JOSEPH\n844895                XEDIT CORP\n146231              NY NH & H RR\n373564               J BRENOWITZ\n524297             MARTIN OLINER\n871048       VALERIE SHAKESPEARE\n114694                       NaN\n539374         CONS LAND INV INC\n435070               KLEIN HARRY\n718883           GARDEN VIEW LTD\n877038              DASH MARLENE\n1000954    MATIONAL COLLECTORS &\nName: OWNER, dtype: object\n\n\n\n# you can look at this list and add some to the exclusions if you want\nplt.figure(figsize=(6,14))\nNY_data_top_n['OWNER'].value_counts().head(50).sort_values().plot(kind='barh')\n\n\n\n\n\n\n\n\n\nNY_data_top_n.shape\n\n(10000, 67)\n\n\n\nNY_top_lotsize_ne_0 = NY_data_top_n[NY_data_top_n['LTFRONT'] != 0]\nNY_top_sizes_ne_0 = NY_top_lotsize_ne_0[NY_top_lotsize_ne_0['BLDDEPTH'] != 0]\n\n\nnfields = 34\ndata_base_vars = NY_data_top_n.iloc[:,nfields:nfields+9]\ndata_base_vars.head()\n\n\n\n\n\n\n\n\n\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n\n\n\n\n956520\n-0.109693\n31.053329\n12.586493\n-0.092675\n35.717026\n15.129681\n-0.089171\n51.484331\n28.990580\n\n\n658933\n-0.100307\n25.979800\n10.859515\n-0.081413\n33.529659\n14.345071\n-0.068305\n42.678673\n24.796319\n\n\n917942\n-0.082428\n0.627763\n-0.031184\n8.275884\n169.725243\n42.304772\n16.688598\n306.471822\n105.826575\n\n\n649717\n89.063881\n36.576029\n16.284157\n90.587261\n8.849828\n6.925421\n153.273624\n18.237425\n17.653725\n\n\n982930\n15.633665\n0.239314\n-0.085664\n28.627389\n0.594399\n0.095272\n65.313551\n1.484807\n0.634812\n\n\n\n\n\n\n\n\n\n# The heatmaps are good for seeing which variables are driving the high scores\ndata_heatmap = data_base_vars.abs().head(23)\nplt.rcParams['figure.figsize'] = (20,10)\nax = sns.heatmap(data_heatmap, center=0, vmin=0, vmax=50, cmap='Reds')\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nplt.xticks(rotation=90)\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]),\n [Text(0.5, 1, 'r1'),\n  Text(1.5, 1, 'r2'),\n  Text(2.5, 1, 'r3'),\n  Text(3.5, 1, 'r4'),\n  Text(4.5, 1, 'r5'),\n  Text(5.5, 1, 'r6'),\n  Text(6.5, 1, 'r7'),\n  Text(7.5, 1, 'r8'),\n  Text(8.5, 1, 'r9')])\n\n\n\n\n\n\n\n\n\n\ndata_all_vars = NY_data_top_n.iloc[:,nfields:]\ndata_all_vars.head()\n\n\n\n\n\n\n\n\n\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr1_zip5\n...\nr6_taxclass\nr7_taxclass\nr8_taxclass\nr9_taxclass\nvalue_ratio\nsize_ratio\nlot_bldsize\nhigh_market_value_indicator\nlot_building_interaction\nmarket_assessed_value_interaction\n\n\nRECORD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n956520\n-0.109693\n31.053329\n12.586493\n-0.092675\n35.717026\n15.129681\n-0.089171\n51.484331\n28.990580\n-0.120225\n...\n670.807012\n-0.075673\n614.704237\n709.558482\n-0.102717\n316.656659\n-0.068627\n1.998831\n1.575971\n-0.007597\n\n\n658933\n-0.100307\n25.979800\n10.859515\n-0.081413\n33.529659\n14.345071\n-0.068305\n42.678673\n24.796319\n-0.110202\n...\n636.374724\n0.066416\n509.715716\n607.332283\n-0.110012\n443.540911\n-0.068628\n1.998831\n2.671111\n-0.007586\n\n\n917942\n-0.082428\n0.627763\n-0.031184\n8.275884\n169.725243\n42.304772\n16.688598\n306.471822\n105.826575\n0.084077\n...\n15.300514\n9.478741\n208.261239\n40.667816\n12.040692\n-0.051220\n1.776500\n-0.500292\n0.244888\n662.000028\n\n\n649717\n89.063881\n36.576029\n16.284157\n90.587261\n8.849828\n6.925421\n153.273624\n18.237425\n17.653725\n277.932913\n...\n2.396351\n88.289486\n12.295657\n6.686020\n0.478378\n-0.050460\n0.812136\n-0.500292\n-0.004937\n-0.007600\n\n\n982930\n15.633665\n0.239314\n-0.085664\n28.627389\n0.594399\n0.095272\n65.313551\n1.484807\n0.634812\n9.801218\n...\n11.027945\n445.303652\n18.567554\n18.446932\n-0.085063\n-0.051105\n1.514320\n-0.500292\n-0.003375\n-0.007600\n\n\n\n\n5 rows √ó 33 columns\n\n\n\n\n\ndata_heatmap = data_all_vars.abs().head(30)\nplt.rcParams['figure.figsize'] = (20,10)\nax = sns.heatmap(data_heatmap, center=0, vmin=0, vmax=50, cmap='Reds')\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nplt.xticks(rotation=90)\n\n(array([ 0.5,  1.5,  2.5,  3.5,  4.5,  5.5,  6.5,  7.5,  8.5,  9.5, 10.5,\n        11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5, 21.5,\n        22.5, 23.5, 24.5, 25.5, 26.5, 27.5, 28.5, 29.5, 30.5, 31.5, 32.5]),\n [Text(0.5, 1, 'r1'),\n  Text(1.5, 1, 'r2'),\n  Text(2.5, 1, 'r3'),\n  Text(3.5, 1, 'r4'),\n  Text(4.5, 1, 'r5'),\n  Text(5.5, 1, 'r6'),\n  Text(6.5, 1, 'r7'),\n  Text(7.5, 1, 'r8'),\n  Text(8.5, 1, 'r9'),\n  Text(9.5, 1, 'r1_zip5'),\n  Text(10.5, 1, 'r2_zip5'),\n  Text(11.5, 1, 'r3_zip5'),\n  Text(12.5, 1, 'r4_zip5'),\n  Text(13.5, 1, 'r5_zip5'),\n  Text(14.5, 1, 'r6_zip5'),\n  Text(15.5, 1, 'r7_zip5'),\n  Text(16.5, 1, 'r8_zip5'),\n  Text(17.5, 1, 'r9_zip5'),\n  Text(18.5, 1, 'r1_taxclass'),\n  Text(19.5, 1, 'r2_taxclass'),\n  Text(20.5, 1, 'r3_taxclass'),\n  Text(21.5, 1, 'r4_taxclass'),\n  Text(22.5, 1, 'r5_taxclass'),\n  Text(23.5, 1, 'r6_taxclass'),\n  Text(24.5, 1, 'r7_taxclass'),\n  Text(25.5, 1, 'r8_taxclass'),\n  Text(26.5, 1, 'r9_taxclass'),\n  Text(27.5, 1, 'value_ratio'),\n  Text(28.5, 1, 'size_ratio'),\n  Text(29.5, 1, 'lot_bldsize'),\n  Text(30.5, 1, 'high_market_value_indicator'),\n  Text(31.5, 1, 'lot_building_interaction'),\n  Text(32.5, 1, 'market_assessed_value_interaction')])\n\n\n\n\n\n\n\n\n\n\ntop_records_df = pd.DataFrame(top_records)"
  }
]